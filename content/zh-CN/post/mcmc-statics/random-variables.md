---
title: "éšæœºå˜é‡å’Œé‡‡æ ·"
description: "äº†è§£éšæœºå˜é‡ã€æ¦‚ç‡å¯†åº¦å‡½æ•°ã€æœŸæœ›ç­‰æ¦‚å¿µã€å¸¸è§é‡‡æ ·æ–¹æ³•ä»¥åŠç®€å•åˆ†å¸ƒï¼ˆå‡åŒ€ã€æ­£æ€ã€æŒ‡æ•°ï¼‰çš„é‡‡æ ·æ–¹å¼"
summary: "äº†è§£éšæœºå˜é‡ã€æ¦‚ç‡å¯†åº¦å‡½æ•°ã€æœŸæœ›ç­‰æ¦‚å¿µã€å¸¸è§é‡‡æ ·æ–¹æ³•ä»¥åŠç®€å•åˆ†å¸ƒï¼ˆå‡åŒ€ã€æ­£æ€ã€æŒ‡æ•°ï¼‰çš„é‡‡æ ·æ–¹å¼"
date: 2025-08-02
draft: false
tags: ["éšæœºå˜é‡", "é‡‡æ ·", "éšæœºè¯•éªŒ", "è¯¾ç¨‹ç¬”è®°", "æ•°å­¦", "python"]
---

{{< toc >}}

# éšæœºå˜é‡ï¼ˆRandom Variablesï¼‰


```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ ·å¼
sns.set_theme(style="whitegrid")

def plot_discrete_rv(values, pmf, cdf, samples, rv_name):
    # å‡†å¤‡ç»éªŒ CDFï¼ˆECDFï¼‰
    n_samples = len(samples)
    sorted_samples = np.sort(samples)
    ecdf_x = np.unique(sorted_samples)
    ecdf_y = [np.sum(sorted_samples <= x) / n_samples for x in ecdf_x]
    plt.figure(figsize=(15, 8))

    # ç†è®º PMF
    plt.subplot(2, 2, 1)
    plt.stem(values, pmf, basefmt=" ", linefmt='-.')
    plt.title(f"Theoretical PMF: {rv_name}")
    plt.xlabel("x")
    plt.ylabel("f(X=x) = P(X = x)")
    plt.ylim(0, 1.1)

    # ç†è®º CDF
    plt.subplot(2, 2, 2)
    plt.step(values, cdf, where='post', color='green')
    plt.title(f"Theoretical CDF: {rv_name}")
    plt.xlabel("x")
    plt.ylabel("F(x) = P(X â‰¤ x)")
    plt.ylim(0, 1.1)
    plt.grid(True)

    # é‡‡æ ·ç›´æ–¹å›¾
    
    plt.subplot(2, 2, 3)
    sns.countplot(x=samples, hue=samples, legend=False, palette='pastel', stat='proportion', order=values)
    plt.title(f"Empirical Distribution ({n_samples} samples)")
    plt.xlabel("x")
    plt.ylabel("Relative Frequency")

    # ç»éªŒ CDFï¼ˆECDFï¼‰
    plt.subplot(2, 2, 4)
    plt.step(ecdf_x, ecdf_y, where='post', color='orange')
    plt.title("Empirical CDF")
    plt.xlabel("x")
    plt.ylabel("ECDF")
    plt.ylim(0, 1.1)
    plt.grid(True)
    plt.tight_layout()
    plt.show()
```

## å‡åŒ€åˆ†å¸ƒï¼ˆUniform RVï¼‰

### ç¦»æ•£å‡åŒ€åˆ†å¸ƒï¼ˆDiscrete Uniform Random Variableï¼‰

> å¦‚æœä¸€ä¸ªéšæœºå˜é‡ $X$ åœ¨ä¸€ç»„**æœ‰é™çš„ç¦»æ•£æ•°å€¼é›†åˆ**ä¸­å–å€¼ï¼Œä¸”æ¯ä¸ªå€¼å‡ºç°çš„æ¦‚ç‡ç›¸åŒï¼Œé‚£ä¹ˆå®ƒå°±æ˜¯ä¸€ä¸ª**ç¦»æ•£å‹å‡åŒ€éšæœºå˜é‡**ã€‚

ç¤ºä¾‹ï¼š

* æ·ä¸€ä¸ªéª°å­ï¼š
  $X \in \{1, 2, 3, 4, 5, 6\}$ï¼Œæ¯ä¸ªç‚¹çš„æ¦‚ç‡æ˜¯ $\frac{1}{6}$
* éšæœºé€‰æ‹©ä¸€å¼ æ‰‘å…‹ç‰Œï¼ˆä» 1 åˆ° 52ï¼‰



**æ•°å­¦å®šä¹‰**

è®¾ $X \sim \text{DiscreteUniform}(a, b)$ï¼Œå…¶ä¸­ $a$, $b \in \mathbb{Z}$ï¼Œä¸” $a \leq b$ã€‚

* æ”¯æŒé›†ï¼ˆå–å€¼èŒƒå›´ï¼Œå€¼åŸŸï¼‰æ˜¯ï¼š

  $$
  k \in \{a, a+1, a+2, \dots, b\}
  $$

* æ¯ä¸ªå€¼çš„æ¦‚ç‡æ˜¯ï¼š

  $$
  P(X = k) = \frac{1}{b - a + 1}, \quad \text{for } k \in \{a, \dots, b\}
  $$

* æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆPMFï¼‰ï¼š
  $$
  f(X=k) = P(X=k) = \left\{
  \begin{aligned}
  \frac{1}{b-a+1}, \text{for } a \le k \le b\\
  0, \text{ otherwise}
  \end{aligned}
  \right.
  $$  

* ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF):
  $$
  F(X=k) = P(X\le k) = \left\{
  \begin{aligned}
  0,  \text{for } k \lt a \\
  \frac{k-a+1}{b-a+1}, \text{for } a \le k \le b\\
  1, \text{ for } k \gt b
  \end{aligned}
  \right.
  $$ 

* æœŸæœ›ï¼ˆ$\mu$ï¼‰ï¼š$\frac{a+b}{2}$
* æ–¹å·®ï¼ˆ$\sigma^2$ï¼‰ï¼š$\frac{(b-a+1)^2-1}{12}$
  


```python
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯é‡å¤
np.random.seed(42)

# 1. å®šä¹‰å‚æ•°
a, b = 1, 6  # å‡åŒ€åˆ†å¸ƒçš„èŒƒå›´
values = np.arange(a, b+1)  # ç¦»æ•£å–å€¼ï¼š1~6
n = len(values)
pmf = np.ones(n) / n  # æ¯ä¸ªå€¼çš„æ¦‚ç‡å‡ç­‰
cdf = np.cumsum(pmf)  # ç´¯ç§¯åˆ†å¸ƒå‡½æ•°

# 2. æŠ½æ ·
n_samples = 1000
samples = np.random.choice(values, size=n_samples, p=pmf)


# 3. å¯è§†åŒ–ï¼šç†è®ºåˆ†å¸ƒ + é‡‡æ ·é¢‘ç‡å¯¹æ¯”
plot_discrete_rv(values, pmf, cdf, samples, f"DiscreteUniform({a},{b})")

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_4_0.png)
    


#### é‡‡æ ·

æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª**è¿ç»­å‡åŒ€åˆ†å¸ƒ $U \sim \text{Uniform}(0, 1)$** æ¥ç”Ÿæˆç¦»æ•£å‡åŒ€éšæœºæ•°ã€‚

æ­¥éª¤ï¼š

1. è®¾åŒºé—´ä¸ºæ•´æ•° $a$ åˆ° $b$ï¼ˆå«ç«¯ç‚¹ï¼‰ï¼Œæ€»å…±æœ‰ $N = b - a + 1$ ä¸ªæ•°
2. ç”Ÿæˆä¸€ä¸ªéšæœºæ•° $U \sim \text{Uniform}(0, 1)$
3. å°† $U$ æ˜ å°„åˆ°æ•´æ•°èŒƒå›´å†…ï¼š

   $$
   X = a + \left\lfloor U \cdot N \right\rfloor
   $$

   âœ… è¿™æ ·å¾—åˆ°çš„æ•´æ•°å°±æ˜¯ $\{a, a+1, ..., b\}$ ä¸­çš„ä¸€ä¸ªï¼Œä¸”ç­‰æ¦‚ç‡

å¦‚æœ $a=0, b=1$ï¼Œåˆ™ï¼š
1. $N = b - a + 1 = 1 - 0 + 1 = 2$
2. $U \sim \text{Uniform}(0, 1)$
3. $X = a + \left\lfloor U \cdot N \right\rfloor = 0 + \left\lfloor U \cdot 2 \right\rfloor = \left\lfloor U \cdot 2 \right\rfloor$

**æ€»ç»“ï¼š**
| æ­¥éª¤   | æè¿°                                                                               |
| ---- | -------------------------------------------------------------------------------- |
| ç›®æ ‡   | ä» $\{a, a+1, ..., b\}$ ä¸­ç­‰æ¦‚ç‡é‡‡æ ·                                                    |
| æ–¹æ³•   | ç”Ÿæˆ $U \sim \text{Uniform}(0,1)$ï¼Œç„¶å $X = a + \lfloor U \cdot (b - a + 1) \rfloor$ |
| å·¥å…·å‡½æ•° | `random.random()` or `random.randint(a, b)`                                      |
| åº”ç”¨   | æ¨¡æ‹Ÿéª°å­ã€è½®ç›˜ã€æŠ½ç­¾ã€å‡åŒ€æ•´æ•°é‡‡æ ·ç­‰   



```python
import random

def discrete_uniform_sample(a, b, n):
    N = b - a + 1
    # U ~ Uniform(0, 1)
    U = [random.random() for _ in range(n)] # random.random() è¿”å›åŒºé—´ [0.0, 1.0) çš„æµ®ç‚¹æ•°
    # X ~ Discrete Uniform(a, b)
    X = [a+int(u*N) for u in U]  # When u = 0.999, u*N = 0.999 * N, which is close to N-1, so a + int(u*N) will be b.
    return U, X


discrete_uniform_sample(0, 1, 10)
```




    ([0.4444854289944321,
      0.951251619861675,
      0.7646892516581814,
      0.9854176841589392,
      0.0983350059391166,
      0.5245935455925463,
      0.962496892423623,
      0.7602027193895072,
      0.3724452123714195,
      0.8460390235179297],
     [0, 1, 1, 1, 0, 1, 1, 1, 0, 1])



##### æ›´ç®€æ´æ–¹å¼ï¼ˆå†…ç½®å‡½æ•°ï¼‰

å½“ç„¶ï¼ŒPython ä¹Ÿæä¾›äº†ç›´æ¥é‡‡æ ·çš„æ–¹æ³•ï¼š`random.randint(a, b)  # åŒ…å« a å’Œ b`

å®ƒå®ç°çš„å°±æ˜¯ä¸Šé¢çš„åŸç†ã€‚


```python
[random.randint(0, 1) for _ in range(10)]  # ä½¿ç”¨å†…ç½®å‡½æ•°ç›´æ¥é‡‡æ ·ï¼ŒéªŒè¯ç»“æœæ˜¯å¦æ­£ç¡®
```




    [0, 1, 1, 0, 0, 1, 0, 0, 0, 0]



##### éªŒè¯é‡‡æ ·ç»“æœ

æˆ‘ä»¬é‡‡æ · 10,000 æ¬¡ï¼Œçœ‹çœ‹åˆ†å¸ƒæ˜¯å¦å‡åŒ€ã€‚


ä½ ä¼šçœ‹åˆ° 1 åˆ° 6 ä¹‹é—´çš„æŸ±çŠ¶å›¾é«˜åº¦å¤§è‡´ç›¸ç­‰ï¼Œè¿™è¯´æ˜æˆ‘ä»¬æ­£ç¡®é‡‡æ ·ã€‚


```python
import random
import matplotlib.pyplot as plt
from math import comb

# é‡‡æ ·
a, b, N = 1, 10, 30000
origin_samples, samples = discrete_uniform_sample(a, b, N)
print(f"Empirical mean: {sum(samples)/len(samples):.3f}")
# ç»Ÿè®¡é¢‘ç‡
counts = [samples.count(k) / N for k in range(a, b+1)]

# è®¡ç®—ç†è®ºæ¦‚ç‡ï¼ˆPMFï¼‰
theoretical = [1/(b-a+1) for _ in range(a, b+1)]
print(f"PMF = {theoretical}")

# Step 5: å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.bar(range(a, b+1), counts, width=0.4, label='Sampled Frequency', color='skyblue', align='center')
plt.bar(range(a, b+1), theoretical, width=0.4, label='Theoretical PMF', color='orange', align='edge')
plt.xlabel("Value")
plt.ylabel("Probability")
plt.title(f"Discrete Uniform Sample ({a}~{b})")
plt.legend()
plt.grid(True)
plt.show()
```

    Empirical mean: 5.515
    PMF = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_10_1.png)
    


### è¿ç»­å‡åŒ€åˆ†å¸ƒï¼ˆContinuous Uniform Random Variableï¼‰

**ä»€ä¹ˆæ˜¯è¿ç»­å‹å‡åŒ€åˆ†å¸ƒï¼Ÿ**

> ä¸€ä¸ªéšæœºå˜é‡ $X \sim \text{Uniform}(a, b)$ï¼Œå¦‚æœå®ƒåœ¨åŒºé—´ $[a, b]$ ä¸Šçš„æ¯ä¸€ä¸ªå€¼éƒ½ç­‰å¯èƒ½åœ°å‡ºç°ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç§°å®ƒæœä»**è¿ç»­å‡åŒ€åˆ†å¸ƒ**ã€‚


**æ•°å­¦å®šä¹‰**

* æ”¯æŒé›†ï¼ˆå–å€¼èŒƒå›´ï¼‰ï¼š$X \in [a, b]$
* æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼š

  $$
  f_X(x) = \begin{cases}
  \frac{1}{b - a} & \text{if } x \in [a, b] \\
  0 & \text{otherwise}
  \end{cases}
  $$
* ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ï¼š

  $$
  F_X(x) = \begin{cases}
  0 & \text{if } x < a \\
  \frac{x - a}{b - a} & \text{if } a \leq x \leq b \\
  1 & \text{if } x > b
  \end{cases}
  $$

* æœŸæœ›ï¼ˆ$\mu$ï¼‰ï¼š$\frac{a+b}{2}$
* æ–¹å·®ï¼ˆ$\sigma^2$ï¼‰ï¼š$\frac{(b-a)^2}{12}$


**å‚è€ƒï¼š**
- [Wiki: Continuous uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution)


#### é‡‡æ ·



**é‡‡æ ·åŸç†ï¼ˆInverse Transform Samplingï¼‰**

æœ€ç®€å•æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ï¼š

> å¦‚æœ $U \sim \text{Uniform}(0,1)$ï¼Œé‚£ä¹ˆ
>
> $$
> X = a + (b - a) \cdot U \sim \text{Uniform}(a, b)
> $$


*ğŸ’¡ä¸ºä»€ä¹ˆæˆç«‹ï¼Ÿ*

å› ä¸ºï¼š

1. $U \in [0, 1]$ï¼Œæ˜¯æ ‡å‡†å‡åŒ€åˆ†å¸ƒ
2. ç¼©æ”¾åŒºé—´é•¿åº¦ä¸º $(b - a)$ï¼Œå†åŠ ä¸Š $a$ ç›¸å½“äºâ€œçº¿æ€§æ˜ å°„â€
3. å˜æ¢åçš„éšæœºå˜é‡ $X$ åœ¨ $[a, b]$ ä¸Šä¹Ÿå‡åŒ€åˆ†å¸ƒ


**é‡‡æ ·æ­¥éª¤ï¼š**
```text
Step 1ï¼šç”Ÿæˆä¸€ä¸ª U ~ Uniform(0, 1)
Step 2ï¼šé€šè¿‡çº¿æ€§å˜æ¢ X = a + (b - a) * U
Step 3ï¼šX å°±æ˜¯ä½ è¦çš„ sample from Uniform(a, b)
```


**æ€»ç»“è¡¨æ ¼**

| é¡¹ç›®       | å†…å®¹                                                        |
| -------- | --------------------------------------------------------- |
| åˆ†å¸ƒåç§°     | è¿ç»­å‡åŒ€åˆ†å¸ƒ Uniform(a, b)                                      |
| PDF      | $f(x) = \frac{1}{b - a}$                                  |
| é‡‡æ ·æ–¹æ³•     | $X = a + (b - a) \cdot U$ï¼Œå…¶ä¸­ $U \sim \text{Uniform}(0,1)$ |
| Pythonå‡½æ•° | `random.random()` æˆ– `random.uniform(a, b)`                |
| åº”ç”¨åœºæ™¯     | è’™ç‰¹å¡æ´›æ–¹æ³•ã€æ¨¡æ‹Ÿå®éªŒã€éšæœºåˆå§‹åŒ–ç­‰                                        |


```python
import random

def sample_uniform(a, b):
    U = random.random()        # U ~ Uniform(0,1)
    X = a + (b - a) * U        # X ~ Uniform(a, b)
    return X


def sample_uniform_list(a, b, n):
    return [sample_uniform(a, b) for _ in range(n)]
```


```python
sample_uniform_list(0, 1, 10)
```




    [0.6400024988096578,
     0.05979675338996093,
     0.5161926269415474,
     0.4823864030690008,
     0.31338893853775884,
     0.4885049387562129,
     0.7751242044584421,
     0.03653104468277457,
     0.1006986841203773,
     0.05647975387925808]



##### ç›´æ¥ç”¨å†…ç½®å‡½æ•° `uniform(a, b)`

è¿™æ˜¯æ ‡å‡†åº“å°è£…å¥½çš„å½¢å¼ï¼Œå†…éƒ¨å®ç°å…¶å®ä¹Ÿæ˜¯ `a + (b - a) * random.random()`ã€‚



```python
random.uniform(0, 1)
```




    0.8338807889744516



##### éªŒè¯é‡‡æ ·æ•ˆæœ

åŸç†ï¼š**æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¡ç®—é‡‡æ ·ç»“æœçš„ç›´æ–¹å›¾ï¼Œæ¥éªŒè¯é‡‡æ ·æ•°æ®æ˜¯å¦ä¸æ¨¡å‹ä¸€è‡´ã€‚**

æˆ‘ä»¬é‡‡æ · 10,000 ä¸ª $X \sim \text{Uniform}(2, 5)$ï¼Œç”»å‡ºç›´æ–¹å›¾çœ‹çœ‹åˆ†å¸ƒæ˜¯å¦å‡åŒ€ï¼š

âœ… å¦‚æœçœ‹åˆ°ç›´æ–¹å›¾éå¸¸æ¥è¿‘å¹³çš„ï¼Œè¯´æ˜åˆ†å¸ƒæ˜¯å‡åŒ€çš„ã€‚


```python
import matplotlib.pyplot as plt

a, b, n = 0, 1, 10000
pdf = 1 / (b - a)  # å‡åŒ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°
samples = sample_uniform_list(a, b, n)

plt.figure(figsize=(15, 10))
# é‡‡æ ·ç»“æœ
plt.subplot(2, 2, 1)
plt.scatter(range(n), samples, alpha=0.5, color='blue')
plt.title(f"Sample results from Uniform({a}, {b})")
plt.xlabel("n")
plt.ylabel("Sample Value (X)")
plt.ylim(0, 1.1)

# é‡‡æ ·ç›´æ–¹å›¾ï¼ˆé¢‘ç‡ï¼‰
N = 10
plt.subplot(2, 2, 3)
plt.hist(samples, bins=N, density=False, edgecolor='black')
plt.title(f"Histogram of Sample from Uniform({a}, {b})")
plt.xlabel("Sample Value (X)")
plt.ylabel("Absolute Frequency")

# é‡‡æ ·ç›´æ–¹å›¾ï¼ˆç›¸å¯¹é¢‘ç‡ï¼‰
plt.subplot(2, 2, 4)
plt.hist(samples, bins=N, density=True, edgecolor='black')
plt.hlines(pdf, a, b, colors='red', linestyles='solid', label='PDF')
plt.title(f"Histogram of Sample from Uniform({a}, {b})")
plt.xlabel("Sample Value (X)")
plt.ylabel(f"h = Relative Frequency * N (N={N})")

plt.show()
```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_18_0.png)
    


## ä¼¯åŠªåˆ©éšæœºå˜é‡ï¼ˆBernouli RVï¼‰

åˆç§°**ä¸¤ç‚¹åˆ†å¸ƒ**æˆ–è€…**0-1åˆ†å¸ƒ**ã€‚**ç¦»æ•£**å‹éšæœºå˜é‡ã€‚

**æ•°å­¦å®šä¹‰**

è®¾ $X \sim \text{Bernouli}(p)$ï¼Œå…¶ä¸­ $0 \le p \le 1$ã€‚

* æ”¯æŒé›†ï¼ˆå–å€¼èŒƒå›´ï¼Œå€¼åŸŸï¼‰æ˜¯ï¼š

  $$
  k \in \{0, 1\}
  $$

* æ¯ä¸ªå€¼çš„æ¦‚ç‡æ˜¯ï¼š

  $$
  P(X = 1) = p \\
  P(X = 0) = 1 - p
  $$                                                        

* æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆPMFï¼‰ï¼š
  $$
  f(X=k) = P(X=k) = \left\{
  \begin{aligned}
  p, \text{if } k=1\\
  1-p, \text{if } k=0
  \end{aligned}
  \right.
  $$  

* ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF):
  $$
  F(X=k) = P(X\le k) = \left\{
  \begin{aligned}
  0,  \text{if } k \lt 0 \\
  1-p, \text{if } 0 \le k \lt 1\\
  1, \text{ for } k \ge 1
  \end{aligned}
  \right.
  $$ 

* æœŸæœ›ï¼ˆ$\mu$ï¼‰ï¼š$p$
* æ–¹å·®ï¼ˆ$\sigma^2$ï¼‰ï¼š$p(1-p)$
  

å‚è€ƒï¼š
- [ç»´åŸºç™¾ç§‘ï¼šä¼¯åŠªåˆ©åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Bernoulli_distribution)


```python
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯é‡å¤
np.random.seed(42)

# 1. å®šä¹‰å‚æ•°
p = 0.7
values = np.array([0, 1])  # ç¦»æ•£å–å€¼ï¼š1~6
n = len(values)
pmf = np.array([1-p, p])
cdf = np.cumsum(pmf)  # ç´¯ç§¯åˆ†å¸ƒå‡½æ•°

# 2. æŠ½æ ·
n_samples = 1000
samples = np.array([np.random.binomial(n=1, p=p) for _ in range(n_samples)])


# 3. å¯è§†åŒ–ï¼šç†è®ºåˆ†å¸ƒ + é‡‡æ ·é¢‘ç‡å¯¹æ¯”
plot_discrete_rv(values, pmf, cdf, samples, f"Bernoulli(p={p})")


```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_20_0.png)
    


### ä¼¯åŠªåˆ©å®šç†ï¼ˆBernouli Theormï¼‰
ä¼¯åŠªåˆ©å®šç†ï¼š æè¿°äº†æ¦‚ç‡ä¸é¢‘ç‡ä¹‹é—´çš„å…³ç³»ï¼Œå½“é‡å¤æ¬¡æ•°å¢åŠ æ—¶ï¼Œäº‹ä»¶çš„ç›¸å¯¹é¢‘ç‡ä¼šè¶‹è¿‘äºå…¶æ¦‚ç‡ã€‚

è®¾ $X_1, X_2, ..., X_n$ æ˜¯ $n$ æ¬¡ç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¼¯åŠªåˆ©éšæœºå˜é‡ï¼ˆå³ $X_i \sim \text{Bernoulli}(p)$ï¼‰ï¼Œä»¤

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i
$$

å³æˆåŠŸçš„ç›¸å¯¹é¢‘ç‡ã€‚

é‚£ä¹ˆï¼Œä¼¯åŠªåˆ©å®šç†å‘Šè¯‰æˆ‘ä»¬ï¼š

$$
\lim_{n \to \infty} \mathbb{P}\left( \left| \bar{X}_n - p \right| > \epsilon \right) = 0 \quad \text{å¯¹ä»»æ„ } \epsilon > 0
$$

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¼¯åŠªåˆ©å®šç†æ¥éªŒè¯æ¨¡æ‹Ÿé‡‡æ ·çš„æ­£ç¡®æ€§ï¼š
1. è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„çš„å®é™…é¢‘ç‡ $f_{a_i}$
2. è®¡ç®—æ¯ä¸ªäº‹ä»¶çš„ç›¸å¯¹é¢‘ç‡ $f_i = \frac{f_{a_i}}{N}$
3. å°†æ¯ä¸ªäº‹ä»¶çš„ç›¸å¯¹é¢‘ç‡ $f_i$ ä¸æ¦‚ç‡ $p$ è¿›è¡Œå¯¹æ¯”ã€‚æ ¹æ®ä¼¯åŠªåˆ©å®šç†ï¼Œå½“ $N$ å˜å¤§æ—¶ï¼Œ$f_i$ ä¼šé€¼è¿‘ $p$

**å‚è€ƒï¼š**
- [ProofWiki: Bernoulli's principle](https://proofwiki.org/wiki/Bernoulli%27s_Theorem)


```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML

# -----------------------------
# å‚æ•°è®¾ç½®
# -----------------------------
p = 0.3                # æˆåŠŸæ¦‚ç‡
n_trials = 10000        # æ€»å®éªŒæ¬¡æ•°
interval = 50          # åŠ¨ç”»é—´éš”æ—¶é—´ï¼ˆmsï¼‰

# -----------------------------
# ç”Ÿæˆä¼¯åŠªåˆ©å®éªŒæ•°æ®
# -----------------------------
np.random.seed(0)
samples = np.random.binomial(n=1, p=p, size=n_trials)
cumulative_freq = np.cumsum(samples) / np.arange(1, n_trials + 1)

# -----------------------------
# åˆ›å»ºå›¾å½¢
# -----------------------------
fig, ax = plt.subplots(figsize=(8, 4))
ax.set_ylim(0, 1)
ax.set_xlim(1, n_trials)
ax.axhline(y=p, color='red', linestyle='--', label=f'True Probability p = {p}')
line, = ax.plot([], [], lw=2, label='Empirical Frequency')
text = ax.text(0.05, 0.9, '', transform=ax.transAxes)
ax.set_xlabel('Number of Trials')
ax.set_ylabel('Success Frequency')
ax.set_title('Bernoulli Theorem Animation')
ax.legend()

# -----------------------------
# åŠ¨ç”»æ›´æ–°å‡½æ•°
# -----------------------------
def update(frame):
    x = np.arange(1, frame + 1)
    y = cumulative_freq[:frame]
    line.set_data(x, y)
    text.set_text(f'n = {frame}, freq = {y[-1]:.3f}')
    return line, text

# -----------------------------
# åˆ›å»ºåŠ¨ç”»
# -----------------------------
ani = FuncAnimation(fig, update, frames=np.arange(1, n_trials + 1, 10),
                    interval=interval, blit=True)
# Save as GIF
from matplotlib.animation import PillowWriter
ani.save("bernoulli_theorem_animation.gif", writer=PillowWriter(fps=10))

plt.close(fig)

from IPython.display import Image
Image(filename="bernoulli_theorem_animation.gif")

#plt.close()
# åœ¨ notebook ä¸­æ˜¾ç¤ºåŠ¨ç”»
#HTML(ani.to_jshtml())
```

![png](/img/contents/post/mcmc-statics/2_random-variables/bernoulli_theorem_animation.gif)



### é‡‡æ ·

#### åŸºäºå‡åŒ€åˆ†å¸ƒ Uniform(0, 1)

**ğŸŒ± åŸç†**

åˆ©ç”¨ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ $U \sim \text{Uniform}(0,1)$ æ¥å®ç°ï¼š

* å¦‚æœ $U < p$ï¼Œè¾“å‡º 1ï¼ˆæˆåŠŸï¼‰
* å¦åˆ™è¾“å‡º 0ï¼ˆå¤±è´¥ï¼‰

è¿™æ˜¯å› ä¸º Uniform(0,1) åœ¨åŒºé—´å†…æ˜¯å‡åŒ€çš„ï¼Œæ‰€ä»¥æ¦‚ç‡å°äº $p$ çš„é‚£ä¸€æ®µæ°å¥½å°±æ˜¯ â€œæˆåŠŸâ€çš„æ¦‚ç‡ã€‚


ğŸ¬ åŠ¨ç”»è¯´æ˜ï¼š
å·¦å›¾ï¼šSampling from Uniform(0,1)
* æ¯ä¸€å¸§ï¼Œç”Ÿæˆä¸€ä¸ª $Uâˆ¼Uniform(0,1)$ çš„æ ·æœ¬ï¼›
* è“è‰²ç‚¹æŒ‰éšæœºé«˜åº¦æ˜¾ç¤ºé‡‡æ ·å€¼ï¼›
* çº¢è‰²è™šçº¿è¡¨ç¤ºé˜ˆå€¼ $p=0.3$ï¼Œå³ï¼š
  * $U<p$ â‡’ ä¼¯åŠªåˆ©å€¼ä¸º 1ï¼›
  * $Uâ‰¥p$ â‡’ ä¼¯åŠªåˆ©å€¼ä¸º 0ã€‚

å³å›¾ï¼šBernoulli Sample Counts
* å®æ—¶æ›´æ–° 0 å’Œ 1 çš„å‡ºç°æ¬¡æ•°æŸ±çŠ¶å›¾ï¼›
* æœ€ç»ˆï¼Œ1 çš„æ•°é‡çº¦ä¸º 30%ï¼Œç¬¦åˆæ¦‚ç‡ $p=0.3$ï¼›
* 0 çš„æ•°é‡çº¦ä¸º 70%ã€‚


```python
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import numpy as np
from IPython.display import HTML

# Parameters
p = 0.3
n_frames = 100

# Pre-generate uniform samples
uniform_samples = np.random.uniform(0, 1, n_frames)
bernoulli_samples = (uniform_samples < p).astype(int)

# Set up the figure and axes
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
uniform_ax = axs[0]
bernoulli_ax = axs[1]

# Initialize plots
uniform_scatter = uniform_ax.scatter([], [], c='blue', alpha=0.6)
bernoulli_bar = bernoulli_ax.bar([0, 1], [0, 0], color='orange', edgecolor='black')

# Configure uniform axis
uniform_ax.axvline(p, color='red', linestyle='--', label=f'p = {p}')
uniform_ax.set_xlim(0, 1)
uniform_ax.set_ylim(0, 1)
uniform_ax.set_title('Sampling from Uniform(0,1)')
uniform_ax.set_xlabel('Value')
uniform_ax.set_ylabel('Random Height')
uniform_ax.legend()
uniform_ax.grid(True)

# Configure Bernoulli axis
bernoulli_ax.set_xlim(-0.5, 1.5)
bernoulli_ax.set_ylim(0, n_frames)
bernoulli_ax.set_title('Bernoulli Sample Counts')
bernoulli_ax.set_xlabel('Value')
bernoulli_ax.set_ylabel('Count')
bernoulli_ax.set_xticks([0, 1])
bernoulli_ax.grid(True)

# Store sample counts
count_0 = 0
count_1 = 0
x_vals = []
y_vals = []

# Animation update function
def update(frame):
    global count_0, count_1, x_vals, y_vals

    u = uniform_samples[frame]
    x_vals.append(u)
    y_vals.append(np.random.rand())  # random y position for scatter

    bern_sample = bernoulli_samples[frame]
    if bern_sample == 0:
        count_0 += 1
    else:
        count_1 += 1

    # Update scatter
    uniform_scatter.set_offsets(np.column_stack((x_vals, y_vals)))

    # Update bar chart
    bernoulli_bar[0].set_height(count_0)
    bernoulli_bar[1].set_height(count_1)

    return uniform_scatter, bernoulli_bar

# Create animation
ani = animation.FuncAnimation(fig, update, frames=n_frames, interval=100, blit=False)
# ä¿å­˜ä¸º GIF
from matplotlib.animation import PillowWriter
ani.save("sample_uniform_to_bernoulli.gif", writer=PillowWriter(fps=10))

plt.close(fig)

from IPython.display import Image
Image(filename="sample_uniform_to_bernoulli.gif")
#plt.close()
# åœ¨ notebook ä¸­æ˜¾ç¤ºåŠ¨ç”»
#HTML(ani.to_jshtml())
```

![png](/img/contents/post/mcmc-statics/2_random-variables/sample_uniform_to_bernoulli.gif)




```python
import random

def sample_bernoulli(p):
    U = random.random() # U ~ Uniform(0, 1)
    return 1 if U < p else 0

sample_bernoulli(0.7)
```




    1



##### éªŒè¯


```python
p, N = 0.7, 10000
samples = [sample_bernoulli(p) for _ in range(N)]
print(f"Empirical mean: {sum(samples)/len(samples):.3f}")  # åº”è¯¥æ¥è¿‘ 0.7
```

    Empirical mean: 0.703


#### ä½¿ç”¨ `numpy`


```python
import numpy as np

np.random.binomial(n=1, p=0.7)
```




    1



##### éªŒè¯


```python
p, N = 0.7, 10000
samples = [np.random.binomial(n=1, p=p) for _ in range(N)]
print(f"Empirical mean: {sum(samples)/len(samples):.3f}")  # åº”è¯¥æ¥è¿‘ 0.7
```

    Empirical mean: 0.702


## äºŒé¡¹éšæœºå˜é‡ï¼ˆBinomial Random Variableï¼‰

ä¸€ä¸ªéšæœºå˜é‡ $Xâˆ¼Binomial(n,p)$ï¼Œè¡¨ç¤ºåœ¨**é‡å¤**è¿›è¡Œ $n$ æ¬¡**ç‹¬ç«‹**çš„ä¼¯åŠªåˆ©è¯•éªŒï¼ˆæ¯æ¬¡æˆåŠŸæ¦‚ç‡ä¸º $p$ï¼‰ä¸­ï¼ŒæˆåŠŸï¼ˆè®°ä¸º1ï¼‰å‘ç”Ÿçš„**æ€»æ¬¡æ•°**ã€‚$X$ æ˜¯ç¦»æ•£éšæœºå˜é‡ã€‚

**æ•°å­¦å®šä¹‰**

è®¾ $X \sim \text{Binomial}(n, p)$ï¼ˆæˆ–è€…æ˜¯ $X \sim B(n,p)$ï¼‰ï¼Œå…¶ä¸­ $n \gt 0, 0 \le p \le 1$ã€‚

* æ”¯æŒé›†ï¼ˆå–å€¼èŒƒå›´ï¼Œå€¼åŸŸï¼‰æ˜¯ï¼š

  $$
  k \in \{0, \dots, n\}
  $$
  - è¡¨ç¤ºæˆåŠŸçš„æ¬¡æ•°

* æ¯ä¸ªå€¼çš„æ¦‚ç‡æ˜¯ï¼š

  $$
  P(X = 0) = (1-p)^n\\
  \dots \\
  P(X = k) = \begin{pmatrix} n \\ k \end{pmatrix}p^k(1-p)^{(n-k)} \\
  \dots \\
  P(X = n) = p^n
  $$  
  -                                                       

* æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆPMFï¼‰ï¼š
  $$
  f(X=k) = P(X=k) = \begin{pmatrix} n \\ k \end{pmatrix}p^k(1-p)^{(n-k)} \\
  å…¶ä¸­ï¼Œ\begin{pmatrix} n \\ k \end{pmatrix} = \frac{n!}{k!(n-k)!}
  $$  

* ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDF):
  $$
  F(X=k) = P(X\le k) = \sum _{i=0}^{\lfloor k\rfloor }{n \choose i}p^{i}(1-p)^{n-i}
  $$ 
  - where $\lfloor k\rfloor$ is the "floor" under k, i.e. the greatest integer less than or equal to k.



* æœŸæœ›ï¼ˆ$\mu$ï¼‰ï¼š$np$
  * If $X_{1},\ldots ,X_{n}$ are identical (and independent) Bernoulli random variables with parameter $p$, then $X = X_1 + \dots + X_n$ and $\operatorname {E} [X]=\operatorname {E} [X_{1}+\cdots +X_{n}]=\operatorname {E} [X_{1}]+\cdots +\operatorname {E} [X_{n}]=p+\cdots +p=np.$
* æ–¹å·®ï¼ˆ$\sigma^2$ï¼‰ï¼š$np(1-p)$
  * the variance of a sum of independent random variables = the sum of the variances

**å‚è€ƒï¼š**
- [Wiki: Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution)



```python
from scipy.stats import binom
import numpy as np

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯é‡å¤
np.random.seed(42)

# 1. å®šä¹‰å‚æ•°
n, p = 5, 0.3 # X ï½ Binomial(n, p)
values = range(n + 1)  # ç¦»æ•£å–å€¼ï¼š0, 1, ..., n
pmf = binom.pmf(values, n, p)
cdf = np.cumsum(pmf)  # ç´¯ç§¯åˆ†å¸ƒå‡½æ•°

# 2. æŠ½æ ·
n_samples = 1000
samples = np.random.binomial(n=n, p=p, size=n_samples)

# 3. å¯è§†åŒ–ï¼šç†è®ºåˆ†å¸ƒ + é‡‡æ ·é¢‘ç‡å¯¹æ¯”
plot_discrete_rv(values, pmf, cdf, samples, f'Binomial(n={n}, p={p})')


```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_34_0.png)
    


### é‡‡æ ·

é‡‡æ · **Binomial éšæœºå˜é‡** $X \sim \text{Binomial}(n, p)$ï¼ˆè¡¨ç¤ºä» $n$ æ¬¡ç‹¬ç«‹ä¼¯åŠªåˆ©è¯•éªŒä¸­æˆåŠŸæ¬¡æ•°ï¼‰çš„æ–¹æ³•å¾ˆå¤šï¼Œé€‚ç”¨äºä¸åŒå‚æ•°èŒƒå›´ã€‚ä¸‹é¢æ˜¯ç›®å‰å·²çŸ¥çš„æ‰€æœ‰ç»å…¸å’Œå®ç”¨æ–¹æ³•ï¼ŒæŒ‰ç±»å‹å½’ç±»æ•´ç†ï¼Œå¹¶æŒ‡å‡ºé€‚ç”¨æƒ…å†µã€‚

| æ–¹æ³•                                                               | ä¸»è¦æ€æƒ³                            | é€‚ç”¨èŒƒå›´                     | æ˜¯å¦ç²¾ç¡® | æ˜¯å¦å®¹æ˜“å®ç°    |
| ---------------------------------------------------------------- | ------------------------------- | ------------------------ | ---- | --------- |
| 1. **Bernoulli é‡å¤æ³•**                                             | å¯¹ $n$ æ¬¡ä¼¯åŠªåˆ©é‡‡æ ·å¹¶æ±‚å’Œ                 | ä»»æ„ $n, p$ï¼Œå°¤å…¶æ˜¯å° $n$       | âœ… ç²¾ç¡® | âœ… ç®€å•      |
| 2. **åå‡½æ•°é‡‡æ ·æ³•ï¼ˆInverse Transformï¼‰**                                 | ç”¨ CDF æ‰¾ç¬¬ä¸€ä¸ªä½¿ $F(k) â‰¥ u$ çš„ $k$    | å° $n$ æ—¶å¯è¡Œ                | âœ… ç²¾ç¡® | âš ï¸ æ˜“æ…¢     |
| 3. **è¡¨æŸ¥æ‰¾æ³•ï¼ˆCDF æŸ¥è¡¨ï¼‰**                                              | é¢„è®¡ç®—æ‰€æœ‰ $F(k)$ï¼Œå†æŸ¥                 | å° $n$ï¼ˆ<100ï¼‰              | âœ… ç²¾ç¡® | âœ… å¿«é€Ÿï¼ˆé¢„è®¡ç®—ï¼‰ |
| 4. **Rejection Sampling**                                        | æ„é€ æ˜“é‡‡æ ·çš„ proposal åˆ†å¸ƒå†æ‹’ç»           | ä¸­ç­‰ $n$ï¼Œæˆ–ç”¨äºæ¨¡æ‹Ÿ             | âœ… ç²¾ç¡® | âš ï¸ éš¾åº¦é«˜    |
| 5. **æ­£æ€è¿‘ä¼¼æ³•ï¼ˆNormal Approximationï¼‰**                               | ç”¨ $\mathcal{N}(np, np(1-p))$ è¿‘ä¼¼ | å¤§ $n$ï¼Œ$np(1-p) \ge 10$   | âŒ è¿‘ä¼¼ | âœ… éå¸¸å¿«     |
| 6. **Poisson è¿‘ä¼¼æ³•**                                               | å½“ $n$ å¤§ã€$p$ å°ï¼Œ$\lambda = np$    | $p \le 0.05$ï¼Œ$np \le 10$ | âŒ è¿‘ä¼¼ | âœ… å¿«é€Ÿ      |
| 7. **BTPE ç®—æ³•ï¼ˆFast Binomial by Kachitvichyanukul and Schmeiserï¼‰** | åˆ†æ®µæ‹’ç»é‡‡æ ·ï¼ˆç»å…¸é«˜æ•ˆï¼‰                    | ä»»æ„ $n, p$ï¼Œç‰¹åˆ«é€‚åˆå¤§ $n$      | âœ… ç²¾ç¡® | âš ï¸ å®ç°å¤æ‚   |
| 8. **æ¯”ç‰¹æ“ä½œæ³•ï¼ˆBit Trickï¼‰**                                          | ç”¨ä½æ“ä½œæ¨¡æ‹Ÿå¤šæ¬¡ä¼¯åŠªåˆ©                     | $n$ ä¸å¤§ï¼ˆå¦‚è’™ç‰¹å¡æ´›ï¼‰            | âœ… ç²¾ç¡® | âš ï¸ ç‰¹æ®Šä¼˜åŒ–   |
| 9. **Alias æ–¹æ³•ï¼ˆéä¸»æµï¼‰**                                             | ç¦»æ•£å˜é‡çš„é«˜æ•ˆé‡‡æ ·                       | å¾ˆå° $n$ ä¸”å¤§é‡é‡å¤é‡‡æ ·           | âœ… ç²¾ç¡® | âš ï¸ åˆå§‹åŒ–å¤æ‚  |


**ğŸ”§ æ¨èä½¿ç”¨**

| ä½¿ç”¨åœºæ™¯       | æ¨èæ–¹æ³•                                   |
| ---------- | -------------------------------------- |
| å­¦ä¹ ç†è§£åŸç†     | ä¼¯åŠªåˆ©é‡å¤é‡‡æ · / åå‡½æ•°é‡‡æ ·                        |
| å°æ ·æœ¬ã€å•æ¬¡é‡‡æ ·   | ä»»æ„ï¼ˆéƒ½å¿«ï¼‰                                 |
| å¤§æ ·æœ¬ã€é€Ÿåº¦ä¼˜å…ˆ   | æ­£æ€è¿‘ä¼¼æˆ– BTPEï¼ˆä½¿ç”¨ `numpy.random.binomial`ï¼‰ |
| å° $p$ã€ç¨€ç–äº‹ä»¶ | Poisson è¿‘ä¼¼                             |
| å¤šæ¬¡é‡å¤é‡‡æ ·     | ä½¿ç”¨æŸ¥è¡¨ / BTPE ç®—æ³•                         |


**âœ… æ€»ç»“å›¾ç¤ºï¼ˆæ¦‚å¿µæµç¨‹ï¼‰**

```text
Binomial(n, p)
â”‚
â”œâ”€â”€ å° n: ç›´æ¥ä¼¯åŠªåˆ©é‡å¤
â”œâ”€â”€ å° p: ç”¨ Poisson(np)
â”œâ”€â”€ å¤§ n, np(1-p)>10: ç”¨æ­£æ€(np, np(1-p))
â”œâ”€â”€ ä»»æ„ n,p: åå‡½æ•° or æŸ¥è¡¨
â””â”€â”€ é«˜æ•ˆå·¥ä¸šçº§: BTPE ç®—æ³•ï¼ˆNumPyï¼‰
```




```python
import matplotlib.pyplot as plt
from math import comb
def verify_binomial_sample(n, p, samples):
    """
    éªŒè¯äºŒé¡¹åˆ†å¸ƒé‡‡æ ·çš„æ­£ç¡®æ€§
    :param n: äºŒé¡¹åˆ†å¸ƒçš„è¯•éªŒæ¬¡æ•°
    :param p: æˆåŠŸæ¦‚ç‡
    :param samples: é‡‡æ ·ç»“æœåˆ—è¡¨
    :return: None
    """
    N = len(samples)
    empirical_mean = sum(samples) / N
    theoretical_mean = n * p
    print(f"Empirical mean: {empirical_mean:.3f}, Theoretical mean: {theoretical_mean:.3f}")
    
    empirical_variance = sum((x - empirical_mean) ** 2 for x in samples) / len(samples)
    theoretical_variance = n * p * (1 - p)
    print(f"Empirical variance: {empirical_variance:.3f}, Theoretical variance: {theoretical_variance:.3f}")

    # ç»Ÿè®¡é¢‘ç‡
    counts = [samples.count(k) / N for k in range(n + 1)]
    # è®¡ç®—ç†è®ºæ¦‚ç‡ï¼ˆPMFï¼‰
    theoretical = [comb(n, k) * (p ** k) * ((1 - p) ** (n - k)) for k in range(n + 1)]
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.bar(range(n + 1), counts, width=0.4, label='Sampled Frequency', color='skyblue', align='edge')
    plt.bar([k - 0.4 for k in range(n + 1)], theoretical, width=0.4, label='Theoretical PMF', color='orange', align='edge')
    plt.xlabel("Number of Successes")
    plt.ylabel("Probability")
    plt.title(f"Binomial Distribution: n={n}, p={p}")
    plt.legend()
    plt.grid(True)
    plt.show()
```

#### æ–¹æ³•ä¸€ï¼šä¼¯åŠªåˆ©é‡å¤æ³•ï¼ˆåŸºäºä¼¯åŠªåˆ©éšæœºå˜é‡ï¼‰

å¦‚æœä½ èƒ½ä»ä¼¯åŠªåˆ©åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œé‚£å°±å¯ä»¥é€šè¿‡**å¯¹ $n$ æ¬¡ä¼¯åŠªåˆ©é‡‡æ ·æ±‚å’Œ**ï¼Œå¾—åˆ°ä¸€ä¸ªäºŒé¡¹å¼æ ·æœ¬ã€‚

âœ… æ­¥éª¤æ¦‚è§ˆï¼š
1. åˆå§‹åŒ–è®¡æ•°å™¨ä¸º 0
2. é‡å¤ n æ¬¡ä»¥ä¸‹æ“ä½œï¼š
 - ä» [0, 1] ä¸­é‡‡æ ·ä¸€ä¸ªä¼¯åŠªåˆ©å€¼ï¼ˆæˆåŠŸçš„æ¦‚ç‡æ˜¯ $p$ï¼‰
 - å¦‚æœé‡‡æ ·ä¸º 1ï¼ˆè¡¨ç¤ºæˆåŠŸï¼‰ï¼Œåˆ™è®¡æ•°å™¨åŠ ä¸€
3. æœ€ç»ˆçš„è®¡æ•°å™¨æ•°å€¼å°±æ˜¯ä¸€æ¬¡äºŒé¡¹åˆ†å¸ƒçš„é‡‡æ ·å€¼


ä½¿ç”¨ NumPy å‘é‡åŒ–é‡‡æ ·ï¼š
```py
import numpy as np
np.random.binomial(n=10, p=0.5, size=1000)
```


```python
import random

def sample_binomial_mimic(n, p):
    count = 0
    for _ in range(n): # repeat N times Bernouli sampling
        u = random.random()  # ä» [0,1) ä¸­é‡‡æ ·ä¸€ä¸ªå‡åŒ€å˜é‡
        if 1-p <= u < 1:
            count += 1       # æˆåŠŸå°±åŠ ä¸€
    return count

def sample_binomial_mimic_list(n, p, num_samples):
    return [sample_binomial_mimic(n, p) for _ in range(num_samples)]
```


```python
n, p, num_samples = 10, 0.5, 10
sample_binomial_mimic_list(n, p, num_samples)
```




    [2, 5, 6, 4, 4, 4, 7, 4, 8, 5]



##### Visualize


```python
import matplotlib.pyplot as plt

n, p, num_samples = 10, 0.3, 1000
samples = sample_binomial_mimic_list(n, p, num_samples)
plt.hist(samples, bins=range(12), align='left', rwidth=0.8, color='skyblue')
plt.xlabel("Number of Successes")
plt.ylabel("Frequency")
plt.title("Sampling from Binomial(n=10, p=0.3)")
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_41_0.png)
    


##### éªŒè¯

éªŒè¯çš„æ€è·¯æ˜¯ï¼šé‡‡å¾ˆå¤šæ ·æœ¬ï¼Œç„¶åç”»å‡ºç›´æ–¹å›¾ï¼Œå†ä¸ç†è®ºçš„äºŒé¡¹å¼æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆPMFï¼‰å¯¹æ¯”ã€‚


```python
# é‡‡æ ·
n = 10
p = 0.3
N = 10000  # é‡‡æ ·æ¬¡æ•°
samples = sample_binomial_mimic_list(n, p, N)
#print(f"Empirical mean: {sum(samples)/len(samples):.3f}")  # åº”è¯¥æ¥è¿‘ np = 10x0.4 = 4.0

verify_binomial_sample(n, p, samples)
```

    Empirical mean: 2.997, Theoretical mean: 3.000
    Empirical variance: 2.153, Theoretical variance: 2.100



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_43_1.png)
    


#### æ–¹æ³•äºŒï¼šåå‡½æ•°é‡‡æ ·æ³•ï¼ˆInverse Transformï¼‰
å®ƒæ˜¯ä¸€ç§éå¸¸ç»å…¸çš„ã€**ä¸ä¾èµ–äºé‡å¤æ¨¡æ‹Ÿä¼¯åŠªåˆ©åˆ†å¸ƒ**çš„é‡‡æ ·æ–¹æ³•ã€‚ä¸è¿‡å®ƒ**ä¸å¤ªé€‚åˆé«˜ n çš„ Binomial**ï¼Œå› ä¸ºå®ƒæ¶‰åŠç´¯ç§¯æ¦‚ç‡çš„æŸ¥æ‰¾å’Œæœç´¢ï¼Œä½†åœ¨ç†è®ºä¸Šï¼Œå®ƒæ˜¯å®Œå…¨å¯è¡Œçš„ï¼Œè€Œä¸” **æ²¡æœ‰ç”¨åˆ°ä¼¯åŠªåˆ©éšæœºå˜é‡**ï¼

**åŸç†æ­¥éª¤å¦‚ä¸‹ï¼š**
1. ç”Ÿæˆä¸€ä¸ªå‡åŒ€éšæœºæ•° $u \sim \text{Uniform}(0, 1)$
2. ä¾æ¬¡ç´¯åŠ äºŒé¡¹å¼åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆPMFï¼‰ï¼š

   $$
   F(k) = \sum_{i=0}^{k} P(X=i)
   $$
3. æ‰¾åˆ°ç¬¬ä¸€ä¸ª $k$ï¼Œä½¿å¾— $F(k) \ge u$
4. è¿”å›è¿™ä¸ª $k$ï¼Œä½œä¸ºé‡‡æ ·ç»“æœ

è¿™å°±å®Œæˆäº†ä¸€æ¬¡é‡‡æ ·ã€‚


**ä¼˜ç¼ºç‚¹åˆ†æ**

| ä¼˜ç‚¹       | ç¼ºç‚¹                        |
| -------- | ------------------------- |
| ç†è®ºé€šç”¨æ€§å¼º   | å¯¹é«˜ç»´ã€é«˜ n çš„äºŒé¡¹åˆ†å¸ƒæ•ˆç‡è¾ƒä½ï¼ˆè¦è®¡ç®—å¾ˆå¤šé¡¹ï¼‰ |
| ä¸éœ€è¦æ¨¡æ‹Ÿä¼¯åŠªåˆ© | æ¯æ¬¡é‡‡æ ·éƒ½è¦é‡æ–°ä» k=0 å¼€å§‹æ‰«æ        |
| å¯ç”¨äºç²¾ç¡®çš„é‡‡æ · | æ— æ³•å‘é‡åŒ–ï¼Œä¸èƒ½ç›´æ¥å¹¶è¡ŒåŠ é€Ÿ            |

**é€‚åˆç”¨åœ¨ï¼š**
* $n$ è¾ƒå°çš„æƒ…å†µï¼ˆå¦‚ $n \leq 20$ï¼‰
* æ•™å­¦å’Œç†è®ºéªŒè¯ç›®çš„


**ğŸ“Œ æ€»ç»“**

| æ–¹æ³•         | æ˜¯å¦ç”¨ä¼¯åŠªåˆ© | é€Ÿåº¦    | é€‚ç”¨æƒ…å†µ        |
| ---------- | ------ | ----- | ----------- |
| ç´¯åŠ ä¼¯åŠªåˆ©      | âœ…      | å¿«ï¼ˆç®€å•ï¼‰ | é€šç”¨ã€å° n      |
| é€†å˜æ¢é‡‡æ ·      | âŒ      | ä¸­ç­‰    | ç†è®ºéªŒè¯ã€éå‘é‡åŒ–åœºæ™¯ |
| æ­£æ€/æ³Šæ¾è¿‘ä¼¼    | âŒ      | å¿«     | n å¤§ã€p å°/ä¸­   |
| è¡¨æŸ¥æ³• + äºŒåˆ†æŸ¥æ‰¾ | âŒ      | éå¸¸å¿«   | å›ºå®š nï¼Œå¤šæ¬¡é‡‡æ ·   |

##### è¾…åŠ©ç¤ºæ„å›¾ç†è§£

å¦‚æœä½ ç”»å‡º Binomial(n=5, p=0.5) çš„ PMFï¼š

| k | P(X=k)  | ç´¯ç§¯å’Œ F(k) |
| - | ------- | -------- |
| 0 | 0.03125 | 0.03125  |
| 1 | 0.15625 | 0.1875   |
| 2 | 0.3125  | 0.5      |
| 3 | 0.3125  | 0.8125   |
| 4 | 0.15625 | 0.96875  |
| 5 | 0.03125 | 1.0      |

å¦‚æœä½ ç”Ÿæˆä¸€ä¸ª $u = 0.4$ï¼Œä½ ä¼šå‘ç° $F(2) = 0.5 \ge 0.4$ï¼Œé‚£ä¹ˆå°±é‡‡æ ·å‡º $k = 2$ã€‚


```python
import matplotlib.pyplot as plt
from math import comb
import numpy as np
import random

# å‚æ•°
n = 10
p = 0.4

# è®¡ç®— PMF å’Œ CDF
k_vals = list(range(n + 1))
pmf = [comb(n, k) * p**k * (1 - p)**(n - k) for k in k_vals]
cdf = np.cumsum(pmf)

####### é‡‡æ · ####### 
# ç”Ÿæˆä¸€ä¸ªéšæœºæ•° u
u = 0.52  # ä¹Ÿå¯ä»¥æ”¹æˆ random.random()

# æ‰¾åˆ°ç¬¬ä¸€ä¸ª cdf[k] >= u
k_selected = None
for k, value in enumerate(cdf):
    if u <= value:
        k_selected = k
        break

####### ç»˜åˆ¶éªŒè¯ ####### 
# ç»˜å›¾
plt.figure(figsize=(10, 6))

# PMF å›¾ï¼ˆæŸ±çŠ¶å›¾ï¼‰
plt.bar(k_vals, pmf, alpha=0.6, label='PMF: P(X = k)', color='skyblue')

# CDF å›¾ï¼ˆé˜¶æ¢¯çº¿ï¼‰
plt.step(k_vals, cdf, where='mid', color='orange', label='CDF: P(X â‰¤ k)', linewidth=2)

# ç”»å‡º u å’Œå¯¹åº”çš„ k_selected
plt.axhline(y=u, color='red', linestyle='--', label=f'u = {u:.2f}')
plt.axvline(x=k_selected, color='green', linestyle='--', label=f'sampled k = {k_selected}')

# æ ‡æ³¨
plt.title(f"Inverse Transform Sampling for Binomial(n={n}, p={p})")
plt.xlabel("k")
plt.ylabel("Probability")
plt.legend()
plt.grid(True)
plt.xticks(k_vals)
plt.ylim(0, 1.05)

plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_46_0.png)
    


##### ä»£ç å®ç°


```python
import random
from math import comb

def sample_binomial_inverse(n, p):
    # Step 1: generate a uniform random number
    u = random.random()
    
    # Step 2: initialize cumulative probability
    cumulative = 0.0

    for k in range(n + 1):
        prob = comb(n, k) * (p ** k) * ((1 - p) ** (n - k))
        cumulative += prob
        if u <= cumulative:
            return k

    return n  # fallback

def sample_binomial_inverse_list(n, p, num_samples):
    return [sample_binomial_inverse(n, p) for _ in range(num_samples)]
```


```python
n, p, num_samples = 10, 0.5, 10
sample_binomial_inverse_list(n, p, num_samples)
```




    [3, 4, 2, 3, 5, 5, 7, 2, 4, 8]



##### å¯è§†åŒ–


```python
import matplotlib.pyplot as plt

n, p, num_samples = 10, 0.3, 1000
samples = sample_binomial_inverse_list(n, p, num_samples)

plt.hist(samples, bins=range(12), align='left', rwidth=0.8, color='skyblue')
plt.xlabel("Number of Successes")
plt.ylabel("Frequency")
plt.title("Sampling from Binomial(n=10, p=0.3)")
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_51_0.png)
    


##### éªŒè¯


```python
# é‡‡æ ·
n = 10
p = 0.3
N = 10000  # é‡‡æ ·æ¬¡æ•°
samples = sample_binomial_inverse_list(n, p, N)
#print(f"Empirical mean: {sum(samples)/len(samples):.3f}")  # åº”è¯¥æ¥è¿‘ np = 10x0.4 = 4.0

verify_binomial_sample(n, p, samples)
```

    Empirical mean: 3.011, Theoretical mean: 3.000
    Empirical variance: 2.097, Theoretical variance: 2.100



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_53_1.png)
    


#### æ–¹æ³•ä¸‰ï¼šè¡¨æŸ¥æ‰¾æ³•ï¼ˆCDF æŸ¥è¡¨ï¼‰


```python
import random
from math import comb

binomial_table, binomial_table_n, binomial_table_p = None, None, None
def generate_binomial_table(n, p):
    """
    ç”ŸæˆäºŒé¡¹åˆ†å¸ƒçš„ CDF æŸ¥æ‰¾è¡¨
    :param n: äºŒé¡¹åˆ†å¸ƒçš„è¯•éªŒæ¬¡æ•°
    :param p: æˆåŠŸæ¦‚ç‡
    :return: None
    """
    global binomial_table
    binomial_table = {}
    cumulative = 0.0
    for k in range(n + 1):
        prob = comb(n, k) * (p ** k) * ((1 - p) ** (n - k))
        cumulative += prob
        binomial_table[k] = cumulative

def sample_binomial_table(n, p):
    # Step 1: generate a uniform random number
    u = random.random()
    
    # Step 2: look up in the CDF table
    if binomial_table is None or binomial_table_n != n or binomial_table_p != p:
        generate_binomial_table(n, p)
    # Step 3: find the first k such that CDF[k] >= u
    for k in range(n + 1):
        if u <= binomial_table[k]:
            return k
    # If no k found, return n
    return n  # fallback

def sample_binomial_table_list(n, p, num_samples):
    return [sample_binomial_table(n, p) for _ in range(num_samples)]
```

##### éªŒè¯ 


```python
# é‡‡æ ·
n = 10
p = 0.3
N = 10000  # é‡‡æ ·æ¬¡æ•°
samples = sample_binomial_table_list(n, p, N)
#print(f"Empirical mean: {sum(samples)/len(samples):.3f}")  # åº”è¯¥æ¥è¿‘ np = 10x0.4 = 4.0

verify_binomial_sample(n, p, samples)
```

    Empirical mean: 3.002, Theoretical mean: 3.000
    Empirical variance: 2.100, Theoretical variance: 2.100



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_57_1.png)
    


#### æ–¹æ³•å››ï¼šæ‹’ç»-é‡‡æ ·æ³•ï¼ˆRejection Samplingï¼‰
ä½¿ç”¨ **Rejection Samplingï¼ˆæ¥å—-æ‹’ç»é‡‡æ ·ï¼‰æ¥é‡‡æ ·**ä¸€ä¸ª**äºŒé¡¹å¼éšæœºå˜é‡**ï¼Œå…¶å®å¹¶ä¸æ˜¯æœ€å¸¸è§çš„æ–¹æ³•ï¼ˆæ›´å¸¸ç”¨çš„æ˜¯ç›´æ¥é‡‡æ ·æˆ–ç”¨æ³Šæ¾/æ­£æ€è¿‘ä¼¼ï¼‰ï¼Œä½†å®ƒåœ¨ç†è®ºä¸Šæ˜¯å¯è¡Œçš„ï¼Œå°¤å…¶åœ¨éš¾ä»¥ç›´æ¥é‡‡æ ·æˆ–éœ€è¦ä»æˆªæ–­/ç½•è§å‚æ•°çš„äºŒé¡¹å¼åˆ†å¸ƒä¸­é‡‡æ ·æ—¶ã€‚

**âœ… proposal åˆ†å¸ƒçš„ç†æƒ³é€‰æ‹©ï¼šå‡åŒ€åˆ†å¸ƒ $g(k) = \frac{1}{n+1}$**

* ç®€å•æ˜“å®ç°ï¼›
* ä½†è‹¥ç›®æ ‡åˆ†å¸ƒæ˜¯åæ–œçš„ï¼ˆå¦‚ $p=0.05$ï¼‰ï¼Œé‚£ä¹ˆå¤§å¤šæ•° sample ä¼šè¢«æ‹’ç»ï¼ˆæ•ˆç‡å·®ï¼‰ï¼›
* ä»ç„¶æ˜¯æ•™å­¦ä¸­æœ€å¸¸ç”¨ç¤ºä¾‹ã€‚


**âš ï¸ æ³¨æ„äº‹é¡¹ï¼š**
* **æ•ˆç‡å–å†³äº M**ï¼šå¦‚æœ $p$ å¾ˆå°æˆ–å¾ˆå¤§ï¼Œ$f(k)$ ä¼šéå¸¸åï¼Œå¯¼è‡´å¾ˆä½çš„æ¥å—ç‡ï¼›
* è‹¥æƒ³æ›´é«˜æ•ˆï¼Œå¯ä½¿ç”¨**ç¦»æ•£é«˜æ–¯åˆ†å¸ƒã€æ³Šæ¾åˆ†å¸ƒã€æ­£æ€è¿‘ä¼¼**ç­‰ä½œä¸ºæ›´â€œè´´è¿‘â€ç›®æ ‡çš„ proposal åˆ†å¸ƒï¼›
* Rejection Sampling çš„é€šç”¨æ€§å¼ºï¼Œä½†ä¸æ˜¯é‡‡æ · Binomial çš„é¦–é€‰æ–¹æ³•ï¼ˆé¦–é€‰æ˜¯ç›´æ¥ç®—æ³•ã€æˆ–æ­£æ€/æ³Šæ¾è¿‘ä¼¼ï¼‰ã€‚

**âœ… æ€»ç»“**

| æ­¥éª¤      | å†…å®¹                                                     |
| ------- | ------------------------------------------------------ |
| ğŸ¯ ç›®æ ‡   | ä» Binomial(n, p) ä¸­é‡‡æ ·                                   |
| ğŸ§° æ–¹æ³•   | æ„é€  proposal $g(k)$ï¼Œæ»¡è¶³ $f(k) \leq M g(k)$               |
| ğŸ“ é‡‡æ ·æœºåˆ¶ | é‡‡æ · $k \sim g(k)$ï¼Œä»¥æ¦‚ç‡ $\alpha = \frac{f(k)}{M g(k)}$ æ¥å— |
| ğŸ“‰ ç¼ºç‚¹   | æ•ˆç‡å— $p$ å’Œ $M$ å½±å“è¾ƒå¤§ï¼Œé‡‡æ ·è¾ƒæ…¢                                |
| ğŸ§  ä¼˜åŠ¿   | æ€è·¯ç®€å•ã€åˆ†å¸ƒé€šç”¨æ€§å¼º                                            |


```python
import numpy as np
from scipy.stats import binom
import matplotlib.pyplot as plt

def rejection_sample_binomial(n, p, N_samples=1000):
    samples = []

    # ç›®æ ‡åˆ†å¸ƒï¼šBin(n, p)
    bin_pmf = [binom.pmf(k, n, p) for k in range(n+1)]

    # Proposal åˆ†å¸ƒï¼šUniform(0, n)
    g = 1.0 / (n + 1)

    # æ‰¾æœ€å¤§å€¼ç”¨äºè®¡ç®— M
    M = max(bin_pmf) / g

    while len(samples) < N_samples:
        k = np.random.randint(0, n+1)
        u = np.random.uniform(0, 1)

        accept_prob = bin_pmf[k] / (M * g)
        if u < accept_prob:
            samples.append(k)

    return samples
```


```python
# éªŒè¯
# é‡‡æ ·
n = 10
p = 0.3
N = 10000  # é‡‡æ ·æ¬¡æ•°
samples = rejection_sample_binomial(n, p, N)
#print(f"Empirical mean: {sum(samples)/len(samples):.3f}")  # åº”è¯¥æ¥è¿‘ np = 10x0.4 = 4.0

verify_binomial_sample(n, p, samples)
```

    Empirical mean: 3.014, Theoretical mean: 3.000
    Empirical variance: 2.120, Theoretical variance: 2.100



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_60_1.png)
    


#### æ–¹æ³•äº”ï¼šæ­£æ€è¿‘ä¼¼æ³•ï¼ˆNormal Approximationï¼‰

**åŸç†ï¼šDe Moivreâ€“Laplace ä¸­å¿ƒæé™å®šç†**

å½“ $n$ è¶³å¤Ÿå¤§æ—¶ï¼ŒäºŒé¡¹å¼åˆ†å¸ƒ $X \sim \text{Bin}(n, p)$ å¯ç”±æ­£æ€åˆ†å¸ƒè¿‘ä¼¼ï¼š

$$
X \approx Y \sim \mathcal{N}(\mu, \sigma^2)
$$

å…¶ä¸­ï¼š

* å‡å€¼ï¼š$\mu = np$
* æ–¹å·®ï¼š$\sigma^2 = np(1-p)$

è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä»è¯¥æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªå€¼ä½œä¸ºè¿‘ä¼¼ã€‚

**é€‚ç”¨æ¡ä»¶ï¼š**
* $n$ è¦è¶³å¤Ÿå¤§
* $p$ ä¸è¦å¤ªæ¥è¿‘ 0 æˆ– 1

**é€šå¸¸ç»éªŒæ³•åˆ™æ˜¯ï¼š**

$$
np \geq 10 \quad \text{ä¸”} \quad n(1-p) \geq 10
$$


**é‡‡æ ·æ–¹æ³•**
1. ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ $Z \sim \mathcal{N}(0, 1)$ é‡‡æ ·ï¼›
2. æ„é€  $Y = np + \sqrt{np(1-p)} \cdot Z$
3. å°† $Y$ å››èˆäº”å…¥å¾—åˆ°æ•´æ•° $k$ï¼Œå¹¶è£å‰ªåˆ°åˆæ³•èŒƒå›´ $[0, n]$

**è¯¯å·®åˆ†æ**

| é¡¹ç›®       | æè¿°                                 |
| -------- | ---------------------------------- |
| **è¯¯å·®æ¥æº** | è¿ç»­ â†’ ç¦»æ•£ã€è¿‘ä¼¼ tail åå·®                 |
| **æ•ˆæœæœ€å¥½** | $p \approx 0.5$ ä¸” $n$ è¶³å¤Ÿå¤§          |
| **åæ€æƒ…å†µ** | å½“ $p \ll 0.5$ æˆ– $p \gg 0.5$ æ—¶å°¾éƒ¨è¯¯å·®å¤§ |
| **æ”¹è¿›æ–¹æ³•** | åŠ  continuity correctionï¼ˆè§ä¸‹ï¼‰        |


**ğŸ”§ Continuity Correctionï¼ˆè¿ç»­æ€§ä¿®æ­£ï¼‰**

ç”±äºæ­£æ€åˆ†å¸ƒæ˜¯è¿ç»­çš„ï¼Œè€ŒäºŒé¡¹å¼æ˜¯ç¦»æ•£çš„ï¼Œ**è¿ç»­æ€§ä¿®æ­£**å¯ç•¥å¾®æå‡ç²¾åº¦ï¼š

* å°†ç¦»æ•£å€¼ $k$ æ˜ å°„åˆ° $[k - 0.5, k + 0.5]$ çš„åŒºé—´ä¸Šï¼›
* é‡‡æ ·æ—¶åŠ  0.5 æˆ– -0.5 æŠµæ¶ˆè¯¯å·®ï¼š

$$
X \approx \mathcal{N}(np, np(1-p)) \Rightarrow X' = \text{round}(Y + 0.5)
$$

ä»£ç ä¸­å¯å°è¯• `samples = np.floor(samples + 0.5).astype(int)`ã€‚


```python
import numpy as np
from scipy.stats import binom
import matplotlib.pyplot as plt

def normal_approx_binomial(n, p, N_samples=10000):
    mu = n * p
    sigma = np.sqrt(n * p * (1 - p))
    
    # é‡‡æ ·
    samples = np.random.normal(loc=mu, scale=sigma, size=N_samples)
    
    # å››èˆäº”å…¥å¹¶è£å‰ª
    samples = np.round(samples).astype(int)
    samples = np.clip(samples, 0, n)
    
    return samples.tolist()
```


```python
# éªŒè¯
# é‡‡æ ·
n = 10
p = 0.3
N = 10000  # é‡‡æ ·æ¬¡æ•°
samples = normal_approx_binomial(n, p, N)
#print(f"Empirical mean: {sum(samples)/len(samples):.3f}")  # åº”è¯¥æ¥è¿‘ np = 10x0.4 = 4.0

verify_binomial_sample(n, p, samples)
```

    Empirical mean: 3.019, Theoretical mean: 3.000
    Empirical variance: 2.116, Theoretical variance: 2.100



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_63_1.png)
    


## æŒ‡æ•°éšæœºå˜é‡ï¼ˆExponential Random Variablesï¼‰

**è¿ç»­å‹**éšæœºå˜é‡ã€‚**æŒ‡æ•°åˆ†å¸ƒ**å¯ä»¥ç”¨æ¥å»ºæ¨¡å¹³å‡å‘ç”Ÿç‡æ’å®šã€è¿ç»­ã€ç‹¬ç«‹çš„äº‹ä»¶å‘ç”Ÿçš„é—´éš”ã€‚

**æ•°å­¦å®šä¹‰**

å¯¹äºä¸€ä¸ªå‚æ•°ä¸º $\lambda > 0$ çš„æŒ‡æ•°åˆ†å¸ƒ $X \sim \text{Exp}(\lambda)$ï¼š

* æ”¯æŒé›†ï¼ˆå–å€¼èŒƒå›´ï¼Œå€¼åŸŸï¼‰æ˜¯ï¼š$X \in [0, \infty]$                                                   

* æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ï¼š

  $$
  f_X(x) = \begin{cases}
    \lambda e^{-\lambda x}, & x \geq 0 \\
    0, & x < 0
    \end{cases}
  $$

* ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ï¼š

  $$
  F_X(x) = \begin{cases}
    1 - e^{-\lambda x}, & x \geq 0 \\
    0, & x < 0
    \end{cases}
  $$

* æœŸæœ›ï¼ˆ$\mu$ï¼‰ï¼š$\frac{1}{\lambda}$
* æ–¹å·®ï¼ˆ$\sigma^2$ï¼‰ï¼š$\frac{1}{\lambda^2}$

**å‚è€ƒï¼š**
- [Wiki: Exponential distribution](https://en.wikipedia.org/wiki/Exponential_distribution)
- [ProofWiki: Expectation of Exponential Distribution](https://proofwiki.org/wiki/Expectation_of_Exponential_Distribution)
- [ProofWiki: Variance of Exponential Distribution](https://proofwiki.org/wiki/Variance_of_Exponential_Distribution)


```python
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
np.random.seed(42)

# æŒ‡æ•°åˆ†å¸ƒå‚æ•° Î»ï¼ˆé€Ÿç‡å‚æ•°ï¼‰
lambda_val = 12

# é‡‡æ ·æ•°é‡
n_samples = 10000

# ä½¿ç”¨ numpy é‡‡æ ·
samples = np.random.exponential(scale=1/lambda_val, size=n_samples)

# åˆ›å»ºå›¾å½¢
plt.figure(figsize=(8, 5))

# ç»˜åˆ¶é‡‡æ ·çš„ç›´æ–¹å›¾ï¼ˆå½’ä¸€åŒ–ä¸ºå¯†åº¦ï¼‰
plt.hist(samples, bins=50, density=True, alpha=0.6, color='skyblue', label='Sampled Histogram')

# ç”Ÿæˆç†è®ºæ›²çº¿
x_vals = np.linspace(0, np.max(samples), 200)
pdf = lambda_val * np.exp(-lambda_val * x_vals)
plt.plot(x_vals, pdf, 'r-', lw=2, label=f'Theoretical PDF (Î»={lambda_val})')
a, b = 2, 1
y_vals = a+b*x_vals
ypdf = a*pdf+b
plt.plot(y_vals, pdf, 'b-', lw=2, label=f'Theoretical PDF (Î»={lambda_val};y={a}*x+{b})')

# å›¾å½¢è®¾ç½®
plt.title('Exponential Distribution Visualization')
plt.xlabel('x')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_65_0.png)
    


### é‡‡æ ·

#### ä½¿ç”¨åå‡½æ•°æ³•ï¼ˆInverse Transform Samplingï¼‰


å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ $Uâˆ¼Uniform(0,1)$ï¼Œå¯ä»¥ç”¨åå‡½æ•°æ³•æ¥é‡‡æ ·æŒ‡æ•°éšæœºå˜é‡ï¼š

$ X = -\frac{ln(1-U)}{\lambda}$

**æ¨å¯¼**
$$
F(x) = 1 - e^{-\lambda x} = u\\
1 - u = e^{-\lambda x} \\
ln(1-u) = -\lambda x \\
x = -\frac{ln(1-u)}{\lambda}

$$

ğŸ‘‰ è¿™ä¸ªå…¬å¼éå¸¸å¸¸è§ï¼Œä¹Ÿæ˜¯åœ¨æ¨¡æ‹ŸæŒ‡æ•°åˆ†å¸ƒæ—¶çš„åŸºæœ¬æ–¹æ³•ã€‚

> ä¹Ÿå¯ä»¥ç”¨ `numpy.random.exponential(scale=1/lambda_val, size=n_samples)`


```python
import random
import math

def sample_exponential_inverse(lambda_val):
    """
    é‡‡æ ·æŒ‡æ•°åˆ†å¸ƒ
    :param lambda_val: æŒ‡æ•°åˆ†å¸ƒçš„é€Ÿç‡å‚æ•° Î»
    :param n_samples: é‡‡æ ·æ•°é‡
    :return: é‡‡æ ·ç»“æœ
    """
    u = random.random()  # ä» [0, 1) ä¸­é‡‡æ ·ä¸€ä¸ªå‡åŒ€å˜é‡
    return -math.log(1-u) / lambda_val  # ä½¿ç”¨é€†å˜æ¢é‡‡æ ·å…¬å¼ X = -ln(1-U)/Î»
    
def sample_exponential_inverse_list(lambda_val, n_samples):
    return [sample_exponential_inverse(lambda_val) for _ in range(n_samples)]
```


```python
import numpy as np
import matplotlib.pyplot as plt

# å‚æ•°è®¾ç½®
lambda_val = 1.5
n = 10000

# ä½¿ç”¨åå‡½æ•°æ³•é‡‡æ ·
X = sample_exponential_inverse_list(lambda_val, n)

# éªŒè¯ï¼šç»˜å›¾
plt.hist(X, bins=100, density=True, alpha=0.6, label='Sampled Histogram')
x_vals = np.linspace(0, 5, 200)
plt.plot(x_vals, lambda_val * np.exp(-lambda_val * x_vals), 'r-', label='True PDF')
plt.xlabel('X')
plt.ylabel('PDF')
plt.title('Exponential Distribution (Î» = {:.1f}, n = {})'.format(lambda_val, n))
plt.legend()
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_68_0.png)
    


## å‡ ä½•éšæœºå˜é‡ï¼ˆGeometric Random Variablesï¼‰

### æ¦‚è¿°
å®ƒæ˜¯**ç¦»æ•£å‹**éšæœºå˜é‡ä¸­éå¸¸ç»å…¸çš„ä¸€ä¸ªï¼Œå‡ºç°åœ¨å¾ˆå¤šè¯•éªŒç›´åˆ°ç¬¬ä¸€æ¬¡æˆåŠŸçš„æƒ…æ™¯ä¸­ã€‚


ä¸€ä¸ª**å‡ ä½•éšæœºå˜é‡** $X \sim \text{Geometric}(p)$ï¼Œè¡¨ç¤ºï¼š

> **åœ¨ç‹¬ç«‹é‡å¤ä¼¯åŠªåˆ©è¯•éªŒä¸­ï¼Œç¬¬ä¸€æ¬¡æˆåŠŸå‡ºç°çš„è¯•éªŒç¼–å·ã€‚**

* æ¯æ¬¡è¯•éªŒéƒ½æ˜¯ç‹¬ç«‹çš„ï¼ŒæˆåŠŸæ¦‚ç‡æ˜¯ $p \in (0,1)$
* $X \in \{1, 2, 3, \dots\}$


**ğŸ“Œ æ¦‚ç‡è´¨é‡å‡½æ•°ï¼ˆPMFï¼‰**

å‡ ä½•åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°ä¸ºï¼š

$$
P(X = k) = (1 - p)^{k - 1} \cdot p, \quad \text{for } k = 1, 2, 3, \dots
$$

è¿™è¡¨ç¤ºï¼š

* ç¬¬ $k-1$ æ¬¡éƒ½å¤±è´¥ï¼ˆæ¦‚ç‡ä¸º $(1-p)^{k-1}$ï¼‰ï¼›
* ç¬¬ $k$ æ¬¡æˆåŠŸï¼ˆæ¦‚ç‡ä¸º $p$ï¼‰ã€‚


**ğŸ“Š ä¸¾ä¸ªä¾‹å­**

æ¯”å¦‚ä½ åœ¨æŠ›ä¸€ä¸ªä¸å…¬å¹³çš„ç¡¬å¸ï¼Œæ­£é¢ï¼ˆæˆåŠŸï¼‰çš„æ¦‚ç‡æ˜¯ $p = 0.3$ï¼Œä½ åœ¨ç­‰ç¬¬ä¸€æ¬¡å‡ºç°æ­£é¢ï¼š

* $P(X = 1) = 0.3$ ï¼ˆç¬¬ä¸€æ¬¡å°±æ­£é¢ï¼‰
* $P(X = 2) = 0.7 \cdot 0.3 = 0.21$ï¼ˆç¬¬ä¸€æ¬¡åé¢ï¼Œç¬¬äºŒæ¬¡æ­£é¢ï¼‰
* $P(X = 3) = 0.7^2 \cdot 0.3 = 0.147$

**ğŸ“ ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰**

$$
P(X \leq k) = 1 - (1 - p)^k, k=1,2, \dots
$$

æ¨å¯¼è¿‡ç¨‹ï¼š
$$
F(k) = P(X \leq k) \\
= \sum^k_{i=1}P(X=i) \\
= \sum^k_{i=1}(1 - p)^{i - 1} \cdot p \\
= p\cdot\sum^k_{i=1}(1 - p)^{i - 1}
$$

ä»¤ $j=i-1$ï¼Œåˆ™ï¼š
$$
F(k) = p\cdot\sum^k_{i=1}(1 - p)^{i - 1} \\
= p\cdot\sum^{k-1}_{j=0}(1 - p)^j \\
= p\cdot\frac{(1-p)^{k-1}-1}{(1-p)-p} \\
= 1-(1-p)^{k-1}
$$

ğŸ§  è§£é‡Šç›´è§‰ï¼š
è¿™ä¸ªè¡¨è¾¾å¼è¡¨ç¤ºï¼Œåœ¨å‰ $k$ æ¬¡è¯•éªŒä¸­è‡³å°‘æˆåŠŸä¸€æ¬¡çš„æ¦‚ç‡ã€‚åè¿‡æ¥è¯´ï¼Œæ‰€æœ‰å‰ $k$ æ¬¡éƒ½å¤±è´¥çš„æ¦‚ç‡æ˜¯ $(1âˆ’p)^k$ï¼Œæ‰€ä»¥ CDF æ˜¯ï¼š
$$
P(è‡³å°‘ä¸€æ¬¡æˆåŠŸ) = 1 - P(å…¨éƒ¨å¤±è´¥) = 1âˆ’(1âˆ’p)^k
$$

#### ğŸ“ˆ æœŸæœ›

**æœŸæœ›å€¼**ï¼ˆMeanï¼‰ï¼š

$$
\mathbb{E}[X] = \frac{1}{p}
$$

**ğŸ¯ æ¨å¯¼æœŸæœ› $\mathbb{E}[X]$**

æˆ‘ä»¬è¦è®¡ç®—ï¼š

$$
\mathbb{E}[X] = \sum_{k=1}^{\infty} k \cdot (1 - p)^{k-1} \cdot p
$$

ä»¤ $q = 1 - p$ï¼Œåˆ™å˜ä¸ºï¼š

$$
\mathbb{E}[X] = p \sum_{k=1}^{\infty} k q^{k-1}
$$

è¿™æ˜¯ä¸€ä¸ªç»å…¸çº§æ•°ï¼š

$$
\sum_{k=1}^{\infty} k q^{k-1} = \frac{1}{(1 - q)^2}
\quad \text{(å½“ } |q| < 1 \text{)}
$$

ä»£å…¥ï¼š

$$
\mathbb{E}[X] = p \cdot \frac{1}{(1 - q)^2} = p \cdot \frac{1}{p^2} = \frac{1}{p}
$$

#### ğŸ“ˆ æ–¹å·®
**æ–¹å·®**ï¼ˆVarianceï¼‰ï¼š

$$
\text{Var}(X) = \frac{1 - p}{p^2}
$$

**ğŸ“Š æ¨å¯¼æ–¹å·® $\mathrm{Var}(X)$**

æˆ‘ä»¬ä½¿ç”¨å…¬å¼ï¼š

$$
\mathrm{Var}(X) = \mathbb{E}[X^2] - \left(\mathbb{E}[X]\right)^2
$$


**1ï¸âƒ£ æ¨å¯¼ $\mathbb{E}[X^2]$**

æˆ‘ä»¬è¦ç®—ï¼š

$$
\mathbb{E}[X^2] = \sum_{k=1}^{\infty} k^2 \cdot q^{k-1} \cdot p
= p \cdot \sum_{k=1}^{\infty} k^2 q^{k-1}
$$

è¿™æ˜¯ä¸€ä¸ªå·²çŸ¥çº§æ•°ï¼ˆä½ å¯ä»¥ä»å…¬å¼æ‰‹å†Œä¸­æ‰¾åˆ°ï¼‰ï¼š

$$
\sum_{k=1}^{\infty} k^2 q^{k-1} = \frac{1 + q}{(1 - q)^3}
$$

ä»£å…¥å¾—ï¼š

$$
\mathbb{E}[X^2] = p \cdot \frac{1 + q}{(1 - q)^3}
= p \cdot \frac{1 + (1 - p)}{p^3}
= p \cdot \frac{2 - p}{p^3}
= \frac{2 - p}{p^2}
$$


**2ï¸âƒ£ ä»£å…¥æ–¹å·®å…¬å¼**

$$
\mathrm{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
= \frac{2 - p}{p^2} - \left( \frac{1}{p} \right)^2
= \frac{2 - p - 1}{p^2}
= \frac{1 - p}{p^2}
$$


#### ğŸ’¡ åº”ç”¨åœºæ™¯ä¸¾ä¾‹

* æŠ›ç¡¬å¸ç›´åˆ°ç¬¬ä¸€æ¬¡æ­£é¢
* æŸæœºå™¨ç¬¬ä¸€æ¬¡æˆåŠŸå“åº”çš„æ¬¡æ•°
* ç½‘ç»œä¸­ç­‰å¾…ç¬¬ä¸€ä¸ªæˆåŠŸåŒ…åˆ°è¾¾
* é©¬å°”ç§‘å¤«é“¾ä¸­çš„ç¬¬ä¸€æ¬¡è·³è½¬æ—¶é—´


**ğŸ§  å°ç»“**

| å±æ€§   | å‡ ä½•éšæœºå˜é‡                |
| ---- | --------------------- |
| ç±»å‹   | ç¦»æ•£å‹                   |
| å–å€¼   | æ­£æ•´æ•° $\{1,2,3,\dots\}$ |
| æ ¸å¿ƒæ€æƒ³ | ç¬¬ä¸€æ¬¡æˆåŠŸçš„è¯•éªŒç¼–å·            |
| PMF                  | $P(X = k) = (1 - p)^{k - 1} \cdot p$ |
| CDF                  | $1 - (1 - p)^k$                      |
| æœŸæœ›   | $\frac{1}{p}$         |
| æ–¹å·®   | $\frac{1-p}{p^2}$     |
| é‡‡æ ·   | æ¨¡æ‹Ÿä¼¯åŠªåˆ©åºåˆ—ï¼Œæˆ–ç”¨åå‡½æ•°é‡‡æ ·æ³•      |


**å‚è€ƒï¼š**
- [Wiki: Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution)
- [ProofWiki: Geometric Distribution Gives Rise to Probability Mass Function](https://proofwiki.org/wiki/Geometric_Distribution_Gives_Rise_to_Probability_Mass_Function)
- [Stackexchange: Solving for the CDF of the Geometric Probability Distribution](https://math.stackexchange.com/questions/2161184/solving-for-the-cdf-of-the-geometric-probability-distribution)
- [ProofWiki: Expectation of Geometric Distribution](https://proofwiki.org/wiki/Expectation_of_Geometric_Distribution)
- [ProofWiki: Variance of Geometric Distribution](https://proofwiki.org/wiki/Variance_of_Geometric_Distribution)



```python
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats

# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯é‡å¤
np.random.seed(42)
# 1. å®šä¹‰å‚æ•°
p = 0.3
values = np.arange(1, 15)
n = len(values)
pmf = stats.geom(p).pmf(values)
cdf = np.cumsum(pmf)  # ç´¯ç§¯åˆ†å¸ƒå‡½æ•°

# 2. æŠ½æ ·
n_samples = 1000
samples = np.random.geometric(p=p, size=n_samples)

# 3. å¯è§†åŒ–ï¼šç†è®ºåˆ†å¸ƒ + é‡‡æ ·é¢‘ç‡å¯¹æ¯”
plot_discrete_rv(values, pmf, cdf, samples, f"Geometric(p={p})")

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_70_0.png)
    


### é‡‡æ ·

**é‡‡æ ·æ–¹æ³•å¯¹æ¯”**

| æ–¹æ³•       | åŸç†       | æ•ˆç‡ | ä¼˜ç¼ºç‚¹        |
| -------- | -------- | -- | ---------- |
| ä¼¯åŠªåˆ©æ¨¡æ‹Ÿ    | æ¨¡æ‹Ÿæ¯æ¬¡è¯•éªŒ   | ä½  | ç›´è§‚ä½†æ…¢ï¼Œé€‚åˆæ•™å­¦  |
| åå‡½æ•°é‡‡æ ·    | CDF åå‡½æ•°  | é«˜  | é€‚åˆæ•°å­¦èƒŒæ™¯å¥½è€…   |
| æŸ¥è¡¨æ³•      | CDF + æŸ¥æ‰¾ | é«˜  | é‡å¤é‡‡æ ·å¿«ï¼Œéœ€é¢„å¤„ç† |
| NumPy å†…å»º | é«˜æ•ˆç®—æ³•     | é«˜  | å·¥ç¨‹å®ç”¨ï¼Œä½†ç•¥é»‘ç®±  |



```python
import matplotlib.pyplot as plt
from scipy import stats

def verify_geometric_sample(p, samples):
    """
    éªŒè¯å‡ ä½•åˆ†å¸ƒé‡‡æ ·çš„æ­£ç¡®æ€§
    :param p: æˆåŠŸæ¦‚ç‡
    :param samples: é‡‡æ ·ç»“æœåˆ—è¡¨ï¼ˆå€¼åŸŸåº”ä¸º 1, 2, 3, ...ï¼‰
    :return: None
    """
    N = len(samples)

    # å‡å€¼ä¸æ–¹å·®éªŒè¯
    empirical_mean = sum(samples) / N
    theoretical_mean = 1 / p
    print(f"Empirical mean: {empirical_mean:.3f}, Theoretical mean: {theoretical_mean:.3f}")
    
    empirical_variance = sum((x - empirical_mean) ** 2 for x in samples) / N
    theoretical_variance = (1 - p) / (p ** 2)
    print(f"Empirical variance: {empirical_variance:.3f}, Theoretical variance: {theoretical_variance:.3f}")

    # ç»Ÿè®¡æœ€å¤§å€¼ä½œä¸ºç»˜å›¾èŒƒå›´
    k_max = max(samples)
    ks = list(range(1, k_max + 1))

    # ç»Ÿè®¡é¢‘ç‡ï¼ˆç»éªŒ PMFï¼‰
    from collections import Counter
    counter = Counter(samples)
    counts = [counter.get(k, 0) / N for k in ks]

    # ç†è®º PMF
    theoretical = stats.geom(p).pmf(ks)

    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.bar([k - 0.2 for k in ks], counts, width=0.4, label='Sampled Frequency', color='skyblue', align='center')
    plt.bar([k + 0.2 for k in ks], theoretical, width=0.4, label='Theoretical PMF', color='orange', align='center')
    plt.xlabel("Number of Trials Until First Success (k)")
    plt.ylabel("Probability")
    plt.title(f"Geometric Distribution Verification (p={p})")
    plt.legend()
    plt.grid(True)
    plt.show()

```

#### æ–¹æ³• 1ï¼šåˆ©ç”¨ NumPy å†…ç½®å‡½æ•°

NumPy å·²å†…ç½® Geometric åˆ†å¸ƒé‡‡æ ·
* âœ… **ä¼˜ç‚¹**ï¼šé«˜æ€§èƒ½ã€ç®€æ´
* âŒ **ç¼ºç‚¹**ï¼šé€‚åˆå·¥ç¨‹ä½¿ç”¨ï¼Œå­¦ä¹ åŸç†æ—¶ä¸æ¨èç›´æ¥ä½¿ç”¨


```python
import numpy as np

p = 0.3
n = 10000  # é‡‡æ ·æ¬¡æ•°
samples = np.random.geometric(p=p, size=n).tolist()
verify_geometric_sample(p, samples)
```

    Empirical mean: 3.276, Theoretical mean: 3.333
    Empirical variance: 7.363, Theoretical variance: 7.778



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_74_1.png)
    


#### æ–¹æ³• 2ï¼šåå‡½æ•°é‡‡æ ·æ³•

å‡ ä½•åˆ†å¸ƒæœ‰ä¸€ä¸ªå°é—­å½¢å¼ï¼ˆclose-formï¼‰çš„ CDFï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨**åå‡½æ•°é‡‡æ ·æ³•**ï¼ˆinverse transform samplingï¼‰ï¼š

$$
X = \left\lceil \frac{\log(1 - U)}{\log(1 - p)} \right\rceil
\quad \text{å…¶ä¸­ } U \sim \text{Uniform}(0, 1)
$$

**åŸç†ï¼š**

åˆ©ç”¨ Geometric åˆ†å¸ƒçš„ CDFï¼ˆ$F(x) \sim \text{Uniform}(0,1)$ï¼‰ï¼š

$$
F(k) = 1 - (1 - p)^k
$$

å¦‚æœæˆ‘ä»¬ä» $U \sim \text{Uniform}(0,1)$ ä¸­å¾—åˆ°ä¸€ä¸ªå€¼ $u$ï¼Œé‚£ä¹ˆæœ‰ $1-(1-p)^{i-1} \lt u \le 1-(1-p)^{i} $ï¼Œå…¶ä¸­ï¼Œ$i$ å°±æ˜¯æˆ‘ä»¬è¦çš„é‡‡æ ·å€¼ã€‚æˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡ºä¸€ä¸ªå°é—­å½¢å¼ï¼š
$$
1-(1-p)^{i-1} \lt u \le 1-(1-p)^{i} \\
-1+(1-p)^{i-1} \gt -u \ge -1+(1-p)^{i} \\
(1-p)^{i-1} \gt 1-u \ge (1-p)^{i} \\
ln((1-p)^{i-1}) \gt ln(1-u) \ge ln((1-p)^{i}) \\
(i-1)\cdot ln(1-p) \gt ln(1-u) \ge i\cdot ln(1-p) \\
i-1 \lt \frac{ln(1-u)}{ln(1-p)} \le i
$$

å› æ­¤ï¼Œ$i = int(\frac{ln(1-u)}{ln(1-p)}) + 1$ï¼Œå…¶ä¸­ï¼Œ$int$ è¡¨ç¤ºå®æ•°éƒ¨åˆ†çš„æ•´æ•°éƒ¨åˆ†

å³ï¼š

$$
k = \left\lceil \frac{\log(1 - U)}{\log(1 - p)} \right\rceil
$$

å…¶ä¸­ $U \sim \text{Uniform}(0,1)$

> å®é™…ä¸­ï¼Œå› ä¸º $1 - U$ ä¸ $U$ åœ¨åˆ†å¸ƒä¸Šä¸€è‡´ï¼Œæ‰€ä»¥å¸¸å†™ä¸ºï¼š

$$
k = \left\lceil \frac{\log(U)}{\log(1 - p)} \right\rceil
$$

**ä¼˜åŠ£ï¼š**
* âœ… **ä¼˜ç‚¹**ï¼šæ•ˆç‡é«˜ï¼Œå•æ¬¡é‡‡æ ·å¸¸æ•°æ—¶é—´
* âœ… **é€‚åˆä½ $p$** å€¼æ—¶ä½¿ç”¨
* âŒ **ç¼ºç‚¹**ï¼šéœ€è¦è®¡ç®—å¯¹æ•°å‡½æ•°ï¼Œå¯èƒ½ç•¥æ…¢äºæŸ¥è¡¨æ³•


```python
def sample_geometric_inverse(p):
    """
    ä½¿ç”¨é€†å˜æ¢æ³•é‡‡æ ·å‡ ä½•åˆ†å¸ƒ
    :param p: æˆåŠŸæ¦‚ç‡
    :return: é‡‡æ ·ç»“æœ
    """
    u = np.random.uniform(0, 1)  # ä» [0, 1) ä¸­é‡‡æ ·ä¸€ä¸ªå‡åŒ€å˜é‡
    return int(np.ceil(np.log(1 - u) / np.log(1 - p)))  # ä½¿ç”¨é€†å˜æ¢å…¬å¼

def sample_geometric_inverse_list(p, n_samples):
    """
    ç”Ÿæˆå‡ ä½•åˆ†å¸ƒçš„é‡‡æ ·åˆ—è¡¨
    :param p: æˆåŠŸæ¦‚ç‡
    :param n_samples: é‡‡æ ·æ•°é‡
    :return: é‡‡æ ·ç»“æœåˆ—è¡¨
    """
    return [sample_geometric_inverse(p) for _ in range(n_samples)]
```


```python
p = 0.3
n = 10000  # é‡‡æ ·æ¬¡æ•°
samples = sample_geometric_inverse_list(p, n)
verify_geometric_sample(p, samples)
```

    Empirical mean: 3.396, Theoretical mean: 3.333
    Empirical variance: 8.143, Theoretical variance: 7.778



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_77_1.png)
    


#### æ–¹æ³• 3ï¼šæŸ¥è¡¨æ³•ï¼ˆCDF Table Lookupï¼‰

**åŸç†ï¼š**
æå‰æ„å»º CDF è¡¨ï¼ˆPMF ç´¯åŠ ï¼‰ï¼Œå†ç”¨ Uniform å˜é‡æŸ¥æ‰¾å¯¹åº”åŒºé—´ã€‚

**æ­¥éª¤ï¼š**
1. ç»™å®š $p$ï¼Œè®¡ç®—å‰ $N$ ä¸ªæ¦‚ç‡å’Œ $\text{CDF}(k)$
2. ç”Ÿæˆ $U \sim \text{Uniform}(0, 1)$
3. æ‰¾åˆ°æœ€å°çš„ $k$ï¼Œä½¿å¾— $\text{CDF}(k) \geq U$

**ä¼˜åŠ£ï¼š**
* âœ… **ä¼˜ç‚¹**ï¼šé‡‡æ ·å¿«é€Ÿï¼ˆO(N) æˆ–æ›´å¿«å¦‚äºŒåˆ†æŸ¥æ‰¾ï¼‰
* âœ… **é€‚åˆé‡å¤é‡‡æ ·åœºæ™¯**
* âŒ **ç¼ºç‚¹**ï¼šéœ€è¦é¢„å­˜ CDF è¡¨ï¼ˆå†…å­˜å ç”¨ï¼‰


```python
import random

def build_cdf_table(p, max_k=100):
    cdf = []
    total = 0.0
    for k in range(1, max_k + 1):
        prob = (1 - p) ** (k - 1) * p
        total += prob
        cdf.append(total)
    return cdf

def sample_geometric_lookup(cdf_table):
    u = random.random()
    for i, value in enumerate(cdf_table):
        if u <= value:
            return i + 1
    return len(cdf_table)  # fallback

def sample_geometric_lookup_list(p, n_samples, max_k=100):
    cdf_table = build_cdf_table(p, max_k)
    return [sample_geometric_lookup(cdf_table) for _ in range(n_samples)]
```


```python
p = 0.3
n = 10000  # é‡‡æ ·æ¬¡æ•°
samples = sample_geometric_lookup_list(p, n)
verify_geometric_sample(p, samples)
```

    Empirical mean: 3.371, Theoretical mean: 3.333
    Empirical variance: 7.935, Theoretical variance: 7.778



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_80_1.png)
    


#### æ–¹æ³• 4ï¼šä¼¯åŠªåˆ©è¯•éªŒæ¨¡æ‹Ÿæ³•ï¼ˆBernoulli Trial Simulationï¼‰

**åŸç†**ï¼šä» $X \sim \text{Geometric}(p)$ çš„å®šä¹‰å‡ºå‘ï¼šå®ƒè¡¨ç¤º**ç¬¬ä¸€ä¸ªæˆåŠŸå‡ºç°çš„ä½ç½®**ã€‚å› æ­¤ï¼Œåªè¦é‡å¤æŠ•æ·ä¼¯åŠªåˆ©(p)åˆ†å¸ƒçš„ç¡¬å¸ï¼Œç›´åˆ°ç¬¬ä¸€æ¬¡å‡ºç°â€œæˆåŠŸâ€ï¼ˆå³ä¸º 1ï¼‰ã€‚

**æ­¥éª¤ï¼š**
1. åˆå§‹åŒ–è®¡æ•°å™¨ $k = 1$
2. é‡å¤ç”Ÿæˆ $U \sim \text{Uniform}(0, 1)$
3. å¦‚æœ $1-p \le U \le 1$ï¼Œè¡¨ç¤ºæˆåŠŸï¼Œè¿”å› $k$
4. å¦åˆ™ $k \leftarrow k + 1$ï¼Œé‡å¤

**ä¼˜åŠ£ï¼š**
* âœ… **ä¼˜ç‚¹**ï¼šåŸç†ç›´è§‚ï¼Œé€‚åˆæ•™å­¦æ¼”ç¤º
* âŒ **ç¼ºç‚¹**ï¼šå½“ $p$ å¾ˆå°æ—¶ï¼Œå¯èƒ½éœ€è¦å¾ˆå¤šæ¬¡è¿­ä»£ï¼Œæ•ˆç‡ä½



```python
import random

def sample_geometric_mimic(p):
    """
    æ¨¡æ‹Ÿå‡ ä½•åˆ†å¸ƒé‡‡æ ·
    :param p: æˆåŠŸæ¦‚ç‡
    :return: é‡‡æ ·ç»“æœ
    """
    k = 1
    while True:
        u = random.random()  # ä» [0, 1) ä¸­é‡‡æ ·ä¸€ä¸ªå‡åŒ€å˜é‡
        if 1-p <=u <=1:  # æˆåŠŸ
            return k
        k += 1  # å¤±è´¥ï¼Œå¢åŠ è®¡æ•°

def sample_geometric_mimic_list(p, n_samples):
    """
    ç”Ÿæˆå‡ ä½•åˆ†å¸ƒçš„é‡‡æ ·åˆ—è¡¨
    :param p: æˆåŠŸæ¦‚ç‡
    :param n_samples: é‡‡æ ·æ•°é‡
    :return: é‡‡æ ·ç»“æœåˆ—è¡¨
    """
    return [sample_geometric_mimic(p) for _ in range(n_samples)]
```


```python
# é‡‡æ ·
p = 0.3
N = 10000  # é‡‡æ ·æ¬¡æ•°
samples = sample_geometric_mimic_list(p, N)

verify_geometric_sample(p, samples)
```

    Empirical mean: 3.339, Theoretical mean: 3.333
    Empirical variance: 7.983, Theoretical variance: 7.778



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_83_1.png)
    


## æ­£æ€éšæœºå˜é‡ï¼ˆNormal Random Variablesï¼‰

### åŸºç¡€çŸ¥è¯†


æ­£æ€åˆ†å¸ƒï¼ˆNormal Distributionï¼‰ï¼Œä¹Ÿå«**é«˜æ–¯åˆ†å¸ƒ**ï¼Œæ˜¯ä¸€ç§**è¿ç»­å‹æ¦‚ç‡åˆ†å¸ƒ**ï¼Œåœ¨ç»Ÿè®¡å­¦ä¸­éå¸¸é‡è¦ï¼Œæ˜¯**ä¸­å¿ƒæé™å®šç†**çš„æ ¸å¿ƒåˆ†å¸ƒã€‚å®ƒçš„æ›²çº¿å‘ˆ**é’Ÿå½¢å¯¹ç§°**ï¼Œåœ¨è‡ªç„¶ç•Œä¸å·¥ç¨‹é¢†åŸŸä¸­é¢‘ç¹å‡ºç°ã€‚


#### ğŸ“ æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰

æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š

$$
f(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
$$

* $\mu$ï¼šå‡å€¼ï¼ˆä½ç½®å‚æ•°ï¼Œå†³å®šå³°å€¼ä¸­å¿ƒï¼‰
* $\sigma^2$ï¼šæ–¹å·®ï¼ˆå°ºåº¦å‚æ•°ï¼Œå†³å®šæ›²çº¿å®½åº¦ï¼‰
* $\sigma$ï¼šæ ‡å‡†å·®

#### ç‰¹æ®Šæƒ…å†µï¼šæ ‡å‡†æ­£æ€åˆ†å¸ƒ

å½“ $\mu = 0, \sigma = 1$ æ—¶ï¼š

$$
f(x) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{x^2}{2}\right)
$$


#### ğŸ“ˆ å½¢çŠ¶ä¸ç‰¹æ€§

* **å¯¹ç§°æ€§**ï¼šå…³äº $\mu$ å¯¹ç§°
* **å•å³°æ€§**ï¼šå‡å€¼å¤„æœ€é«˜ç‚¹
* **å·¦å³å°¾å·´æ— é™å»¶ä¼¸ï¼Œä½†æ€»é¢ç§¯ä¸º 1**
* **68-95-99.7 è§„åˆ™ï¼ˆç»éªŒæ³•åˆ™ï¼‰**ï¼š

  * çº¦ 68% çš„æ¦‚ç‡é›†ä¸­åœ¨ $\mu \pm \sigma$
  * çº¦ 95% çš„æ¦‚ç‡é›†ä¸­åœ¨ $\mu \pm 2\sigma$
  * çº¦ 99.7% çš„æ¦‚ç‡é›†ä¸­åœ¨ $\mu \pm 3\sigma$


#### ğŸ§® æœŸæœ›ä¸æ–¹å·®çš„æ¨å¯¼ï¼ˆæ ‡å‡†æ­£æ€ï¼‰

* **æœŸæœ›**ï¼š

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot f(x) \, dx = 0
$$

* **æ–¹å·®**ï¼š

$$
\mathbb{V}[X] = \mathbb{E}[X^2] = \int_{-\infty}^{\infty} x^2 \cdot f(x) \, dx = 1
$$

è¿™äº›æ¨å¯¼éœ€è¦ç”¨åˆ°å¯¹ç§°æ€§å’Œé«˜æ–¯ç§¯åˆ†æŠ€å·§ï¼ˆå¦‚æ¢å…ƒæ³•æˆ–é…æ–¹æ³•ï¼‰ã€‚


#### ğŸ§ª æ­£æ€åˆ†å¸ƒçš„æ¥æºä¸ç›´è§‰ç†è§£ï¼š[ä¸­å¿ƒæé™å®šç†ï¼ˆCLTï¼‰](https://en.wikipedia.org/wiki/Central_limit_theorem)

> **å¤šä¸ªç‹¬ç«‹éšæœºå˜é‡çš„å¹³å‡å€¼åœ¨ n è¶‹äºæ— ç©·å¤§æ—¶è¶‹äºæ­£æ€åˆ†å¸ƒ**ï¼Œæ— è®ºåŸå§‹åˆ†å¸ƒæ˜¯ä»€ä¹ˆã€‚

ä»¤ $\{X_{1},\ldots ,X_{n}\}$ æ˜¯ä¸€ä¸ª[ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.dï¼‰](https://zh.wikipedia.org/wiki/%E7%8B%AC%E7%AB%8B%E5%90%8C%E5%88%86%E5%B8%83)çš„éšæœºå˜é‡åºåˆ—ï¼Œå®ƒä»¬æ»¡è¶³çš„åˆ†å¸ƒçš„æœŸæœ›å€¼ä¸º$\mu$ï¼Œæ–¹å·®ä¸º $\sigma ^{2}$ã€‚

é‚£ä¹ˆï¼Œå…¶**æ ·æœ¬å‡å€¼**ä¸ºï¼š${\bar {X}}_{n}\equiv {\frac {X_{1}+\cdots +X_{n}}{n}}$ã€‚æ ¹æ®å¤§æ•°å®šç†ï¼Œ


è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ­£æ€åˆ†å¸ƒåœ¨è‡ªç„¶ç°è±¡ä¸­å¦‚æ­¤æ™®éï¼šæ¸©åº¦ã€èº«é«˜è¯¯å·®ã€æµ‹é‡è¯¯å·®ã€è€ƒè¯•æˆç»©ç­‰ã€‚


#### ğŸ§  åº”ç”¨åœºæ™¯

* æµ‹é‡è¯¯å·®å»ºæ¨¡
* æœºå™¨å­¦ä¹ ä¸­çš„é«˜æ–¯å‡è®¾
* è´å¶æ–¯æ¨ç†ä¸­çš„å…ˆéªŒ / ä¼¼ç„¶
* è‚¡ç¥¨æ”¶ç›Šå»ºæ¨¡ï¼ˆç²—ç•¥ï¼‰
* æ•°æ®ç”Ÿæˆ / æ¨¡æ‹Ÿ / MC æ–¹æ³•åŸºç¡€åˆ†å¸ƒ



```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

x = np.linspace(-4, 4, 500)
pdf = norm.pdf(x, loc=0, scale=1)

plt.plot(x, pdf, label='Standard Normal PDF', color='blue')
plt.fill_between(x, pdf, alpha=0.3, color='skyblue')
plt.title("Standard Normal Distribution")
plt.xlabel("x")
plt.ylabel("Probability Density")
plt.grid(True)
plt.legend()
plt.show()
```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_86_0.png)
    


### ä»æ ‡å‡†æ­£æ€éšæœºå˜é‡ï¼ˆStandard Normal RVï¼‰ä¸­é‡‡æ ·

æ ‡å‡†æ­£æ€åˆ†å¸ƒæ˜¯å‡å€¼ $\mu = 0$ï¼Œæ ‡å‡†å·® $\sigma = 1$ çš„æ­£æ€åˆ†å¸ƒï¼š

$$
f_Z(z) = \frac{1}{\sqrt{2\pi}} \exp\left(-\frac{z^2}{2}\right)
$$



| æ–¹æ³•                 | åŸç†     | æ˜¯å¦æ¨è | æ˜¯å¦å¿«é€Ÿ | å¤‡æ³¨          |
| ------------------ | ------ | ---- | ---- | ----------- |
| `np.random.normal` | å†…ç½®åº“    | âœ…    | âœ…    | æœ€æ–¹ä¾¿         |
| Box-Muller         | æåæ ‡å˜æ¢  | âœ…    | âœ…    | åŸç†ç›´è§‚ï¼Œé€‚åˆæ•™å­¦   |
| Inverse CDF        | åå‡½æ•°é‡‡æ ·  | âŒ    | âŒ    | æ— è§£æè§£ï¼Œéœ€è¿‘ä¼¼æˆ–æŸ¥è¡¨ |
| ä¸­å¿ƒæé™å®šç†         |            |      |        |    ç®€å•ç›´è§‚ï¼Œæ•™å­¦å‹å¥½ï¼Œä½†ç²¾åº¦æœ‰é™ï¼Œä¸é€‚ç”¨äºé«˜ç²¾åº¦æ¨¡æ‹Ÿ       |
| Rejection Sampling | æ¥å—æ‹’ç»æœºåˆ¶ | âš ï¸   | âŒ    | é€šç”¨æ€§å¼ºï¼Œæ•ˆç‡ä½    |




```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

def verify_standard_normal_sample(samples):
    """
    éªŒè¯æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·çš„æ­£ç¡®æ€§
    :param samples: ä» N(0,1) ä¸­é‡‡æ ·çš„ç»“æœ (list or array)
    """
    N = len(samples)
    
    # ç»éªŒç»Ÿè®¡é‡
    empirical_mean = np.mean(samples)
    empirical_var = np.var(samples)
    
    # ç†è®ºç»Ÿè®¡é‡
    theoretical_mean = 0
    theoretical_var = 1

    print(f"Empirical mean: {empirical_mean:.4f} | Theoretical mean: {theoretical_mean}")
    print(f"Empirical var: {empirical_var:.4f} | Theoretical var: {theoretical_var}")
    
    # ç›´æ–¹å›¾ + ç†è®º PDF
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    x = np.linspace(-4, 4, 500)
    plt.hist(samples, bins=50, density=True, alpha=0.6, color='skyblue', label='Sampled Histogram')
    plt.plot(x, norm.pdf(x, loc=theoretical_mean, scale=np.sqrt(theoretical_var)), 'r-', lw=2, label='Theoretical PDF')
    plt.title('PDF Comparison')
    plt.xlabel('x')
    plt.ylabel('Density')
    plt.legend()
    plt.grid(True)

    # ç»éªŒ CDF vs ç†è®º CDF
    plt.subplot(1, 2, 2)
    sorted_samples = np.sort(samples)
    empirical_cdf = np.arange(1, N+1) / N
    plt.plot(sorted_samples, empirical_cdf, label='Empirical CDF', lw=2)
    plt.plot(x, norm.cdf(x, loc=theoretical_mean, scale=np.sqrt(theoretical_var)), 'r--', label='Theoretical CDF')
    plt.title('CDF Comparison')
    plt.xlabel('x')
    plt.ylabel('CDF')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

```

#### âœ… æ–¹æ³•ä¸€ï¼šç”¨ NumPy ç›´æ¥é‡‡æ ·ï¼ˆæœ€æ–¹ä¾¿ï¼‰


```python
import numpy as np

samples = np.random.normal(loc=0, scale=1, size=10000)
# verify the sampling results
verify_standard_normal_sample(samples)
```

    Empirical mean: 0.0045 | Theoretical mean: 0
    Empirical var: 0.9778 | Theoretical var: 1



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_90_1.png)
    


#### âœ… æ–¹æ³•äºŒï¼šBox-Muller å˜æ¢ï¼ˆç»å…¸æ–¹æ³•ï¼‰

Box-Muller å˜æ¢ï¼ˆ[Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform)ï¼‰æ˜¯ä¸€ç§**ä»å‡åŒ€åˆ†å¸ƒé‡‡æ ·ä¸¤ä¸ªå˜é‡ï¼Œç”Ÿæˆä¸¤ä¸ªç‹¬ç«‹æ ‡å‡†æ­£æ€éšæœºå˜é‡**çš„æ–¹æ³•ã€‚è¿™ä¸ªæ–¹æ³•æ˜¯æ„é€ æ€§çš„ã€éè¿‘ä¼¼çš„ï¼ˆä¸åƒä¸­å¿ƒæé™å®šç†é‚£æ ·æ˜¯é€¼è¿‘ï¼‰ã€‚

**ğŸ§  èƒŒåæ€æƒ³ï¼š**

æˆ‘ä»¬å¸Œæœ›ä»ä¸¤ä¸ª**ç‹¬ç«‹**çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒ $\mathcal{N}(0, 1)$ ä¸­é‡‡æ ·å‡ºå˜é‡ $Z_1$ å’Œ $Z_2$ã€‚

æˆ‘ä»¬å·²çŸ¥æ— æ³•ç›´æ¥ä»æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œä½†æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°ä»å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ ·ä¸¤ä¸ªå˜é‡ $U_1, U_2 \sim \text{Uniform}(0, 1)$ï¼Œç„¶åé€šè¿‡**å˜é‡å˜æ¢**å¾—åˆ° $Z_1, Z_2$ã€‚



**ğŸ§® æ•°å­¦æ¨å¯¼æ ¸å¿ƒæ­¥éª¤**

Step 1ï¼šå°†æ­£æ€åˆ†å¸ƒè½¬ä¸ºæåæ ‡

2ç»´ç‹¬ç«‹æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„è”åˆæ¦‚ç‡å¯†åº¦å‡½æ•°ä¸ºï¼š

$$
f_{XY}(x, y) = f_X(x)f_Y(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2}} = \frac{1}{2\pi} e^{-\frac{x^2 + y^2}{2}}
$$

å¦‚æœæˆ‘ä»¬ç”¨[æåæ ‡](https://zh.wikipedia.org/zh-hans/%E6%9E%81%E5%9D%90%E6%A0%87%E7%B3%BB)è¡¨ç¤º $x = r\cos\theta, y = r\sin\theta$ï¼Œåˆ™ï¼š

$$
f(r, \theta) = f(x,y) \cdot |J| = \frac{1}{2\pi} e^{-\frac{r^2}{2}} \cdot r
$$

å…¶ä¸­ $|J| = r$ æ˜¯é›…å¯æ¯”è¡Œåˆ—å¼ï¼ˆé¢ç§¯æ‹‰ä¼¸ï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š

$$
f_{R,\Theta}(r, \theta) = \underbrace{r e^{-r^2/2}}_{\text{ä¸ } r \text{ æœ‰å…³}} \cdot \underbrace{\frac{1}{2\pi}}_{\text{ä¸ } \theta \text{ æœ‰å…³}}
$$

> è¿™è¡¨ç¤º $R$ å’Œ $\Theta$ æ˜¯**ç›¸äº’ç‹¬ç«‹**çš„éšæœºå˜é‡ï¼

æ‰€ä»¥è”åˆ pdf å˜æˆï¼š

$$
f(r, \theta) = \frac{r}{2\pi} e^{-\frac{r^2}{2}} \quad \text{for } r \in [0, \infty), \theta \in [0, 2\pi)
$$



Step 2ï¼šæ„é€ é‡‡æ ·å˜é‡

æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æƒ³åŠæ³•åˆ†åˆ«ä»ä»¥ä¸‹ä¸¤ä¸ªåˆ†å¸ƒä¸­é‡‡æ ·ï¼š

* $\Theta \sim \text{Uniform}(0, 2\pi)$
* $R \sim \text{PDF } f_R(r) = r e^{-r^2/2},\quad r \geq 0$

æˆ‘ä»¬çœ‹ $R$ çš„åˆ†å¸ƒï¼š

$$
f_R(r) = r e^{-r^2/2}
$$

å¯¹æ¯”ï¼šè®¾ $U_1 \sim \text{Uniform}(0,1)$ï¼Œæˆ‘ä»¬ä»¤ï¼š

$$
R = \sqrt{-2 \ln U_1}
$$

æˆ‘ä»¬å¯ä»¥åå‘éªŒè¯ï¼šè¿™ä¸ª R çš„åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ­£æ˜¯ï¼š

$$
f_R(r) = \frac{d}{dr} \mathbb{P}(R \leq r) = \frac{d}{dr} \mathbb{P}(U_1 \geq e^{-r^2/2}) = \frac{d}{dr} \left( 1 - e^{-r^2/2} \right) = r e^{-r^2/2}
$$

æ‰€ä»¥è¿™ä¸ªå˜é‡å˜æ¢æ˜¯æ­£ç¡®çš„ã€‚

å¦ä¸€æ–¹é¢ï¼Œåœ¨äºŒç»´æ­£æ€è”åˆåˆ†å¸ƒä¸‹ï¼Œè§’åº¦æ–¹å‘æ˜¯å‡åŒ€çš„ï¼š

$$
f_\Theta(\theta) = \frac{1}{2\pi},\quad \theta \in [0, 2\pi)
$$

æ‰€ä»¥åªéœ€è¦ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ $U_2 \sim \text{Uniform}(0,1)$ï¼Œé€šè¿‡çº¿æ€§ç¼©æ”¾å³å¯ï¼š

$$
\Theta = 2\pi U_2
$$


**âœ… æœ€ç»ˆå˜æ¢**

å°† $R = \sqrt{-2 \ln U_1}$ï¼Œ$\Theta = 2\pi U_2$ å¸¦å…¥ï¼š

$$
Z_1 = R \cos\Theta = \sqrt{-2 \ln U_1} \cdot \cos(2\pi U_2) \\
Z_2 = R \sin\Theta = \sqrt{-2 \ln U_1} \cdot \sin(2\pi U_2)
$$

è¿™ä¸¤ä¸ªå˜é‡å°±æ˜¯æˆ‘ä»¬å¸Œæœ›å¾—åˆ°çš„**ç‹¬ç«‹æ ‡å‡†æ­£æ€å˜é‡**ã€‚



**ğŸ“Œ Box-Muller çš„ä¼˜ç‚¹å’Œç¼ºç‚¹**

| ä¼˜ç‚¹                 | ç¼ºç‚¹                    |
| ------------------ | --------------------- |
| æ•°å­¦ä¸¥è°¨ï¼Œç”Ÿæˆå€¼ç²¾ç¡®æœä»æ­£æ€åˆ†å¸ƒ   | éœ€è¦ä½¿ç”¨å¯¹æ•°å’Œä¸‰è§’å‡½æ•°ï¼Œè¾ƒæ…¢        |
| æ¯æ¬¡é‡‡æ ·ä¸¤ä¸ªæ­£æ€æ ·æœ¬         | ä¸é€‚åˆåµŒå…¥å¼è®¾å¤‡ã€GPU ç­‰éœ€æ€§èƒ½ä¼˜åŒ–åœºåˆ |
| é€‚ç”¨äºç†è§£æåæ ‡å˜æ¢ä¸æ­£æ€åˆ†å¸ƒçš„å…³ç³» | ä¸èƒ½é«˜æ•ˆçŸ¢é‡åŒ–ä¸º SIMD ä»£ç       |

**ğŸš€ å°ç»“**

Box-Muller æ˜¯ä¸€ä¸ª **ç†è®ºå®Œç¾** çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·æ–¹æ³•ï¼Œå…·æœ‰å¦‚ä¸‹ç‰¹ç‚¹ï¼š

* åˆ©ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„ $U(0,1)$ æ ·æœ¬
* æ„é€ ä¸¤ä¸ª $\mathcal{N}(0,1)$ ç‹¬ç«‹æ ·æœ¬
* å˜æ¢æ–¹å¼åŸºäºäºŒç»´æåæ ‡ç³»ç»Ÿå’Œé›…å¯æ¯”æ¨å¯¼
* æ˜¯ä¸€ç§æ ‡å‡†çš„â€œä»ç®€å•åˆ†å¸ƒå˜æ¢ä¸ºå¤æ‚åˆ†å¸ƒâ€çš„ç»å…¸æ¡ˆä¾‹






```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

def box_muller_sample(n_samples=10000, visualize=True):
    # Step 1: ç”Ÿæˆä¸¤ä¸ªå‡åŒ€åˆ†å¸ƒå˜é‡
    U1 = np.random.uniform(0, 1, n_samples)
    U2 = np.random.uniform(0, 1, n_samples)

    # Step 2: Box-Mullerå˜æ¢
    R = np.sqrt(-2 * np.log(U1))
    theta = 2 * np.pi * U2
    Z1 = R * np.cos(theta)
    Z2 = R * np.sin(theta)

    if visualize:
        num_bins = 100
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.hist(Z1, bins=num_bins, density=True, alpha=0.6, label='Z1')
        x = np.linspace(-4, 4, 500)
        plt.plot(x, norm.pdf(x), 'r--', label='Standard Normal PDF')
        plt.title("Z1 ~ N(0,1)")
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.hist(Z2, bins=num_bins, density=True, alpha=0.6, label='Z2', color='orange')
        plt.plot(x, norm.pdf(x), 'r--', label='Standard Normal PDF')
        plt.title("Z2 ~ N(0,1)")
        plt.legend()

        plt.suptitle("Box-Muller Sampling of Standard Normal Distribution")
        plt.grid(True)
        plt.show()

    return Z1, Z2

```


```python
box_muller_sample(n_samples=10000, visualize=True)
```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_93_0.png)
    





    (array([ 1.48779139,  0.98464068, -0.80586692, ..., -0.52369914,
            -0.56467099, -2.07728924], shape=(10000,)),
     array([ 1.22339848, -0.80627553,  0.55483184, ..., -1.4385899 ,
             2.53391275, -0.82037306], shape=(10000,)))



#### âœ… æ–¹æ³•ä¸‰ï¼šåå‡½æ•°é‡‡æ ·æ³•ï¼ˆç†è®ºå¯è¡Œï¼Œå®é™…ä½†ä¸å¯è¡Œï¼‰

* ç”¨åå‡½æ•°é‡‡æ ·æ³•ï¼šè®¾ $U \sim \text{Uniform}(0,1)$ï¼Œåˆ™ä»¤ï¼š

  $$
  Z = F^{-1}(U)
  $$

  å…¶ä¸­ $F$ æ˜¯æ­£æ€åˆ†å¸ƒçš„ CDFï¼Œ$F^{-1}$ æ˜¯å®ƒçš„åå‡½æ•°ï¼ˆå³åˆ†ä½æ•°å‡½æ•° / probit å‡½æ•°ï¼‰ã€‚

* **é—®é¢˜**ï¼šæ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ CDF æ²¡æœ‰è§£æåå‡½æ•°ï¼Œæ‰€ä»¥**ä¸èƒ½ç”¨åŸºæœ¬å‡½æ•°è¡¨è¾¾**ï¼Œä½†å¯ä»¥é€šè¿‡æŸ¥è¡¨æˆ–è¿‘ä¼¼ï¼ˆå¦‚ Beasley-Springer ç®—æ³•ï¼‰ã€‚

#### âœ… æ–¹æ³•å››ï¼šåˆ©ç”¨ä¸­å¿ƒæé™å®šç†ï¼ˆCentral Limit Theorem, CLTï¼‰ è¿›è¡Œé‡‡æ ·

ä½¿ç”¨ **ä¸­å¿ƒæé™å®šç†ï¼ˆCentral Limit Theorem, CLTï¼‰** è¿›è¡Œé‡‡æ ·ï¼Œæ˜¯ä¸€ç§ç»å…¸ä¸”ç›´è§‚çš„æ–¹å¼æ¥ç”Ÿæˆè¿‘ä¼¼ **æ ‡å‡†æ­£æ€åˆ†å¸ƒ** çš„æ ·æœ¬ã€‚è¿™ç§æ–¹æ³•å¸¸ç”¨äºæ•™å­¦æ¼”ç¤ºï¼ŒåŸç†ç®€å•ï¼Œä½†åœ¨å®é™…é«˜ç²¾åº¦æ¨¡æ‹Ÿä¸­ä¸å¸¸ç”¨ã€‚




**ğŸ§  Step-by-stepï¼šä¸ºä»€ä¹ˆ CLT å¯ä»¥ç”¨æ¥é‡‡æ ·æ ‡å‡†æ­£æ€åˆ†å¸ƒ**

æˆ‘ä»¬ä»¥å¦‚ä¸‹é—®é¢˜ä¸ºç›®æ ‡ï¼š

> èƒ½å¦é€šè¿‡ä¸€ç»„ç®€å•çš„**ç‹¬ç«‹éšæœºå˜é‡**ï¼ˆä¾‹å¦‚å‡åŒ€åˆ†å¸ƒï¼‰æ¥æ„é€ ä¸€ä¸ªè¿‘ä¼¼æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ $\mathcal{N}(0, 1)$ çš„å˜é‡ï¼Ÿ

ç­”æ¡ˆæ˜¯ï¼š**å¯ä»¥ï¼** è¿™å°±æ˜¯ä¸­å¿ƒæé™å®šç†çš„å¨åŠ›ã€‚

**ğŸ¯ Step 1: ä¸­å¿ƒæé™å®šç†çš„æ ¸å¿ƒå†…å®¹**

> **ä¸­å¿ƒæé™å®šç†ï¼ˆCLTï¼‰**ï¼š
> å‡è®¾ä½ æœ‰ä¸€ç»„ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.dï¼‰çš„éšæœºå˜é‡ $X_1, X_2, \dots, X_n$ï¼Œæ¯ä¸ªå˜é‡çš„æœŸæœ›å€¼ä¸º $\mu$ï¼Œæ–¹å·®ä¸º $\sigma^2$ã€‚é‚£ä¹ˆï¼š

$$
Z_n = \frac{\sum_{i=1}^n X_i - n\mu}{\sqrt{n\sigma^2}} \xrightarrow{d} \mathcal{N}(0,1)
$$

å½“ $n \to \infty$ï¼Œè¿™ä¸ªæ ‡å‡†åŒ–çš„å’Œï¼ˆæˆ–è€…å¹³å‡å€¼ï¼‰**åœ¨åˆ†å¸ƒä¸Šè¶‹è¿‘äºæ ‡å‡†æ­£æ€åˆ†å¸ƒ**ã€‚


**ğŸ“Œ Step 2: ç”¨å‡åŒ€åˆ†å¸ƒæ¥ä¸¾ä¾‹è¯´æ˜**

æˆ‘ä»¬é€‰ä¸€ä¸ªç®€å•çš„åˆ†å¸ƒï¼Œä¾‹å¦‚ **Uniform(0, 1)** åˆ†å¸ƒï¼š

* å®ƒçš„æœŸæœ› $\mu = 0.5$
* å®ƒçš„æ–¹å·® $\sigma^2 = \frac{1}{12}$

æˆ‘ä»¬é‡‡æ · $n$ ä¸ªè¿™æ ·çš„å˜é‡ $U_1, \dots, U_n$ï¼Œç„¶åæ„é€ å¦‚ä¸‹å˜é‡ï¼š

$$
Z = \frac{\sum_{i=1}^n U_i - n\mu}{\sqrt{n\sigma^2}} = \frac{\sum_{i=1}^n (U_i - 0.5)}{\sqrt{n \cdot \frac{1}{12}}}
$$

ä¹Ÿå°±æ˜¯è¯´ï¼š

$$
Z = \sum_{i=1}^n (U_i - 0.5) \cdot \sqrt{12 / n}
$$

è¿™ä¸ªå˜é‡ $Z$ ä¼šè¶Šæ¥è¶Šæ¥è¿‘æ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚

> $n$çš„å–å€¼å–å†³äºæ‰€é€‰æ‹©çš„åˆ†å¸ƒã€‚å¯¹äºå‡åŒ€åˆ†å¸ƒï¼Œ$n$çš„å€¼å¯ä»¥å¾ˆå°å°±å¯ä»¥å¿«é€Ÿæ”¶æ•›ã€‚

**ğŸ§ª Step 3: å®éªŒæ¼”ç¤ºæ„é€ è¿‡ç¨‹**

è®¾ $n = 12$ï¼Œé‚£ä¹ˆï¼š

$$
Z = \sum_{i=1}^{12} (U_i - 0.5)
$$

ä¸ºä»€ä¹ˆä¹˜ $\sqrt{12}$ï¼Ÿ

å› ä¸ºï¼š

* æ¯ä¸ª $U_i - 0.5$ æ˜¯å‡å€¼ä¸º 0ã€æ–¹å·®ä¸º $1/12$ çš„å˜é‡ï¼›
* ç›¸åŠ ä¹‹åçš„å’Œæ–¹å·®ä¸º $n \cdot \frac{1}{12}$ï¼Œ
* ä¸ºäº†æ ‡å‡†åŒ–ï¼ˆå˜æˆæ–¹å·®ä¸º 1 çš„å˜é‡ï¼‰ï¼Œè¦ä¹˜ä»¥ $\frac{1}{\sqrt{n \cdot \frac{1}{12}}} = \sqrt{12 / n}$ã€‚



**âœ¨ å¯¹äºæ ‡å‡†æ­£æ€é‡‡æ ·ï¼š**

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ„é€ æ–¹å¼ï¼š

$$
Z = \frac{1}{\sqrt{n}} \sum_{i=1}^n (U_i - \mu)
$$

* $U_i \sim \text{Uniform}(a,b)$ï¼ˆæˆ–å…¶ä»–åˆ†å¸ƒï¼‰
* $\mu$ æ˜¯ $U_i$ çš„æœŸæœ›ï¼Œä¾‹å¦‚å¯¹ $U(0,1)$ æ˜¯ $0.5$
* å½“ $n$ è¶Šå¤§æ—¶ï¼Œ$Z \approx N(0,1)$



```python
import numpy as np

def sample_normal_via_clt(a=0, b=1, n=12, num_samples=10000):
    """
    åˆ©ç”¨ä¸­å¿ƒæé™å®šç†ä» Uniform(a, b) é‡‡æ ·ç”Ÿæˆè¿‘ä¼¼æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„æ ·æœ¬
    :param a: Uniform åˆ†å¸ƒä¸‹ç•Œ
    :param b: Uniform åˆ†å¸ƒä¸Šç•Œ
    :param n: æ¯æ¬¡é‡‡æ ·çš„ Uniform ä¸ªæ•°
    :param num_samples: æ€»å…±ç”Ÿæˆå¤šå°‘ä¸ªæ ·æœ¬
    :return: ä¸€ä¸ªè¿‘ä¼¼æ ‡å‡†æ­£æ€çš„æ ·æœ¬æ•°ç»„
    """
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mu = (a + b) / 2
    sigma = (b - a) / np.sqrt(12) # å‡åŒ€åˆ†å¸ƒçš„æ ‡å‡†å·®
    # Step 1: ä» Uniform(a, b) ä¸­ç”Ÿæˆ n ä¸ªæ ·æœ¬ï¼Œé‡å¤ num_samples æ¬¡
    uniform_samples = np.random.uniform(a, b, size=(num_samples, n))
    #print(uniform_samples.shape) #(num_samples, n)
    # Step 2: å¯¹æ¯ä¸€ç»„æ ·æœ¬æ±‚å’Œå¹¶æ ‡å‡†åŒ–
    sample_sums = np.sum(uniform_samples, axis=1) # (num_samples,)
    z_samples = (sample_sums - n * mu) / (np.sqrt(n) * sigma)
    return z_samples
```


```python
samples = sample_normal_via_clt(a=0, b=1, n=10, num_samples=100000)
verify_standard_normal_sample(samples)
```

    Empirical mean: 0.0034 | Theoretical mean: 0
    Empirical var: 1.0084 | Theoretical var: 1



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_97_1.png)
    


#### âœ… æ–¹æ³•äº”ï¼šæ‹’ç»é‡‡æ ·ï¼ˆRejection Samplingï¼‰

* ä½¿ç”¨æ˜“é‡‡æ ·çš„ proposal åˆ†å¸ƒï¼ˆå¦‚ Cauchyã€Laplace ç­‰ï¼‰ï¼Œé€šè¿‡æ¥å—ç‡æ§åˆ¶ç”Ÿæˆæ ‡å‡†æ­£æ€ã€‚
* ä¸æ¨èæ–°æ‰‹ä¸€å¼€å§‹å°±ç”¨ï¼Œä½†é€‚åˆäº†è§£å„ç§é‡‡æ ·æ¡†æ¶ã€‚



```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, laplace

# ç›®æ ‡åˆ†å¸ƒ: æ ‡å‡†æ­£æ€
target_pdf = lambda x: norm.pdf(x)
# proposal: Laplace åˆ†å¸ƒ
proposal_pdf = lambda x: laplace.pdf(x)
proposal_sampler = lambda size: laplace.rvs(size=size)

# M æ˜¯ f(x)/q(x) çš„ä¸Šç•Œï¼Œä¼°è®¡ä¸ºæœ€å¤§å€¼ï¼ˆç•¥å¤§äºå®é™…æœ€å¤§ï¼‰
x_vals = np.linspace(-10, 10, 1000)
M = np.max(target_pdf(x_vals) / proposal_pdf(x_vals)) * 1.1

# é‡‡æ ·å‡½æ•°
def rejection_sample(n):
    samples = []
    while len(samples) < n:
        x = proposal_sampler(1)[0]
        u = np.random.uniform()
        if u < target_pdf(x) / (M * proposal_pdf(x)):
            samples.append(x)
    return np.array(samples)

# ç”Ÿæˆæ ·æœ¬
samples = rejection_sample(10000)

# å¯è§†åŒ–
x = np.linspace(-4, 4, 1000)
plt.figure(figsize=(10,6))
plt.hist(samples, bins=50, density=True, alpha=0.5, label="Sampled (AR)")
plt.plot(x, norm.pdf(x), label="Target N(0,1)", lw=2)
plt.plot(x, M * proposal_pdf(x), '--', label=f"M * Proposal (M={M:.2f})", color="red")
plt.title("Rejection Sampling from N(0,1)")
plt.legend()
plt.grid(True)
plt.show()
```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_99_0.png)
    


# éšæœºå˜é‡çš„è®¡ç®—

## éšæœºå˜é‡å˜æ¢ï¼ˆRandom Variable Transformationï¼‰

### çº¿æ€§å˜æ¢
**ğŸ¯ é—®é¢˜æè¿°**

è®¾ï¼š
* $X$ æ˜¯ä¸€ä¸ªè¿ç»­å‹éšæœºå˜é‡ï¼Œå·²çŸ¥å…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆpdfï¼‰ä¸º $f_X(x)$ï¼Œ
* $a \neq 0$, $b \in \mathbb{R}$ æ˜¯å¸¸æ•°ï¼Œ
* å®šä¹‰ $Y = aX + b$ï¼Œ
* æ±‚ $Y$ çš„æ¦‚ç‡å¯†åº¦å‡½æ•° $f_Y(y)$ã€‚


**ğŸ§  åŸºæœ¬åŸç†**

è¿™æ˜¯å•è°ƒå˜æ¢ä¸‹çš„å¯†åº¦å‡½æ•°å˜æ¢è§„åˆ™ï¼š

$$
f_Y(y) = f_X\left( \frac{y - b}{a} \right) \cdot \left| \frac{1}{a} \right|
$$

> âš ï¸ æ³¨æ„ç»å¯¹å€¼ç¬¦å·æ˜¯å› ä¸º $a$ æœ‰å¯èƒ½æ˜¯è´Ÿæ•°ã€‚


**ğŸ“Œ æ¨å¯¼è¿‡ç¨‹**

æˆ‘ä»¬æ¥ç³»ç»Ÿæ¨å¯¼ä¸€ä¸‹è¿™ä¸ªå…¬å¼ã€‚

ç¬¬ä¸€æ­¥ï¼šä»CDFå‡ºå‘

å…ˆæ±‚å‡º $Y$ çš„ CDFï¼ˆç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼‰ï¼š

- å¦‚æœ $a > 0$ï¼š

$$
F_Y(y) = P(Y \leq y) = P(aX + b \leq y) = P\left(X \leq \frac{y - b}{a} \right) = F_X\left( \frac{y - b}{a} \right)
$$

ç„¶åå¯¹ $y$ æ±‚å¯¼ï¼š

$$
f_Y(y) = \frac{d}{dy} F_X\left( \frac{y - b}{a} \right) = f_X\left( \frac{y - b}{a} \right) \cdot \frac{1}{a}
$$

- å¦‚æœ $a < 0$ï¼š

$$
F_Y(y) = P(aX + b \leq y) = P\left(X \geq \frac{y - b}{a} \right) = 1 - F_X\left( \frac{y - b}{a} \right)
$$

ç„¶åï¼š

$$
f_Y(y) = \frac{d}{dy} \left[ 1 - F_X\left( \frac{y - b}{a} \right) \right] = -f_X\left( \frac{y - b}{a} \right) \cdot \frac{1}{a}
$$

å› ä¸º $a < 0$ï¼Œæ‰€ä»¥ç»“æœä»ç„¶æ˜¯ï¼š

$$
f_Y(y) = f_X\left( \frac{y - b}{a} \right) \cdot \left| \frac{1}{a} \right|
$$


**âœ… ç»“è®ºï¼ˆå˜æ¢å…¬å¼ï¼‰**

ä¸ç®¡ $a > 0$ è¿˜æ˜¯ $a < 0$ï¼Œç»Ÿä¸€å…¬å¼ä¸ºï¼š

$$
\boxed{
f_Y(y) = f_X\left( \frac{y - b}{a} \right) \cdot \left| \frac{1}{a} \right|
}
$$



#### ğŸ§ª ç¤ºä¾‹ï¼š$X \sim \text{Exponential}(\lambda)$ï¼Œ$Y = 2X + 3$

åŸå§‹ pdfï¼š

$$
f_X(x) = \lambda e^{-\lambda x}, \quad x \geq 0
$$

ä»¤ $Y = 2X + 3$ï¼Œé‚£ä¹ˆï¼š

* $a = 2$, $b = 3$
* $ f_Y(y) = f_X\left( \frac{y - 3}{2} \right) \cdot \frac{1}{2} = \lambda e^{-\lambda \cdot \frac{y - 3}{2}} \cdot \frac{1}{2}, \quad y \geq 3$



```python
import numpy as np
import matplotlib.pyplot as plt

# Parameters
lambda_ = 1  # parameter for Exponential distribution
a = 2
b = 3

# Define the original PDF of X ~ Exp(lambda)
def f_X(x):
    return lambda_ * np.exp(-lambda_ * x) * (x >= 0)

# Define the transformed PDF of Y = aX + b
def f_Y(y):
    x = (y - b) / a
    return f_X(x) * (1 / abs(a)) * (y >= b)

# Create x and y ranges
x_vals = np.linspace(0, 10, 400)
y_vals = np.linspace(3, 20, 400)

# Evaluate PDFs
fx_vals = f_X(x_vals)
fy_vals = f_Y(y_vals)

# Plot
plt.figure(figsize=(10, 5))
plt.plot(x_vals, fx_vals, label=r'$f_X(x)$ (Exponential)', color='blue')
plt.plot(y_vals, fy_vals, label=r'$f_Y(y)$ (Transformed)', color='orange')
plt.title('PDF of X and Y = aX + b (a=2, b=3)')
plt.xlabel('x or y')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_103_0.png)
    


### éçº¿æ€§å˜æ¢ï¼ˆnonlinear transformationï¼‰
éå¸¸å¥½ï¼è®¨è®º\*\*éçº¿æ€§å˜æ¢ï¼ˆnonlinear transformationï¼‰\*\*æ˜¯ç†è§£æ¦‚ç‡è®ºä¸ç»Ÿè®¡æ¨ç†çš„å…³é”®éƒ¨åˆ†ã€‚æˆ‘ä»¬åˆ†æ­¥éª¤æ·±å…¥è®²è§£ï¼š


**ğŸ§  é—®é¢˜ï¼šéçº¿æ€§å˜æ¢ä¸‹å¦‚ä½•æ±‚éšæœºå˜é‡çš„åˆ†å¸ƒï¼Ÿ**

è®¾ï¼š

* $X$ æ˜¯ä¸€ä¸ªå·²çŸ¥è¿ç»­å‹éšæœºå˜é‡ï¼Œæœ‰å¯†åº¦å‡½æ•° $f_X(x)$
* å®šä¹‰æ–°å˜é‡ï¼š$Y = g(X)$ï¼Œå…¶ä¸­ $g$ æ˜¯ä¸€ä¸ª **å¯å¾®çš„ã€ä¸¥æ ¼å•è°ƒçš„å‡½æ•°**

æˆ‘ä»¬æƒ³è¦æ±‚å‡º $Y$ çš„æ¦‚ç‡å¯†åº¦å‡½æ•° $f_Y(y)$ã€‚



**ğŸ§® ç†è®ºç»“æœï¼šå˜æ¢æ³•å…¬å¼**

å¯¹äºå•è°ƒå¯å¾®å‡½æ•° $g$ï¼Œæœ‰ï¼š

$$
f_Y(y) = f_X\big(g^{-1}(y)\big) \cdot \left| \frac{d}{dy} g^{-1}(y) \right|
$$

è¿™ä¸ªå…¬å¼è¢«ç§°ä½œ**å˜æ¢æ³•ï¼ˆChange of Variablesï¼‰**æˆ–**åå‡½æ•°æ³•ï¼ˆInverse Methodï¼‰**ã€‚



**âœ… æ­¥éª¤æ€»ç»“ï¼ˆä»¥å•è°ƒé€’å¢å‡½æ•°ä¸ºä¾‹ï¼‰**

1. å†™å‡º $Y = g(X)$
2. æ¨å‡ºåå‡½æ•° $X = g^{-1}(Y)$
3. è®¡ç®—åå‡½æ•°çš„å¯¼æ•° $\frac{d}{dy} g^{-1}(y)$
4. å°†è¿™äº›ä»£å…¥å˜æ¢å…¬å¼ï¼Œå¾—å‡º $f_Y(y)$


#### ğŸ“Œ ä¸¾ä¾‹ï¼š$Y = \sqrt{X}$ï¼Œå…¶ä¸­ $X \sim \text{Uniform}(0, 1)$

æˆ‘ä»¬æ¥ä¸€æ­¥ä¸€æ­¥æ“ä½œè¿™ä¸ªå˜æ¢ã€‚

1. åŸå§‹å˜é‡ï¼š

$$
f_X(x) = 1, \quad 0 \le x \le 1
$$

2. éçº¿æ€§å˜æ¢ï¼š

$$
Y = g(X) = \sqrt{X} \Rightarrow X = g^{-1}(Y) = Y^2
$$

3. æ±‚å¯¼ï¼š

$$
\frac{d}{dy} g^{-1}(y) = \frac{d}{dy} (Y^2) = 2y
$$

4. ä»£å…¥å…¬å¼ï¼š

$$
f_Y(y) = f_X(Y^2) \cdot \left| \frac{d}{dy} Y^2 \right| = 1 \cdot 2y = 2y
$$

å®šä¹‰åŸŸï¼š

* å› ä¸º $X \in [0, 1] \Rightarrow Y \in [0, 1]$

æ‰€ä»¥ï¼š

$$
f_Y(y) = 
\begin{cases}
2y, & 0 \le y \le 1 \\
0, & \text{otherwise}
\end{cases}
$$

è¿™æ˜¯ä¸€ä¸ªä¸‰è§’å½¢å½¢çŠ¶çš„å¯†åº¦å‡½æ•°ï¼



```python
import numpy as np
import matplotlib.pyplot as plt

# Define the original PDF: X ~ Uniform(0, 1)
def f_X(x):
    return np.ones_like(x) * ((x >= 0) & (x <= 1))

# Define the transformation: Y = sqrt(X) => X = Y^2
def f_Y(y):
    return 2 * y * ((y >= 0) & (y <= 1))

# Create value ranges
x_vals = np.linspace(-0.2, 1.2, 400)
y_vals = np.linspace(-0.2, 1.2, 400)

# Evaluate PDFs
fx_vals = f_X(x_vals)
fy_vals = f_Y(y_vals)

# Plot the PDFs
plt.figure(figsize=(10, 5))
plt.plot(x_vals, fx_vals, label=r'$f_X(x)$: Uniform(0, 1)', color='blue')
plt.plot(y_vals, fy_vals, label=r'$f_Y(y)$: $Y = \sqrt{X}$', color='orange')
plt.title("""PDF Transformation: $Y = \sqrt{X}$ with $X \sim U(0, 1)$""")
plt.xlabel('x or y')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

```

    <>:24: SyntaxWarning: invalid escape sequence '\s'
    <>:24: SyntaxWarning: invalid escape sequence '\s'
    /var/folders/mx/684cy0qs5zd3c_pdx4wy4pkc0000gn/T/ipykernel_15262/4117088612.py:24: SyntaxWarning: invalid escape sequence '\s'
      plt.title("""PDF Transformation: $Y = \sqrt{X}$ with $X \sim U(0, 1)$""")



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_105_1.png)
    


## æœŸæœ›è¿ç®—ç¬¦(Mean Operator)

### åŸºç¡€
**ğŸ” ä»€ä¹ˆæ˜¯ Mean Operatorï¼Ÿ**

â€œMean operatorâ€æ˜¯æŒ‡**å¯¹éšæœºå˜é‡å–æœŸæœ›çš„è¿ç®—ç¬¦**ã€‚
åœ¨æ¦‚ç‡è®ºä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ç¬¦å·ï¼š

$$
\mathbb{E}[X]
$$

æ¥è¡¨ç¤ºéšæœºå˜é‡ $X$ çš„æœŸæœ›ï¼ˆä¹Ÿç§°**å¹³å‡å€¼**ã€**æœŸæœ›å€¼**ã€**å‡å€¼**ï¼‰ã€‚
è¿™ä¸ªè¿ç®—å¯ä»¥çœ‹æˆæ˜¯å¯¹éšæœºå˜é‡åœ¨å…¶åˆ†å¸ƒä¸‹çš„â€œåŠ æƒå¹³å‡â€ã€‚


#### ğŸ“Š ç¦»æ•£å‹éšæœºå˜é‡çš„æœŸæœ›

å¦‚æœ $X$ æ˜¯ä¸€ä¸ªç¦»æ•£å‹éšæœºå˜é‡ï¼Œå–å€¼ä¸º $x_1, x_2, \dots$ï¼Œæ¦‚ç‡ä¸º $P(X = x_i) = p_i$ï¼Œ
é‚£ä¹ˆå…¶æœŸæœ›ä¸ºï¼š

$$
\mathbb{E}[X] = \sum_{i} x_i \cdot p_i
$$

**ä¾‹å­ï¼š**

æŠ•ä¸€æšå…¬å¹³çš„ç¡¬å¸ï¼Œä»¤ $X = 1$ è¡¨ç¤ºæ­£é¢ï¼Œ$X = 0$ è¡¨ç¤ºåé¢ï¼š

$$
\mathbb{E}[X] = 1 \cdot \frac{1}{2} + 0 \cdot \frac{1}{2} = \frac{1}{2}
$$

#### ğŸ“ˆ è¿ç»­å‹éšæœºå˜é‡çš„æœŸæœ›

å¦‚æœ $X$ æ˜¯ä¸€ä¸ªè¿ç»­éšæœºå˜é‡ï¼Œå…·æœ‰æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰ $f_X(x)$ï¼Œ
é‚£ä¹ˆå…¶æœŸæœ›ä¸ºï¼š

$$
\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx
$$

**ä¾‹å­ï¼š**

è‹¥ $X \sim \text{Uniform}(0, 1)$ï¼Œå³ $f_X(x) = 1$ for $x \in [0, 1]$ï¼Œåˆ™ï¼š

$$
\mathbb{E}[X] = \int_0^1 x \cdot 1 \, dx = \left[ \frac{1}{2}x^2 \right]_0^1 = \frac{1}{2}
$$

#### ğŸ” Mean Operator vs å®é™…å¹³å‡

æœŸæœ›æ˜¯ç†è®ºä¸Šçš„å¹³å‡å€¼ï¼Œæ˜¯å¯¹æ•´ä¸ª**æ¦‚ç‡åˆ†å¸ƒ**è€Œè¨€çš„ã€‚è€Œç°å®ä¸­æˆ‘ä»¬å¾€å¾€åªèƒ½è§‚å¯Ÿåˆ°æœ‰é™çš„æ ·æœ¬ï¼š

$$
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$

è¿™å«åš**æ ·æœ¬å‡å€¼ï¼ˆsample meanï¼‰**ï¼Œå®ƒæ˜¯ç”¨æ¥è¿‘ä¼¼ä¼°è®¡ $\mathbb{E}[X]$ çš„ã€‚æ ¹æ®**å¤§æ•°å®šå¾‹**ï¼Œå½“æ ·æœ¬æ•°é‡è¶‹è¿‘äºæ— ç©·å¤§æ—¶ï¼Œæ ·æœ¬å‡å€¼ä¼šæ”¶æ•›åˆ°ç†è®ºæœŸæœ›ã€‚

#### ğŸ§® Mean Operator çš„æ€§è´¨ï¼ˆçº¿æ€§ï¼‰

è®¾ $X, Y$ æ˜¯éšæœºå˜é‡ï¼Œ$a, b$ æ˜¯å¸¸æ•°ï¼Œåˆ™ï¼š

1. çº¿æ€§æ€§ï¼ˆLinearityï¼‰ï¼š

$$
\mathbb{E}[aX + bY] = a \mathbb{E}[X] + b \mathbb{E}[Y]
$$

2. æ’ç­‰å‡½æ•°çš„æœŸæœ›ï¼š

$$
\mathbb{E}[c] = c \quad \text{(å¸¸æ•°çš„æœŸæœ›å°±æ˜¯å®ƒæœ¬èº«)}
$$

##### å¯¹äºå‡½æ•°çš„æœŸæœ›

è®¾ $X$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œæˆ‘ä»¬å¸Œæœ›è®¡ç®— $g(X)$ çš„æœŸæœ›ï¼š

$$
\mathbb{E}[g(X)]
$$


###### âœ… ç¦»æ•£å‹æ¨å¯¼

å‡è®¾ $X$ æ˜¯ç¦»æ•£å‹éšæœºå˜é‡ï¼Œå–å€¼é›†åˆä¸º $\{x_1, x_2, \dots, x_n\}$ï¼Œæ¦‚ç‡è´¨é‡å‡½æ•°ä¸º $p(x_i) = P(X = x_i)$ã€‚

æˆ‘ä»¬è¦è®¡ç®— $\mathbb{E}[g(X)]$ï¼Œæ„æ€æ˜¯å¯¹æ¯ä¸ªå¯èƒ½çš„å–å€¼ $x_i$ï¼Œå–å‡½æ•°å€¼ $g(x_i)$ï¼Œå†ä¹˜ä»¥å…¶å‡ºç°çš„æ¦‚ç‡ï¼Œå†åŠ æ€»èµ·æ¥ã€‚

**ğŸ” æ¨å¯¼ï¼š**

$$
\mathbb{E}[g(X)] = \sum_{i=1}^{n} g(x_i) \cdot P(X = x_i)
$$

> âœ… æœ¬è´¨æ˜¯â€œå¯¹éšæœºå˜é‡å‡½æ•°å€¼çš„åŠ æƒå¹³å‡â€ï¼Œæƒé‡å°±æ˜¯å…¶å‘ç”Ÿæ¦‚ç‡ã€‚


###### âœ… è¿ç»­å‹æ¨å¯¼ï¼š

è®¾ $X$ æ˜¯è¿ç»­å‹éšæœºå˜é‡ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º $f_X(x)$ï¼Œæˆ‘ä»¬å¸Œæœ›è®¡ç®— $\mathbb{E}[g(X)]$ã€‚

**ğŸ” æ¨å¯¼ï¼š**

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç§¯åˆ†çš„æ–¹å¼ï¼Œå°†æ‰€æœ‰å¯èƒ½çš„ $x$ ä¸Šçš„ $g(x)$ å€¼è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œæƒé‡æ˜¯ $f_X(x)$ï¼š

$$
\mathbb{E}[g(X)] = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx
$$

> âœ… æœ¬è´¨ä¸Šä»ç„¶æ˜¯â€œåŠ æƒå¹³å‡â€ï¼šå¯¹æ¯ä¸ªå¯èƒ½çš„ $x$ï¼Œç”¨å¯†åº¦å‡½æ•°ç»™å‡ºçš„æƒé‡ã€‚


###### ğŸ“Œ ä¸¾ä¾‹è®²è§£

ä¾‹å­ä¸€ï¼šğŸ² ç¦»æ•£å‹ä¾‹å­ã€‚

è®¾ $X$ æ˜¯æ·ä¸€ä¸ªå››é¢éª°å­çš„ç»“æœï¼ˆå‡åŒ€ç¦»æ•£åˆ†å¸ƒï¼‰ï¼Œå–å€¼ä¸º $\{1,2,3,4\}$ï¼Œæ¯ä¸ªæ¦‚ç‡ $p(x) = 1/4$ã€‚

æˆ‘ä»¬å®šä¹‰ $g(X) = X^2$ï¼Œè®¡ç®—å…¶æœŸæœ›ï¼š

$$
\mathbb{E}[X^2] = \sum_{x=1}^{4} x^2 \cdot \frac{1}{4} = \frac{1}{4}(1^2 + 2^2 + 3^2 + 4^2) = \frac{1}{4}(1 + 4 + 9 + 16) = \frac{30}{4} = 7.5
$$

---

ä¾‹å­äºŒï¼šğŸ“ˆ è¿ç»­å‹ä¾‹å­ã€‚

è®¾ $X \sim \text{Uniform}(0, 1)$ï¼Œå¯†åº¦å‡½æ•° $f_X(x) = 1$ for $x \in [0, 1]$ã€‚

å–å‡½æ•° $g(x) = x^2$ï¼Œè®¡ç®—ï¼š

$$
\mathbb{E}[X^2] = \int_0^1 x^2 \cdot 1 \, dx = \left[ \frac{x^3}{3} \right]_0^1 = \frac{1}{3}
$$

---

###### ğŸ” ä¸ç›´æ¥æœŸæœ›çš„åŒºåˆ«

* $\mathbb{E}[X]$ï¼šæ˜¯åŸå§‹éšæœºå˜é‡çš„æœŸæœ›ï¼›
* $\mathbb{E}[g(X)]$ï¼šæ˜¯å¯¹ $X$ åšå‡½æ•°å˜æ¢åçš„æœŸæœ›ï¼Œ**ä¸ç­‰äº** $g(\mathbb{E}[X])$ï¼ˆé™¤é $g$ æ˜¯çº¿æ€§å‡½æ•°ï¼‰ï¼

ä¾‹å¦‚ä¸Šä¾‹ä¸­ï¼š

* $\mathbb{E}[X] = 0.5$ï¼Œ
* $g(x) = x^2$ï¼Œ
* $g(\mathbb{E}[X]) = (0.5)^2 = 0.25 \neq \mathbb{E}[X^2] = \frac{1}{3}$

---

###### ğŸ“š æ€»ç»“

| ç±»å‹  | å®šä¹‰                                                | å…¬å¼   |
| --- | ------------------------------------------------- | ---- |
| ç¦»æ•£å‹ | $\mathbb{E}[g(X)] = \sum g(x_i) \cdot P(X = x_i)$ | åŠ æƒå¹³å‡ |
| è¿ç»­å‹ | $\mathbb{E}[g(X)] = \int g(x) \cdot f_X(x) \, dx$ | åŠ æƒç§¯åˆ† |


## è¯¯å·®ä¼ æ’­æ³•åˆ™ï¼ˆPropagation Lawsï¼‰

**Propagation laws**ï¼ˆè¯¯å·®ä¼ æ’­å®šå¾‹ï¼‰ï¼Œä¹Ÿå«ä½œ**uncertainty propagation**ï¼Œæ˜¯æŒ‡å½“ä½ é€šè¿‡æŸä¸ªå‡½æ•° $Y = g(X_1, X_2, \dots, X_n)$ æ¥è®¡ç®—ä¸€ä¸ªå˜é‡æ—¶ï¼Œå¦‚æœè¾“å…¥å˜é‡ $X_i$ éƒ½æœ‰ä¸ç¡®å®šæ€§ï¼ˆé€šå¸¸ä»¥æ–¹å·®æˆ–æ ‡å‡†å·®è¡¨ç¤ºï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬æƒ³çŸ¥é“è¾“å‡ºå˜é‡ $Y$ çš„ä¸ç¡®å®šæ€§æ˜¯å¤šå°‘ã€‚

è¿™å¥—ç†è®ºçš„æ ¸å¿ƒæ˜¯å¦‚ä½•æ¨å¯¼ï¼š

$$
\text{Var}(Y) \quad \text{æˆ–è€…} \quad \sigma_Y
$$

### âœ… ä¸€å…ƒæƒ…å½¢ï¼ˆåªæœ‰ä¸€ä¸ªå˜é‡ï¼‰
]
è®¾ $Y = g(X)$ï¼Œè€Œ $X$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå‡å€¼ $\mu_X$ï¼Œæ–¹å·® $\sigma_X^2$ï¼Œ

å¦‚æœ $g$ æ˜¯å…‰æ»‘å‡½æ•°ï¼ˆå¯å¾®ï¼‰ï¼Œå¹¶ä¸” $X$ çš„æ³¢åŠ¨ä¸å¤§ï¼Œå¯ä»¥ä½¿ç”¨ä¸€é˜¶æ³°å‹’å±•å¼€è¿‘ä¼¼ï¼š

$$
Y \approx g(\mu_X) + g'(\mu_X)(X - \mu_X)
$$

äºæ˜¯å¯ä»¥è¿‘ä¼¼å¾—åˆ°ï¼š

$$
\boxed{\text{Var}(Y) \approx \left(g'(\mu_X)\right)^2 \cdot \text{Var}(X)}
$$


### âœ… å¤šå…ƒæƒ…å½¢ï¼ˆå¤šä¸ªè¾“å…¥å˜é‡ï¼‰

è®¾è¾“å‡ºå˜é‡ï¼š

$$
Y = g(X_1, X_2, \dots, X_n)
$$

æˆ‘ä»¬å¯ä»¥å±•å¼€ä¸€é˜¶[æ³°å‹’å±•å¼€å¼](https://en.wikipedia.org/wiki/Taylor_series)ï¼š

$$
Y \approx g(\boldsymbol{\mu}) + \sum_{i=1}^{n} \frac{\partial g}{\partial x_i} (X_i - \mu_i)
$$

äºæ˜¯å¾—åˆ°æ–¹å·®çš„è¿‘ä¼¼ä¼ æ’­å¼ï¼š

$$
\boxed{\text{Var}(Y) \approx \sum_{i=1}^{n} \left(\frac{\partial g}{\partial x_i}\right)^2 \cdot \text{Var}(X_i) + 2 \sum_{i < j} \frac{\partial g}{\partial x_i} \cdot \frac{\partial g}{\partial x_j} \cdot \text{Cov}(X_i, X_j)}
$$

* å¦‚æœ $X_1, \dots, X_n$ **ç‹¬ç«‹**ï¼Œåæ–¹å·®é¡¹ä¸º 0ï¼Œç®€åŒ–ä¸ºï¼š

$$
\boxed{\text{Var}(Y) \approx \sum_{i=1}^{n} \left(\frac{\partial g}{\partial x_i}\right)^2 \cdot \text{Var}(X_i)}
$$

### ğŸ“ ç¤ºä¾‹è®²è§£

#### ğŸ¯ ä¾‹å­ 1ï¼šä¹˜æ³•ä¼ æ’­

è®¾ $Y = XY$ï¼Œå…¶ä¸­ $X \sim N(\mu_X, \sigma_X^2)$ï¼Œ$Y \sim N(\mu_Y, \sigma_Y^2)$ï¼Œä¸” $X, Y$ ç‹¬ç«‹ã€‚

* $g(X, Y) = XY$

æˆ‘ä»¬æœ‰ï¼š

* $\frac{\partial g}{\partial X} = Y$
* $\frac{\partial g}{\partial Y} = X$

ä»£å…¥ä¼ æ’­å…¬å¼ï¼ˆç”¨æœŸæœ›è¿‘ä¼¼å¯¼æ•°ï¼‰ï¼š

$$
\text{Var}(XY) \approx (\mu_Y)^2 \sigma_X^2 + (\mu_X)^2 \sigma_Y^2
$$

---

#### ğŸ“Š ä¾‹å­ 2ï¼šæ¸©åº¦æ¢ç®—ï¼ˆçº¿æ€§å˜æ¢ï¼‰

æ‘„æ°æ¸©åº¦ $C \sim N(\mu, \sigma^2)$ï¼Œæ¢ç®—æˆåæ°æ¸©åº¦ï¼š

$$
F = 1.8 C + 32
$$

ç”±äºæ˜¯çº¿æ€§å˜æ¢ï¼Œç›´æ¥ä½¿ç”¨å…¬å¼ï¼š

$$
\text{Var}(F) = (1.8)^2 \cdot \text{Var}(C)
$$

---

### ğŸ“Œ å°ç»“è¡¨æ ¼

| å½¢å¼     | æè¿°                                                                               |
| ------ | -------------------------------------------------------------------------------- |
| ä¸€å…ƒå˜æ¢   | $\text{Var}(Y) \approx (g'(\mu))^2 \cdot \text{Var}(X)$                          |
| å¤šå…ƒç‹¬ç«‹å˜é‡ | $\text{Var}(Y) \approx \sum (\partial g / \partial x_i)^2 \cdot \text{Var}(X_i)$ |
| å¤šå…ƒç›¸å…³å˜é‡ | åŠ ä¸Šåæ–¹å·®é¡¹                                                                           |
| çº¿æ€§å˜æ¢   | ç²¾ç¡®æˆç«‹ï¼š$Y = aX + b \Rightarrow \text{Var}(Y) = a^2 \cdot \text{Var}(X)$            |

---

### âš ï¸ æ³¨æ„äº‹é¡¹

* è¯¥æ–¹æ³•æ˜¯**è¿‘ä¼¼**æ–¹æ³•ï¼Œå‡†ç¡®æ€§å–å†³äºå‡½æ•°åœ¨å±€éƒ¨çš„çº¿æ€§ç¨‹åº¦ï¼›
* é€‚ç”¨äºè¾“å…¥è¯¯å·®**è¾ƒå°**çš„æƒ…å†µï¼›
* æ›´é«˜é˜¶è¯¯å·®å¯ä»¥ç”¨**äºŒé˜¶æ³°å‹’å±•å¼€**æ”¹è¿›ï¼›
* è‹¥åˆ†å¸ƒä¸æ˜¯æ­£æ€åˆ†å¸ƒï¼Œç»“æœä¾ç„¶æ˜¯è¿‘ä¼¼çš„ã€‚

### ä¸ºä»€ä¹ˆå¯ä»¥ç”¨ X åœ¨å‡å€¼é™„è¿‘çš„çº¿æ€§è¿‘ä¼¼ï¼Ÿ

æˆ‘ä»¬è¯´ï¼š

> è®¾éšæœºå˜é‡ $X$ æœ‰å‡å€¼ $\mu_X$ï¼Œå‡½æ•° $Y = g(X)$ï¼Œ
> é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ç”¨ **X åœ¨å‡å€¼é™„è¿‘çš„çº¿æ€§è¿‘ä¼¼**ï¼š

$$
g(X) \approx g(\mu_X) + g'(\mu_X)(X - \mu_X)
$$

ä½ é—®çš„æ˜¯ï¼š**è¿™ä¸€æ­¥ä¸ºä»€ä¹ˆå¯ä»¥è¿™ä¹ˆåšï¼Ÿæœ‰ä»€ä¹ˆä¾æ®ï¼Ÿ**


âœ… è¿™ä¸€æ­¥å…¶å®æ˜¯ä½¿ç”¨äº†â€œæ³°å‹’ä¸€é˜¶å±•å¼€â€ï¼ˆTaylor Expansionï¼‰

#### ğŸŒŸ 1. ä»€ä¹ˆæ˜¯æ³°å‹’å±•å¼€ï¼Ÿ

æ³°å‹’å±•å¼€æ˜¯ç”¨ä¸€ä¸ªå‡½æ•°åœ¨æŸç‚¹çš„å¯¼æ•°ä¿¡æ¯ï¼Œè¿‘ä¼¼è¡¨ç¤ºè¿™ä¸ªå‡½æ•°åœ¨é™„è¿‘çš„å€¼ã€‚

å¯¹ä¸€ä¸ªå¯å¯¼å‡½æ•° $g(x)$ï¼Œåœ¨ç‚¹ $x = a$ å¤„è¿›è¡Œæ³°å‹’å±•å¼€ï¼Œæœ‰ï¼š

$$
g(x) = g(a) + g'(a)(x - a) + \frac{g''(a)}{2!}(x - a)^2 + \cdots
$$

å¦‚æœæˆ‘ä»¬åªä¿ç•™**ä¸€é˜¶é¡¹**ï¼ˆä¹Ÿå°±æ˜¯å¯¼æ•°é‚£ä¸€é¡¹ï¼‰ï¼Œå°±å«ï¼š

$$
\boxed{
g(x) \approx g(a) + g'(a)(x - a)
}
\quad \text{ï¼ˆä¸€é˜¶æ³°å‹’å±•å¼€ï¼‰}
$$



#### ğŸŒŸ 2. ä¸ºä»€ä¹ˆåœ¨è¯¯å·®ä¼ æ’­é‡Œè¿™ä¹ˆç”¨ï¼Ÿ

è®¾ $X$ æ˜¯ä¸€ä¸ª**å¸¦å™ªå£°çš„è¾“å…¥é‡**ï¼Œæˆ‘ä»¬çŸ¥é“å®ƒçš„å‡å€¼æ˜¯ $\mu_X$ï¼Œæ–¹å·®æ˜¯ $\sigma_X^2$ï¼Œä½†å®ƒæ€»æœ‰ä¸€äº›æ³¢åŠ¨ã€‚

æˆ‘ä»¬å…³å¿ƒçš„æ˜¯ï¼šå½“ $X$ åœ¨ $\mu_X$ é™„è¿‘æ³¢åŠ¨æ—¶ï¼Œè¾“å‡º $Y = g(X)$ ä¼šæ€æ ·æ³¢åŠ¨ï¼Ÿ

äºæ˜¯æˆ‘ä»¬åœ¨ $\mu_X$ é™„è¿‘è¿›è¡Œçº¿æ€§è¿‘ä¼¼ï¼š

$$
g(X) \approx g(\mu_X) + g'(\mu_X)(X - \mu_X)
$$

è¿™ä¸ªå¼å­çš„æ„ä¹‰æ˜¯ï¼š

* $g(\mu_X)$ï¼šæ˜¯å¹³å‡è¾“å‡º
* $g'(\mu_X)(X - \mu_X)$ï¼šæ˜¯è¾“å…¥ $X$ æ³¢åŠ¨å¸¦æ¥çš„**çº¿æ€§å˜åŒ–**

è¿™ç§çº¿æ€§é€¼è¿‘åœ¨ $X$ å˜åŠ¨ä¸å¤§çš„æ—¶å€™æ˜¯å¾ˆåˆç†çš„ â€”â€” å°±åƒæˆ‘ä»¬ç”»å›¾æ—¶ç”¨ç›´çº¿è¿‘ä¼¼æ›²çº¿çš„ä¸€å°æ®µã€‚


#### ğŸ“ˆ å›¾åƒç›´è§‚ç†è§£

1. ç”»ä¸€æ¡å‡½æ•°æ›²çº¿ $y = g(x)$ï¼Œæ¯”å¦‚ $g(x) = \log x$
2. åœ¨ $x = \mu_X$ ç”»ä¸€æ¡åˆ‡çº¿
3. çœ‹çœ‹è¿™æ¡åˆ‡çº¿åœ¨é™„è¿‘æ˜¯å¦ä¸åŸå‡½æ•°å·®ä¸å¤š

è¿™å°±æ˜¯æ³°å‹’å±•å¼€çš„ä¸€é˜¶é€¼è¿‘ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ï¼š

> å¦‚æœä½ åªå…³å¿ƒâ€œå‡½æ•°åœ¨å¹³å‡ç‚¹é™„è¿‘æ˜¯æ€ä¹ˆå˜çš„â€ï¼Œé‚£ä¹ˆåªçœ‹å¯¼æ•°ï¼ˆæ–œç‡ï¼‰å°±è¶³å¤Ÿã€‚


#### ğŸ” ä¸ºä»€ä¹ˆä¸€é˜¶å°±è¶³å¤Ÿï¼Ÿ

å› ä¸ºæˆ‘ä»¬å…³æ³¨çš„æ˜¯**æ–¹å·®**çš„ä¼ æ’­ï¼š

* æ–¹å·®åªå–å†³äºå‡½æ•°çš„**ä¸€é˜¶å˜åŒ–é€Ÿåº¦**ï¼Œå³ $g'(x)$ï¼›
* å¦‚æœä½ ä¿ç•™é«˜é˜¶é¡¹ï¼Œæ¯”å¦‚ $(x - \mu)^2$ï¼Œä½ éœ€è¦çŸ¥é“é«˜é˜¶å¯¼æ•°ï¼Œåˆ†æä¼šå¤æ‚å¾ˆå¤šï¼›
* åœ¨è¯¯å·®å¾ˆå°ï¼ˆå³ $X$ çš„æ³¢åŠ¨å¾ˆå°ï¼‰æ—¶ï¼Œä¸€é˜¶é¡¹å°±ä¸»å¯¼äº†è¯¯å·®ä¼ æ’­çš„è¡Œä¸ºã€‚


#### âœ… æ€»ç»“ä¸€å¥è¯ï¼š

æˆ‘ä»¬ç”¨ï¼š

$$
g(X) \approx g(\mu_X) + g'(\mu_X)(X - \mu_X)
$$

æ˜¯å› ä¸ºï¼š

* è¿™æ˜¯**ä¸€é˜¶æ³°å‹’å±•å¼€**ï¼Œç”¨äºåœ¨ $\mu_X$ é™„è¿‘é€¼è¿‘å‡½æ•°ï¼›
* å½“è¾“å…¥è¯¯å·®å¾ˆå°æ—¶ï¼Œè¿™æ˜¯éå¸¸å¥½çš„è¿‘ä¼¼ï¼›
* è¿™ä¸ªé€¼è¿‘èƒ½å¸®åŠ©æˆ‘ä»¬åˆ†æè¾“å‡º $Y$ çš„æ³¢åŠ¨ï¼ˆå³æ–¹å·®ï¼‰æ˜¯å¦‚ä½•ç”±è¾“å…¥ $X$ çš„æ³¢åŠ¨é€ æˆçš„ã€‚



```python
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰éçº¿æ€§å‡½æ•°å’Œå®ƒçš„å¯¼æ•°
def g(x):
    return np.log(x)

def g_prime(x):
    return 1 / x

# å–å‡å€¼ç‚¹ mu
mu = 1.0
x = np.linspace(0.5, 2.0, 400)

# åŸå‡½æ•°å€¼
y = g(x)

# ä¸€é˜¶çº¿æ€§è¿‘ä¼¼ï¼šåœ¨ mu é™„è¿‘
y_approx = g(mu) + g_prime(mu) * (x - mu)

# ç»˜å›¾
plt.figure(figsize=(8, 6))
plt.plot(x, y, label=r'$g(x) = \log(x)$', color='blue')
plt.plot(x, y_approx, '--', label='Linear approximation at $\mu=1$', color='red')
plt.axvline(mu, color='gray', linestyle=':', label=r'$\mu_X$')

# æ ‡è®°ç‚¹
plt.scatter([mu], [g(mu)], color='black', zorder=5)
plt.text(mu+0.02, g(mu)+0.1, r'$g(\mu_X)$', fontsize=12)

plt.title('Function $g(x)$ and Its Linear Approximation at $\mu_X$')
plt.xlabel('x')
plt.ylabel('g(x)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

```

    <>:24: SyntaxWarning: invalid escape sequence '\m'
    <>:31: SyntaxWarning: invalid escape sequence '\m'
    <>:24: SyntaxWarning: invalid escape sequence '\m'
    <>:31: SyntaxWarning: invalid escape sequence '\m'
    /var/folders/mx/684cy0qs5zd3c_pdx4wy4pkc0000gn/T/ipykernel_15262/692038390.py:24: SyntaxWarning: invalid escape sequence '\m'
      plt.plot(x, y_approx, '--', label='Linear approximation at $\mu=1$', color='red')
    /var/folders/mx/684cy0qs5zd3c_pdx4wy4pkc0000gn/T/ipykernel_15262/692038390.py:31: SyntaxWarning: invalid escape sequence '\m'
      plt.title('Function $g(x)$ and Its Linear Approximation at $\mu_X$')



    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_110_1.png)
    


## æ ·æœ¬å‡å€¼ç®—å­ï¼ˆSample mean operatorï¼‰

åœ¨ç»Ÿè®¡ä¸­ï¼Œ**sample mean operator** å°±æ˜¯å°†ä¸€ä¸ªéšæœºå˜é‡çš„è‹¥å¹²ä¸ªç‹¬ç«‹æ ·æœ¬æ±‚å¹³å‡çš„æ“ä½œï¼š


ç»™å®šæŸä¸ªéšæœºå˜é‡ $X$ï¼Œæˆ‘ä»¬ä»ä¸­ç‹¬ç«‹é‡‡æ · $n$ ä¸ªæ ·æœ¬ï¼š

$$
X_1, X_2, \dots, X_n \sim \text{i.i.d. from } X
$$

åˆ™æ ·æœ¬å‡å€¼ï¼ˆsample meanï¼‰å®šä¹‰ä¸ºï¼š

$$
\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i
$$

è¿™ä¸ªæ“ä½œå°±å«åš sample mean operatorï¼šå®ƒæ¥å—ä¸€ä¸ªæ ·æœ¬åºåˆ—ï¼Œè¾“å‡ºå¹³å‡å€¼ã€‚


> **Sample mean operator æ˜¯ä»æ•°æ®ä¸­ä¼°è®¡æ€»ä½“å‡å€¼çš„æœ€åŸºæœ¬å·¥å…·ï¼Œå…·æœ‰æ— åæ€§ã€æ–¹å·®éšæ ·æœ¬æ•°é™ä½ã€é›†ä¸­æ€§å’Œæ­£æ€æ€§ç­‰é‡è¦æ€§è´¨ã€‚**

### ğŸ“Œ sample mean çš„é‡è¦æ€§è´¨

#### 1. **æ— åæ€§ï¼ˆUnbiasednessï¼‰**

$$
\mathbb{E}[\bar{X}_n] = \mathbb{E}\left[\frac{1}{n} \sum_{i=1}^n X_i\right] = \frac{1}{n} \sum_{i=1}^n \mathbb{E}[X_i] = \mu
$$

ğŸ‘‰ è¯´æ˜ï¼šæ ·æœ¬å‡å€¼æ˜¯æ€»ä½“å‡å€¼çš„æ— åä¼°è®¡ã€‚


#### 2. **æ–¹å·®**

$$
\mathrm{Var}[\bar{X}_n] = \frac{1}{n^2} \sum_{i=1}^n \mathrm{Var}[X_i] = \frac{\sigma^2}{n}
$$

ğŸ‘‰ æ„ä¹‰ï¼šæ ·æœ¬å‡å€¼çš„æ–¹å·®éšç€æ ·æœ¬æ•°å¢åŠ è€Œå‡å°ã€‚


#### 3. **é›†ä¸­æ€§ï¼ˆå¤§æ•°å®šå¾‹ï¼‰**

æ ¹æ®**å¤§æ•°å®šå¾‹**ï¼ˆLaw of Large Numbersï¼‰ï¼š

$$
\bar{X}_n \xrightarrow{a.s.} \mu \quad \text{as } n \to \infty
$$

ğŸ‘‰ æ„ä¹‰ï¼šæ ·æœ¬å‡å€¼å‡ ä¹å¿…ç„¶æ”¶æ•›åˆ°æ€»ä½“å‡å€¼ã€‚

---

#### 4. **è¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼ˆä¸­å¿ƒæé™å®šç†ï¼‰**

å½“ $n$ å¾ˆå¤§æ—¶ï¼Œæ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼š

$$
\bar{X}_n \approx \mathcal{N}\left(\mu, \frac{\sigma^2}{n} \right)
$$

ğŸ‘‰ æ„ä¹‰ï¼šæ— è®ºåŸå§‹åˆ†å¸ƒå¦‚ä½•ï¼Œæ ·æœ¬å‡å€¼åœ¨å¤§æ ·æœ¬ä¸‹è¿‘ä¼¼æœä»æ­£æ€åˆ†å¸ƒã€‚



```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

# å‡è®¾åŸå§‹åˆ†å¸ƒ X æ˜¯æŒ‡æ•°åˆ†å¸ƒï¼ˆä¸æ˜¯æ­£æ€ï¼‰
X = np.random.exponential(scale=1.0, size=(10000,))  # åŸå§‹æ ·æœ¬

# ä¸åŒæ ·æœ¬æ•°é‡ä¸‹çš„ sample mean
sample_sizes = [1, 5, 10, 30, 100]
means = []

for n in sample_sizes:
    sample_means = [np.mean(np.random.choice(X, n, replace=False)) for _ in range(1000)]
    means.append(sample_means)

# å¯è§†åŒ–ä¸åŒ n ä¸‹çš„ sample mean åˆ†å¸ƒ
fig, axs = plt.subplots(1, len(sample_sizes), figsize=(18, 3))

for i, n in enumerate(sample_sizes):
    axs[i].hist(means[i], bins=30, color='skyblue', edgecolor='black', density=True)
    axs[i].set_title(f'n={n}')
    axs[i].axvline(np.mean(X), color='red', linestyle='--', label='True Mean')
    axs[i].legend()

plt.suptitle('Sample Mean Distribution as Sample Size Increases')
plt.tight_layout()
plt.show()
```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_112_0.png)
    


### åˆ©ç”¨ sample mean operator æ¥è¿‘ä¼¼è®¡ç®—ä¸€äº›æ— æ³•è§£ææ±‚è§£çš„ç§¯åˆ†

è¿™å…¶å®å°±æ˜¯**è’™ç‰¹å¡æ´›ç§¯åˆ†ï¼ˆMonte Carlo Integrationï¼‰**çš„æ ¸å¿ƒæ€æƒ³ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ sample mean operator æ¥**è¿‘ä¼¼è®¡ç®—ä¸€äº›æ— æ³•è§£ææ±‚è§£çš„ç§¯åˆ†**ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç»´æˆ–å¤æ‚å‡½æ•°æƒ…å½¢ä¸‹ã€‚

**ğŸ§  æ€è·¯æ€»è§ˆï¼šç”¨æ ·æœ¬å‡å€¼è¿‘ä¼¼ç§¯åˆ†**

è®¾æˆ‘ä»¬æƒ³è®¡ç®—ä¸€ä¸ªç§¯åˆ†ï¼š

$$
I = \int_a^b f(x)\,dx
$$

è‹¥è¿™ä¸ªç§¯åˆ†æ— æ³•è§£ææ±‚å‡ºï¼Œå¯ä»¥æŠŠå®ƒçœ‹æˆæœŸæœ›ï¼š

$$
I = (b - a) \cdot \mathbb{E}_{X \sim \mathcal{U}(a, b)}[f(X)]
$$

å…¶ä¸­ï¼Œ$X$ æ˜¯å‡åŒ€éšæœºå˜é‡

#### âœ… ä¸ºä»€ä¹ˆå¯ä»¥è¿™æ ·å˜ï¼Ÿ

å¦‚æœ $X \sim \mathcal{U}(a, b)$ï¼Œé‚£ä¹ˆå®ƒçš„å¯†åº¦æ˜¯ï¼š

$$
p(x) = \frac{1}{b-a}, \quad x \in [a, b]
$$

æ‰€ä»¥ï¼š

$$
\mathbb{E}[f(X)] = \int_a^b f(x) \cdot p(x)\,dx = \int_a^b f(x) \cdot \frac{1}{b-a} \, dx
= \frac{1}{b-a} \int_a^b f(x)\,dx
$$

æ•´ç†å¾—ï¼š

$$
\int_a^b f(x)\,dx = (b - a) \cdot \mathbb{E}[f(X)]
$$


#### ğŸ“Œ ç”¨æ ·æœ¬å‡å€¼è¿‘ä¼¼æœŸæœ›

æˆ‘ä»¬å¯ä»¥ä» $X \sim \mathcal{U}(a, b)$ ä¸­é‡‡æ · $n$ ä¸ªæ ·æœ¬ $x_1, x_2, \dots, x_n$ï¼Œè®¡ç®—ï¼š

$$
\mathbb{E}[f(X)] \approx \frac{1}{n} \sum_{i=1}^{n} f(x_i)
$$

å› æ­¤ï¼š

$$
\int_a^b f(x)\,dx \approx \frac{b - a}{n} \sum_{i=1}^n f(x_i)
$$


#### ğŸ“š æ‹“å±•ï¼šä¸ºä»€ä¹ˆè¿™å¾ˆæœ‰ç”¨ï¼Ÿ

* åœ¨**é«˜ç»´ç©ºé—´**æˆ–**å¤æ‚ç§¯åˆ†ï¼ˆæ¯”å¦‚è´å¶æ–¯æ¨æ–­ä¸­ï¼‰**ï¼Œæ²¡æœ‰è§£æè§£æ—¶ï¼š

  * ä½ æ— æ³•ç”¨ç‰›é¡¿ç§¯åˆ†ï¼›
  * ä½ å¯ä»¥åªä¾èµ–æ ·æœ¬ï¼


#### âœ… æ€»ç»“ä¸€å¥è¯

> **é€šè¿‡æŠŠç§¯åˆ†è½¬åŒ–ä¸ºæœŸæœ›ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ sample mean operator å’Œå‡åŒ€é‡‡æ ·æ¥è¿‘ä¼¼ä»»ä½•æ— æ³•è§£ææ±‚è§£çš„ç§¯åˆ† â€”â€” è¿™å°±æ˜¯è’™ç‰¹å¡æ´›ç§¯åˆ†çš„æ ¸å¿ƒã€‚**


#### âœ… Python å®ç°ï¼šMonte Carlo ç§¯åˆ†

æˆ‘ä»¬æ¥è¯•è¯•è®¡ç®—ï¼š

$$
I = \int_0^1 e^{-x^2}\,dx
$$

è¿™é‡Œï¼Œ
* $a = 0, b = 1$
* $f(x) = e^{-x^2}$

è¿™æ˜¯æ— æ³•è§£ææ±‚å‡ºçš„ï¼ˆå…¶å®è¿™å°±æ˜¯è¯¯å·®å‡½æ•° erf çš„ä¸€éƒ¨åˆ†ï¼‰ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ•°å€¼ä¼°ç®—å®ƒã€‚



```python
import numpy as np
import matplotlib.pyplot as plt

# è¢«ç§¯å‡½æ•°
def f(x):
    return np.exp(-x**2)

# ç§¯åˆ†åŒºé—´
a, b = 0, 1

# Monte Carlo é‡‡æ ·æ•°é‡
n = 10000
x_samples = np.random.uniform(a, b, n) # 1ï¸âƒ£ å‡åŒ€é‡‡æ · [a, b] åŒºé—´

# æ ·æœ¬å‡å€¼ä¼°è®¡
estimate = (b - a) * np.mean(f(x_samples)) # 2ï¸âƒ£ è®¡ç®—æ ·æœ¬å‡å€¼å¹¶ä¹˜ä»¥åŒºé—´é•¿åº¦

print(f"[Estimate Value] Monte Carlo estimate of âˆ«â‚€Â¹ e^(-xÂ²) dx â‰ˆ {estimate:.6f}")

# è®¡ç®—çœŸå®å€¼
from scipy.special import erf
true_val = np.sqrt(np.pi)/2 * erf(1)
print(f"[True Value] The true value of âˆ«â‚€Â¹ e^(-xÂ²) dx â‰ˆ {true_val:.6f}")
```

    [Estimate Value] Monte Carlo estimate of âˆ«â‚€Â¹ e^(-xÂ²) dx â‰ˆ 0.743534
    [True Value] The true value of âˆ«â‚€Â¹ e^(-xÂ²) dx â‰ˆ 0.746824


# é‡‡æ ·æ–¹æ³•ï¼ˆSample Methodsï¼‰

**åˆè§ï¼š**
- [Sampling from discrete distributions](./extra-Sampling%20from%20discrete_continuous%20distributions.pdf) | [Online version](https://dept.stat.lsa.umich.edu/~jasoneg/Stat406/lab5.pdf)

## åå‡½æ•°æ³•ï¼ˆInverse Transform Samplingï¼‰

ç»™å®šæ¦‚ç‡å¯†åº¦/è´¨é‡å‡½æ•° $f_X(x)$ï¼Œåˆ™ï¼š
1. è®¡ç®— $F_X(x)$
2. åŸºäºå‡åŒ€éšæœºå˜é‡è·å¾—ä¸€ä¸ªå€¼
3. è®¡ç®— $F_X^{-1}(u)$ ä»è€Œå¾—åˆ° $x$ã€‚è¿™ä¸ª $x$ å°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„é‡‡æ ·å€¼ã€‚

æƒ³è±¡ä½ ç”»äº† CDF æ›²çº¿ $F_X(x)$ï¼Œæ¨ªè½´æ˜¯$x$ï¼Œçºµè½´æ˜¯æ¦‚ç‡$[0,1]$ã€‚
- éšæœºç”Ÿæˆä¸€ä¸ª $u \in [0,1]$
- æ‰¾åˆ°è¿™ä¸ª $u$ åœ¨ CDF ä¸Šå¯¹åº”çš„æ¨ªåæ ‡ï¼šè¿™å°±æ˜¯ä½ è¦çš„æ ·æœ¬ $x$

æ³¨æ„ï¼š
> å½“æˆ‘ä»¬å·²ç»çŸ¥é“äº†CDFï¼Œå¹¶ä¸”å¯ä»¥ä»è¿™ä¸ªCDFè·å¾—ä¸€ä¸ªå°é—­å½¢å¼ï¼ˆclose formï¼‰çš„å‡½æ•°ï¼Œå°±å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•


```python
import numpy as np
import plotly.graph_objs as go
from plotly.subplots import make_subplots

# ç›®æ ‡åˆ†å¸ƒï¼šæŒ‡æ•°åˆ†å¸ƒ X ~ Exp(Î»=1)
x_vals = np.linspace(0, 6, 500)
cdf_vals = 1 - np.exp(-x_vals)  # CDF of Exp(1)
inv_cdf = lambda u: -np.log(1 - u)  # x = åå‡½æ•° Fâ»Â¹(u)

# éšæœºé‡‡æ · 5 ä¸ªå‡åŒ€æ•°
np.random.seed(42)
u_samples = np.sort(np.random.uniform(0, 1, 5))
x_samples = inv_cdf(u_samples)

# åˆ›å»ºå­å›¾ï¼šå·¦ä¾§æ˜¾ç¤º CDF æ˜ å°„è¿‡ç¨‹ï¼Œå³ä¾§æ˜¾ç¤º Histogram
fig = make_subplots(rows=1, cols=2,
                    subplot_titles=("Inverse Transform Sampling", "Histogram of Transformed Samples"),
                    column_widths=[0.6, 0.4])

# å·¦å›¾ï¼šCDF æ›²çº¿
fig.add_trace(go.Scatter(x=x_vals, y=cdf_vals, mode='lines', name='CDF F(x)', line=dict(color='blue')),
              row=1, col=1)

# æ·»åŠ æ¯ä¸ª u å€¼çš„æ°´å¹³çº¿å’Œå¯¹åº”çš„ x æ˜ å°„
for u, x in zip(u_samples, x_samples):
    fig.add_trace(go.Scatter(x=[0, x], y=[u, u], mode='lines',
                             line=dict(dash='dot', color='gray'), showlegend=False), row=1, col=1)
    fig.add_trace(go.Scatter(x=[x], y=[u], mode='markers+text',
                             marker=dict(color='red', size=8),
                             text=[f"x={x:.2f}"], textposition='top right', showlegend=False), row=1, col=1)

# æ·»åŠ  u æ ·æœ¬ç‚¹
fig.add_trace(go.Scatter(x=[0]*len(u_samples), y=u_samples, mode='markers',
                         marker=dict(symbol='line-ns-open', color='green', size=10),
                         name='u ~ Uniform(0,1)'), row=1, col=1)

# å³å›¾ï¼šx æ ·æœ¬çš„ç›´æ–¹å›¾
fig.add_trace(go.Histogram(x=x_samples, nbinsx=10, name='Sampled X', marker_color='orange'), row=1, col=2)

# å¸ƒå±€è®¾ç½®
fig.update_layout(height=500, width=900, title_text="Inverse Transform Sampling Visualization (Exponential RV)",
                  showlegend=True)
fig.update_xaxes(title_text="x", row=1, col=1)
fig.update_yaxes(title_text="F(x) or u", row=1, col=1)
fig.update_xaxes(title_text="Sampled x", row=1, col=2)
fig.update_yaxes(title_text="Frequency", row=1, col=2)

fig.show()


```



### ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥ç”¨å‡åŒ€åˆ†å¸ƒæ¥é‡‡æ ·å…¶ä»–ä»»æ„åˆ†å¸ƒï¼ˆæ¯”å¦‚ä¼¯åŠªåˆ©ã€æŒ‡æ•°ã€æ­£æ€ç­‰ï¼‰


è¿™èƒŒåçš„ç†è®ºåŸºç¡€æ˜¯ **æ¦‚ç‡è®ºä¸­å…³äº**â€¯**åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰å’Œå…¶åå‡½æ•°çš„æ€§è´¨**ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šæ¦‚ç‡ç§¯åˆ†å˜æ¢ï¼ˆProbability Integral Transformï¼‰

**å®šç†ï¼ˆæ¦‚ç‡ç§¯åˆ†å˜æ¢ / Probability Integral Transformï¼‰**ï¼š

> è‹¥éšæœºå˜é‡ $X$ çš„åˆ†å¸ƒå‡½æ•°æ˜¯ $F_X(x)$ï¼Œä¸” $F_X$ æ˜¯ä¸¥æ ¼å•è°ƒçš„è¿ç»­å‡½æ•°ï¼Œé‚£ä¹ˆ
> **$U = F_X(X) \sim \text{Uniform}(0,1)$**ï¼Œ
> åè¿‡æ¥ï¼Œ**$X = F_X^{-1}(U)$** ä¹Ÿæœä»åŸå§‹åˆ†å¸ƒ $X$ã€‚

æ¢å¥è¯è¯´ï¼š

* **ä½ å¯ä»¥æŠŠä»»ä½•åˆ†å¸ƒçš„é‡‡æ ·é—®é¢˜ï¼Œå˜æˆ Uniform(0,1) çš„é‡‡æ ·é—®é¢˜ + ä¸€ä¸ªåå‡½æ•°å˜æ¢ã€‚**



#### ğŸ§  ä¸ºä»€ä¹ˆè¿™ä¸ªæˆç«‹ï¼Ÿ

æˆ‘ä»¬æ¥ç›´è§‚æ¨å¯¼ä¸€ä¸‹ç¬¬äºŒä¸ªæ–¹å‘ï¼ˆä¹Ÿæ˜¯æˆ‘ä»¬ç”¨æ¥â€œç”Ÿæˆä»»æ„åˆ†å¸ƒâ€çš„æ–¹å‘ï¼‰ï¼š

**å‡è®¾ï¼š**

* $U \sim \text{Uniform}(0,1)$
* è®¾ $Y = F^{-1}(U)$ï¼Œæˆ‘ä»¬æƒ³è¯æ˜ $Y \sim F$

**è¯æ˜ï¼š**

æˆ‘ä»¬æ¥è®¡ç®— $Y = F^{-1}(U)$ çš„ CDFï¼Œä¹Ÿå°±æ˜¯ï¼š

$$
P(Y \leq y)
$$

ç”±äº $Y = F^{-1}(U)$ï¼Œé‚£ä¹ˆï¼š

$$
P(Y \leq y) = P(F^{-1}(U) \leq y)
$$


**âœ… ç¬¬ä¸€æ­¥ï¼šè¿ç”¨åå‡½æ•°çš„å•è°ƒæ€§**

**å‰ææ¡ä»¶ï¼š$F$ æ˜¯è¿ç»­ã€ä¸¥æ ¼é€’å¢çš„å‡½æ•°**ï¼ˆè¿™æ˜¯ä¿è¯åå‡½æ•°å­˜åœ¨å¹¶å•è°ƒçš„å…³é”®ï¼‰ã€‚

æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¯¹ä¸ç­‰å¼ $F^{-1}(U) \leq y$ åº”ç”¨å‡½æ•° $F$ï¼Œå˜æˆï¼š

$$
F^{-1}(U) \leq y \quad \Leftrightarrow \quad U \leq F(y)
$$

è¿™æ˜¯éå¸¸å…³é”®çš„ä¸€æ­¥ï¼æˆ‘ä»¬æŠŠ **â€œå…³äº $Y$â€ çš„äº‹ä»¶** è½¬åŒ–æˆäº† **â€œå…³äº $U$â€ çš„äº‹ä»¶**ã€‚


**âœ… ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ $U \sim \text{Uniform}(0,1)$**

$$
P(U \leq F(y)) = F(y)
$$

ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºï¼š

* $U \sim \text{Uniform}(0,1)$
* æ‰€ä»¥ **$P(U \leq u) = u$**ï¼Œå¯¹äº $u \in [0,1]$
* è€Œ $F(y) \in [0,1]$ï¼Œå› ä¸º $F$ æ˜¯ä¸€ä¸ªåˆæ³•çš„åˆ†å¸ƒå‡½æ•°

æ‰€ä»¥ï¼š

$$
P(U \leq F(y)) = F(y)
$$


**âœ… ç»¼åˆèµ·æ¥ï¼š**

$$
P(Y \leq y) = P(F^{-1}(U) \leq y) = P(U \leq F(y)) = F(y)
$$

å› æ­¤ï¼Œ**$Y$ çš„åˆ†å¸ƒå‡½æ•°å°±æ˜¯ $F$**ï¼Œæ‰€ä»¥æˆ‘ä»¬è¯´ $Y \sim F$ã€‚


#### ğŸ¯ åº”ç”¨ä¸¾ä¾‹ï¼š

* **ä¼¯åŠªåˆ©é‡‡æ ·ï¼š**

  * å¦‚æœ $U < p$ï¼Œæˆ‘ä»¬å°±è¾“å‡º 1ï¼Œå¦åˆ™è¾“å‡º 0ï¼Œç›¸å½“äºï¼š

    $$
    F^{-1}(u) = 
    \begin{cases}
    1 & u < p \\
    0 & u \ge p
    \end{cases}
    $$

* **æŒ‡æ•°åˆ†å¸ƒé‡‡æ ·ï¼š**

  * Exponential çš„ CDF æ˜¯ $F(x) = 1 - e^{-\lambda x}$
  * åå‡½æ•°æ˜¯ $F^{-1}(u) = -\frac{1}{\lambda} \ln(1 - u)$


#### ğŸ’¡ ç›´è§‰æ€»ç»“

ä½ å¯ä»¥æŠŠ Uniform(0,1) ç†è§£ä¸ºâ€œæŠ½ç­¾â€ï¼Œç„¶åç”¨æ¯ç§åˆ†å¸ƒçš„**åˆ†å¸ƒå‡½æ•°**å‘Šè¯‰æˆ‘ä»¬è¿™ä¸ªâ€œæŠ½ç­¾å·â€å¯¹åº”ä»€ä¹ˆâ€œäº‹ä»¶â€æˆ–â€œæ•°å€¼â€ã€‚


#### ğŸ“Š ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è¿™æ ·åšï¼Ÿ

1. **Uniform(0,1)** æ˜¯æœ€å®¹æ˜“æ¨¡æ‹Ÿçš„åˆ†å¸ƒï¼šå‡ ä¹æ‰€æœ‰è¯­è¨€éƒ½æœ‰ `random()`ã€‚
2. å¦‚æœèƒ½æŠŠä»»ä½•åˆ†å¸ƒè½¬åŒ–ä¸º Uniformï¼Œå°±èƒ½ç»Ÿä¸€é‡‡æ ·æµç¨‹ï¼Œç®€åŒ–ç®—æ³•è®¾è®¡ã€‚
3. ç”¨åœ¨ï¼š**è’™ç‰¹å¡æ´›æ–¹æ³•ã€MCMCã€ç”Ÿæˆæ¨¡å‹ã€ä»¿çœŸç³»ç»Ÿâ€¦â€¦**



```python
import numpy as np
import matplotlib.pyplot as plt

# Target distribution: Exponential(lambda=1)
from scipy.stats import expon

# Generate Uniform samples
n = 1000
U = np.random.uniform(0, 1, n)
U_sorted = np.sort(U)

# Compute inverse CDF (quantile function) for exponential
Y = expon.ppf(U_sorted)

# Prepare plot
fig, axs = plt.subplots(1, 2, figsize=(14, 5))

# Plot CDF of Exponential
x = np.linspace(0, 5, 500)
cdf = expon.cdf(x)

axs[0].plot(x, cdf, label='CDF of Exponential(Î»=1)', color='blue')
axs[0].scatter(Y, U_sorted, color='red', alpha=0.7, label='(Y, U)')
axs[0].set_title('Inverse Transform Sampling Visualization')
axs[0].set_xlabel('y')
axs[0].set_ylabel('u = F(y)')
axs[0].legend()
axs[0].grid(True)

# Plot histogram of sampled Y
axs[1].hist(Y, bins=50, density=True, alpha=0.7, color='orange', label='Sampled Y')
axs[1].plot(x, expon.pdf(x), color='blue', lw=2, label='True PDF')
axs[1].set_title('Sampled Distribution via Inverse Transform')
axs[1].set_xlabel('y')
axs[1].set_ylabel('Density')
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_120_0.png)
    


### ç¦»æ•£å‹åå‡½æ•°é‡‡æ ·

**åå‡½æ•°æ³•ï¼ˆInverse Transform Samplingï¼‰** å¹¶ä¸åªé€‚ç”¨äºè¿ç»­å‹éšæœºå˜é‡ï¼Œå®ƒåŒæ ·é€‚ç”¨äº**ç¦»æ•£å‹éšæœºå˜é‡**ï¼ŒåŸç†æ˜¯ç±»ä¼¼çš„ï¼Œåªæ˜¯æ“ä½œç•¥æœ‰ä¸åŒã€‚ä¸‹é¢æˆ‘å°†è¯¦ç»†ä¸ºä½ è®²è§£ç¦»æ•£å‹éšæœºå˜é‡ä¸Šçš„åº”ç”¨ã€‚

è€Œå¯¹äº**ç¦»æ•£åˆ†å¸ƒ**ï¼Œ$F_X(x)$ æ˜¯é˜¶æ¢¯å‡½æ•°ï¼ˆè·³è·ƒï¼‰ï¼Œæˆ‘ä»¬ä¸èƒ½ç”¨â€œè§£æåå‡½æ•°â€ï¼Œä½†å¯ä»¥é€šè¿‡**æŸ¥æ‰¾æ³•å®ç°â€œåå‡½æ•°â€çš„æ•ˆæœ**ã€‚


#### ğŸ§® ç¦»æ•£å‹åå‡½æ•°é‡‡æ ·ï¼šæ ¸å¿ƒæ­¥éª¤

æˆ‘ä»¬é€šè¿‡**ç´¯ç§¯æ¦‚ç‡è¡¨**æ¥ä»£æ›¿åå‡½æ•°ï¼Œä¸»è¦æµç¨‹å¦‚ä¸‹ï¼š

1. æœ‰ä¸€ä¸ªç¦»æ•£å‹ RV $X$ï¼Œå…¶å¯èƒ½å–å€¼ä¸º $x_1, x_2, ..., x_n$ï¼Œæ¦‚ç‡ä¸º $p_1, p_2, ..., p_n$
2. æ„é€ å…¶ **ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰**ï¼š

   $$
   F(x_k) = \sum_{i=1}^k p_i
   $$
3. ä» $u \sim \text{Uniform}(0, 1)$ ä¸­é‡‡æ ·
4. æ‰¾åˆ°ç¬¬ä¸€ä¸ªä½¿å¾— $F(x_k) \geq u$ çš„ $x_k$ï¼Œè¿™å°±æ˜¯é‡‡æ ·å€¼

#### âœ… ç¤ºä¾‹ï¼šé‡‡æ ·ç¦»æ•£å˜é‡ Xï¼Œå…¶ä¸­ï¼š

* $P(X=1) = 0.1$
* $P(X=2) = 0.3$
* $P(X=3) = 0.4$
* $P(X=4) = 0.2$

é‚£ä¹ˆ CDF ä¸ºï¼š

| å€¼ $x$ | æ¦‚ç‡ $p$ | ç´¯ç§¯ $F(x)$ |
| ----- | ------ | --------- |
| 1     | 0.1    | 0.1       |
| 2     | 0.3    | 0.4       |
| 3     | 0.4    | 0.8       |
| 4     | 0.2    | 1.0       |

å¦‚æœä½ é‡‡åˆ° $u = 0.75$ï¼Œä½ ä¼šè½åœ¨ $F(x=3)=0.8$ï¼Œæ‰€ä»¥è¾“å‡º 3ï¼›
å¦‚æœ $u = 0.85$ï¼Œä½ ä¼šè¾“å‡º 4ã€‚



#### âœ… æ€»ç»“

| å¯¹è±¡ç±»å‹   | æ˜¯å¦èƒ½ç”¨åå‡½æ•°æ³•ï¼Ÿ | å¦‚ä½•å®ç°ï¼Ÿ                      |
| ------ | --------- | -------------------------- |
| è¿ç»­å‹ RV | âœ… æ˜¯       | ç”¨è§£ææˆ–æ•°å€¼æ–¹å¼è®¡ç®— $F^{-1}(u)$     |
| ç¦»æ•£å‹ RV | âœ… æ˜¯       | ç”¨æŸ¥æ‰¾ + ç´¯ç§¯åˆ†å¸ƒå‡½æ•°æ¨¡æ‹Ÿ $F^{-1}(u)$ |


## é‡‡æ ·ç¦»æ•£åˆ†å¸ƒ

### é‡‡æ ·ä¸€ä¸ªäºŒç»´æœ‰é™ç¦»æ•£å‹éšæœºå˜é‡ï¼ˆ2D Finite Discrete Random Variableï¼‰

é‡‡æ ·ä¸€ä¸ª**äºŒç»´æœ‰é™ç¦»æ•£å‹éšæœºå˜é‡ï¼ˆ2D Finite Discrete Random Variableï¼‰**ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯ä»ä¸€ä¸ªç»™å®šçš„**è”åˆæ¦‚ç‡åˆ†å¸ƒè¡¨ $P(X = x_i, Y = y_j)$** ä¸­ç”Ÿæˆéšæœºæ ·æœ¬å¯¹ $(x_i, y_j)$ã€‚

**âœ… æ€»ç»“**

| æ–¹æ³•             | åŸç†                | é€‚ç”¨åœºæ™¯     |            |
| -------------- | ----------------- | -------- | ---------- |
| å±•å¹³è”åˆåˆ†å¸ƒ + åå‡½æ•°é‡‡æ · | ä¸€ç»´åŒ–å¤„ç†æ‰€æœ‰ $(x,y)$ å¯¹ | è”åˆæ¦‚ç‡çŸ©é˜µå·²çŸ¥ |            |
| è¾¹ç¼˜ + æ¡ä»¶é‡‡æ ·      | å…ˆé‡‡ $X$ï¼Œå†é‡‡ (Y      | X)       | æ›´å¤æ‚/ç»“æ„åŒ–çš„æ¨¡å‹ |




#### âœ… æ–¹æ³•ä¸€ï¼šå±•å¹³+åå‡½æ•°é‡‡æ ·

**æ ¸å¿ƒæ€è·¯ï¼š**

å°†äºŒç»´è”åˆåˆ†å¸ƒ**å±•å¹³**ä¸ºä¸€ç»´ï¼Œç„¶ååš**ç´¯ç§¯æ¦‚ç‡è¡¨ + åå‡½æ•°é‡‡æ ·ï¼ˆInverse CDF Samplingï¼‰**ã€‚

**ğŸ”¢ æ­¥éª¤ï¼š**
1. åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„å€¼å¯¹ $(x_i, y_j)$ï¼›
2. å±•å¹³è”åˆæ¦‚ç‡è¡¨ $P(x_i, y_j)$ï¼›
3. æ„é€ ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰ï¼›
4. ä» $[0,1]$ ä¸Šé‡‡æ · $u$ï¼Œæ‰¾ç¬¬ä¸€ä¸ª $u \leq \text{CDF}[k]$ï¼›
5. è¿”å›å¯¹åº”çš„ $(x_i, y_j)$ã€‚

**âœ… å‡è®¾ä½ å·²æœ‰è”åˆåˆ†å¸ƒè¡¨**

æ¯”å¦‚ï¼š

|     | y=0  | y=1  | y=2 |
| --- | ---- | ---- | --- |
| x=0 | 0.1  | 0.2  | 0.1 |
| x=1 | 0.1  | 0.2  | 0.1 |
| x=2 | 0.05 | 0.05 | 0.1 |

è¿™ä¸ªäºŒç»´åˆ†å¸ƒçš„æ€»æ¦‚ç‡æ˜¯ 1ã€‚


```python
import numpy as np
import random

def flatten_joint_distribution(x_vals, y_vals, joint_probs):
    """
    å±•å¹³äºŒç»´è”åˆåˆ†å¸ƒçŸ©é˜µ
    :param joint_probs: äºŒç»´è”åˆåˆ†å¸ƒçŸ©é˜µ
    :return: å±•å¹³åçš„æ¦‚ç‡åˆ†å¸ƒ
    """
    # å±•å¹³è”åˆåˆ†å¸ƒ
    flattened = joint_probs.flatten()
    #print("Flatten joint probabilities:", flattened)

    # æ‰€æœ‰å¯èƒ½çš„(x, y)ç»„åˆï¼ˆé¡ºåºä¸flattenedä¸€è‡´ï¼‰
    xy_pairs = [(x, y) for x in x_vals for y in y_vals]
    #print("XY pairs:", xy_pairs)

    return flattened, xy_pairs


def sample_2d_discrete_inverse_transform(x_vals, y_vals, joint_probs):
    # flatten the joint distribution
    flattened, xy_pairs = flatten_joint_distribution(x_vals, y_vals, joint_probs)
    # apply inverse transform sampling
    cdf = np.cumsum(flattened)
    u = random.random()
    for i, threshold in enumerate(cdf):
        if u <= threshold:
            return xy_pairs[i]
    return xy_pairs[-1]  # fallback for u ~ 1.0
```


```python
import numpy as np
import matplotlib.pyplot as plt

# å®šä¹‰äºŒç»´ç¦»æ•£è”åˆåˆ†å¸ƒ
x_vals = [0, 1, 2]
y_vals = [0, 1, 2]

joint_probs = np.array([
    [0.1, 0.2, 0.1],
    [0.1, 0.2, 0.1],
    [0.05, 0.05, 0.1]
])  # shape (3, 3)

assert np.isclose(joint_probs.sum(), 1.0), "Joint probabilities must sum to 1"

# é‡‡æ ·
N = 1000  # é‡‡æ ·æ¬¡æ•°
samples = [sample_2d_discrete_inverse_transform(x_vals, y_vals, joint_probs) for _ in range(N)]
x_sample, y_sample = zip(*samples)

# å¯è§†åŒ–è”åˆé¢‘ç‡
import seaborn as sns
import pandas as pd

df = pd.DataFrame({'x': x_sample, 'y': y_sample})
pivot_table = pd.crosstab(df['x'], df['y'], normalize='all')

plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
sns.heatmap(joint_probs, annot=True, cmap="YlGnBu", fmt=".3f", xticklabels=y_vals, yticklabels=x_vals)
plt.title("Theoretical Joint Distribution from Samples")
plt.xlabel("Y")
plt.ylabel("X")

plt.subplot(1, 2, 2)
sns.heatmap(pivot_table, annot=True, cmap="YlGnBu", fmt=".3f", xticklabels=y_vals, yticklabels=x_vals)
plt.title("Empirical Joint Distribution from Samples")
plt.xlabel("Y")
plt.ylabel("X")
plt.show()
```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_126_0.png)
    


#### âœ… æ–¹æ³•äºŒï¼šâ€œè¾¹ç¼˜ + æ¡ä»¶åˆ†å¸ƒé‡‡æ ·æ³•â€ï¼ˆMarginal + Conditional Samplingï¼‰

è¿™ç§æ–¹æ³•åœ¨æ¦‚ç‡è®ºã€è´å¶æ–¯å»ºæ¨¡ä¸­éå¸¸å¸¸è§ï¼Œé€»è¾‘ä¹Ÿæ›´ç›´è§‚ï¼Œå°¤å…¶é€‚ç”¨äºç»“æ„åŒ–æ¦‚ç‡æ¨¡å‹ï¼ˆå¦‚è´å¶æ–¯ç½‘ï¼‰æˆ–æ¡ä»¶æ¦‚ç‡ä¿¡æ¯æ¸…æ™°çš„åœºæ™¯ã€‚

é‡‡æ ·ä¸€ä¸ªäºŒç»´ç¦»æ•£éšæœºå˜é‡ $(X, Y)$ çš„åŸºæœ¬æ€è·¯æ˜¯ï¼š

> å…ˆä»è¾¹ç¼˜åˆ†å¸ƒ $P(X = x_i)$ ä¸­é‡‡æ ·ä¸€ä¸ª $x_i$ï¼Œ
> ç„¶åæ ¹æ®è¿™ä¸ª $x_i$ï¼Œä»æ¡ä»¶åˆ†å¸ƒ $P(Y = y_j \mid X = x_i)$ ä¸­é‡‡æ ·ä¸€ä¸ª $y_j$ã€‚



**ğŸ§® æ•°å­¦åŸºç¡€**

è”åˆæ¦‚ç‡å¯åˆ†è§£ä¸ºï¼š

$$
P(X = x_i, Y = y_j) = P(X = x_i) \cdot P(Y = y_j \mid X = x_i)
$$

æ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŒ‰è¿™ä¸ªé¡ºåºé‡‡æ ·ï¼


**âœ… ä¸¾ä¸ªå…·ä½“ä¾‹å­**

å¦‚æœæˆ‘ä»¬æœ‰è”åˆåˆ†å¸ƒè¡¨ $P(X,Y)$ï¼š

|     | y=0  | y=1  | y=2 | åˆè®¡  |
| --- | ---- | ---- | --- | --- |
| x=0 | 0.1  | 0.2  | 0.1 | 0.4 |
| x=1 | 0.1  | 0.2  | 0.1 | 0.4 |
| x=2 | 0.05 | 0.05 | 0.1 | 0.2 |

é‚£ä¹ˆ**è¾¹ç¼˜åˆ†å¸ƒ**ï¼ˆå¯¹Yæ±‚å’Œï¼‰ï¼š

$$
P(X = 0) = 0.4,\quad P(X = 1) = 0.4,\quad P(X = 2) = 0.2
$$

ä»¥åŠ**æ¡ä»¶åˆ†å¸ƒ** $P(Y = y_j \mid X = x_i)$ï¼ˆæ¯ä¸€è¡Œå½’ä¸€åŒ–ï¼‰ï¼š

|     | y=0  | y=1  | y=2  |
| --- | ---- | ---- | ---- |
| x=0 | 0.25 | 0.5  | 0.25 |
| x=1 | 0.25 | 0.5  | 0.25 |
| x=2 | 0.25 | 0.25 | 0.5  |


```python
import numpy as np
import random

def sample_2d_marginal_conditional(x_vals, y_vals, joint_probs):
    # è®¡ç®—è¾¹ç¼˜åˆ†å¸ƒ P(X)
    P_X = joint_probs.sum(axis=1)  # shape (3,)

    # è®¡ç®—æ¡ä»¶åˆ†å¸ƒ P(Y | X)
    P_Y_given_X = joint_probs / P_X[:, np.newaxis]  # æ¯è¡Œé™¤ä»¥å¯¹åº” P(X)
    # 1. é‡‡æ · X
    x = random.choices(x_vals, weights=P_X)[0]
    x_idx = x_vals.index(x)

    # 2. æ ¹æ® P(Y|X=x) é‡‡æ · Y
    y = random.choices(y_vals, weights=P_Y_given_X[x_idx])[0]
    return (x, y)
```


```python
import numpy as np
import matplotlib.pyplot as plt
import random

# å€¼åŸŸ
x_vals = [0, 1, 2]
y_vals = [0, 1, 2]

# è”åˆæ¦‚ç‡çŸ©é˜µ
joint_probs = np.array([
    [0.1, 0.2, 0.1],
    [0.1, 0.2, 0.1],
    [0.05, 0.05, 0.1]
])

N = 1000  # é‡‡æ ·æ¬¡æ•°
# é‡‡æ ·
samples = [sample_2d_marginal_conditional(x_vals, y_vals, joint_probs) for _ in range(N)]
x_sample, y_sample = zip(*samples)

# å¯è§†åŒ–
import seaborn as sns
import pandas as pd

df = pd.DataFrame({'x': x_sample, 'y': y_sample})
pivot_table = pd.crosstab(df['x'], df['y'], normalize='all')

plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
sns.heatmap(joint_probs, annot=True, cmap="YlGnBu", fmt=".3f", xticklabels=y_vals, yticklabels=x_vals)
plt.title("Theoretical Joint Distribution from Marginal + Conditional Sampling")
plt.xlabel("Y")
plt.ylabel("X")

plt.subplot(1, 2, 2)
sns.heatmap(pivot_table, annot=True, cmap="YlGnBu", fmt=".3f", xticklabels=y_vals, yticklabels=x_vals)
plt.title("Empirical Joint Distribution from Marginal + Conditional Sampling")
plt.xlabel("Y")
plt.ylabel("X")
plt.show()
```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_129_0.png)
    


## é‡‡æ ·è¿ç»­åˆ†å¸ƒ

### æ ¼ç‚¹é‡‡æ ·å™¨ï¼ˆGrid samplerï¼‰
**æ ¼ç‚¹é‡‡æ ·å™¨ï¼ˆGrid Samplerï¼‰** æ˜¯ä¸€ç§åŸºäºäºŒè¿›åˆ¶ç»†åˆ†åŒºé—´çš„**é€’å½’é‡‡æ ·æ–¹æ³•**ï¼Œé€‚ç”¨äº**è¿‘ä¼¼ä»ä»»æ„è¿ç»­åˆ†å¸ƒä¸­é‡‡æ ·**ï¼Œå‰ææ˜¯æˆ‘ä»¬å¯ä»¥**è®¡ç®—ä»»æ„åŒºé—´çš„æ¦‚ç‡è´¨é‡ï¼ˆç§¯åˆ†ï¼‰**ã€‚

è¿™ä¸ªæ–¹æ³•æ˜¯**ç”¨äºé«˜ç²¾åº¦æ•°å€¼é‡‡æ ·çš„æ„é€ æ€§æ–¹æ³•**ï¼Œç‰¹åˆ«é€‚åˆï¼š

* åˆ†å¸ƒæ²¡æœ‰å°é—­å½¢å¼çš„é€†CDFï¼›
* ä½†æˆ‘ä»¬å¯ä»¥è®¡ç®—å…¶ CDF æˆ–åŒºé—´æ¦‚ç‡ï¼›
* æƒ³ä»è¯¥åˆ†å¸ƒä¸­ç²¾ç¡®é‡‡æ ·ã€‚


#### ğŸ” åŸç†æ¦‚è¿°ï¼ˆä»¥ \[0,1] åŒºé—´ä¸ºä¾‹ï¼‰

ç›®æ ‡ï¼šä»æŸä¸ªè¿ç»­æ¦‚ç‡å¯†åº¦å‡½æ•° $f(x)$ å®šä¹‰åœ¨ \[0,1] ä¸Šé‡‡æ ·ã€‚

> ğŸ“– å³ä½¿éšæœºå˜é‡æ˜¯å®šä¹‰åœ¨åŒºé—´$[a,b]$ä¸Šçš„ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡**çº¿æ€§å˜æ¢**å°†å…¶è½¬æ¢åˆ°$[0,1]$ã€‚åˆ«å¿˜äº†æœ€åé‡‡æ ·åï¼Œéœ€è¦å†åšä¸€æ¬¡çº¿æ€§é€†å˜æ¢ã€‚

**ğŸ§± Step 1: äºŒè¿›åˆ¶åˆ’åˆ†åŒºé—´**

æˆ‘ä»¬å°†åŒºé—´ \[0, 1] åˆ†æˆ $2^N$ ä¸ªå­åŒºé—´ï¼Œæ¯”å¦‚ $N = 3$ æ—¶ï¼š

* 000 â†’ $[0, \frac{1}{8})$
* 001 â†’ $[\frac{1}{8}, \frac{2}{8})$
* ...
* 111 â†’ $[\frac{7}{8}, 1)$

è¿™äº›å¯ä»¥å¯¹åº”ä¸º **äºŒè¿›åˆ¶ç¼–ç çš„åŒºé—´ç¼–å·**ã€‚


**ğŸ§  Step 2: åˆ©ç”¨æ¡ä»¶æ¦‚ç‡æ„å»ºä¸€ä¸ªæ ·æœ¬ç‚¹**

æˆ‘ä»¬ä¸ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ $2^N$ ä¸ªåŒºé—´çš„æ¦‚ç‡ï¼Œè€Œæ˜¯é€šè¿‡ä¸€ä¸ª**é€ä½å†³ç­–è¿‡ç¨‹**æ¥â€œèµ°â€è¿›ä¸€ä¸ªåŒºé—´ã€‚

å‡è®¾æˆ‘ä»¬å·²ç»ç”Ÿæˆäº†å‰ $k$ ä½ï¼š $b_1 b_2 \dots b_k$

é‚£ä¹ˆè¿™ä¸€éƒ¨åˆ†æ‰€å¯¹åº”çš„åŒºé—´æ˜¯ï¼š

$$
I_k = \left[\frac{b}{2^k}, \frac{b+1}{2^k}\right), \quad b = \sum_{i=1}^k b_i \cdot 2^{k-i}
$$

ä¸‹ä¸€ä½çš„å€¼ $b_{k+1} \in \{0, 1\}$ å°†è¿›ä¸€æ­¥æŠŠ $I_k$ åˆ†æˆå·¦åŠæˆ–å³åŠã€‚

æˆ‘ä»¬é€šè¿‡**è®¡ç®—æ¡ä»¶æ¦‚ç‡**ï¼š

$$
\mathbb{P}(b_{k+1} = 0 \mid b_1 b_2 \dots b_k) = \frac{\mathbb{P}(x \in \text{left half of } I_k)}{\mathbb{P}(x \in I_k)}
$$

ç”¨è¿™ä¸ªæ¡ä»¶æ¦‚ç‡æ·ä¸€ä¸ªç¡¬å¸ï¼Œå†³å®šä¸‹ä¸€ä½æ˜¯ 0 è¿˜æ˜¯ 1ã€‚é‡å¤ $N$ æ¬¡ï¼Œå°±å¾—åˆ°äº†ä¸€ä¸ªè¿‘ä¼¼é‡‡æ ·ç»“æœï¼š

$$
x = \sum_{i=1}^N b_i \cdot 2^{-i} \text{, where } b_i = \{0,1\}
$$



#### ğŸ“Œ æ€»ç»“æµç¨‹

1. **åˆå§‹åŒºé—´**ï¼š\[0,1]
2. **æ¯ä¸€æ­¥**ï¼š

   * å°†å½“å‰åŒºé—´åˆ†ä¸ºå·¦å³ä¸¤åŠ
   * è®¡ç®—å·¦åŠåŒºé—´çš„æ¦‚ç‡ $p_L$
   * ç”¨ $p_L$ è¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·ï¼Œå†³å®šæ˜¯èµ°å·¦è¾¹ï¼ˆbit=0ï¼‰è¿˜æ˜¯å³è¾¹ï¼ˆbit=1ï¼‰
3. **è¿­ä»£ N æ¬¡å**ï¼Œä½ å°±å¾—åˆ°äº†ä¸€ä¸ªäºŒè¿›åˆ¶è¡¨è¾¾ $b_1b_2...b_N$
4. **æœ€ç»ˆé‡‡æ ·å€¼**ï¼š$x = \sum_{i=1}^N b_i \cdot 2^{-i}$ï¼Œå±äº \[0,1] ä¸Šçš„æŸä¸ªå­åŒºé—´ä¸­

> æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¸€æ¬¡æ„é€ æ‰€æœ‰åŒºé—´çš„æ¦‚ç‡ï¼Œå°†å…¶è½¬æ¢ä¸ºç¦»æ•£å‹ï¼Œå†ä½¿ç”¨ç¦»æ•£å‹éšæœºå˜é‡çš„é‡‡æ ·æ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œåå‡½æ•°æ³•ï¼‰è¿›è¡Œé‡‡æ ·ã€‚

#### âœ… ä¼˜ç‚¹

* **é€‚ç”¨äºä»»æ„è¿ç»­åˆ†å¸ƒ**ï¼Œåªè¦èƒ½è®¡ç®—åŒºé—´æ¦‚ç‡
* **æ— éœ€æ±‚é€†CDFæˆ–æ ‡å‡†å˜æ¢**
* å¯ç”¨äºä¸¥æ ¼é‡‡æ ·ã€æµ‹è¯•ã€æ¨¡æ‹Ÿå¤æ‚åˆ†å¸ƒ


#### âš ï¸ æ³¨æ„äº‹é¡¹

* ç²¾åº¦ç”± $N$ æ§åˆ¶ï¼šè¶Šå¤§è¶Šç²¾ç¡®ï¼Œä½†è®¡ç®—ä»£ä»·è¶Šé«˜ï¼›
  * $\epsilon=log_2N$
* å¿…é¡»èƒ½è®¡ç®—ä»»æ„åŒºé—´ $[a,b]$ çš„æ¦‚ç‡ï¼šä¾‹å¦‚é€šè¿‡ CDF å‡½æ•°ï¼›
* å’Œæ™®é€šâ€œå…ˆé‡‡ uniform å†æ±‚é€†CDFâ€çš„æ–¹å¼ä¸åŒï¼Œè¿™æ˜¯ä¸€ç§æ„é€ æ€§çš„ã€ä½çº§åˆ«çš„é‡‡æ ·æ–¹å¼ã€‚



```python
import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®å‚æ•°
N = 6 # æ ‘çš„æ·±åº¦ï¼ˆä¹Ÿå³å°† [0,1] åˆ’åˆ†ä¸º 2^6 = 64 ä¸ªåŒºé—´ï¼‰

def draw_path(ax, level, x0, x1, path=[]):
    if level > N:
        x_mid = (x0 + x1) / 2
        y = -level
        ax.plot(x_mid, y, 'ko')  # ç”»å¶å­èŠ‚ç‚¹
        bin_label = ''.join(str(b) for b in path)
        ax.text(x_mid, y - 0.2, f"{bin_label}", ha='center', fontsize=8)
        return
    
    x_mid = (x0 + x1) / 2
    y = -level
    ax.plot(x_mid, y, 'ko')  # å½“å‰èŠ‚ç‚¹
    
    # å·¦è¾¹
    draw_path(ax, level + 1, x0, x_mid, path + [0])
    ax.plot([x_mid, (x0 + x_mid) / 2], [y, y - 1], 'gray', lw=1)

    # å³è¾¹
    draw_path(ax, level + 1, x_mid, x1, path + [1])
    ax.plot([x_mid, (x1 + x_mid) / 2], [y, y - 1], 'gray', lw=1)

# ç”»å›¾
fig, ax = plt.subplots(figsize=(12, 6))
draw_path(ax, 0, 0.0, 1.0)
ax.set_ylim(-N - 1.5, 1)
ax.set_xlim(-0.05, 1.05)
ax.set_yticks([])
ax.set_xlabel("Interval on [0,1]")
ax.set_title("Grid Sampling: Binary Tree Partition (N=6)")
plt.grid(True, axis='x', linestyle='--', alpha=0.3)
plt.tight_layout()
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_132_0.png)
    


#### ğŸ§ª ä¸¾ä¾‹ï¼š$\text{Beta}(2,5)$ï¼‰


**ç›®æ ‡**ï¼šæˆ‘ä»¬å·²çŸ¥ä¸€ä¸ªè¿ç»­åˆ†å¸ƒï¼ˆå¦‚ Beta åˆ†å¸ƒï¼‰çš„ PDFï¼Œå¯ä»¥è®¡ç®—ä»»æ„åŒºé—´çš„æ¦‚ç‡ï¼Œä½†ä¸èƒ½ç›´æ¥åè§£å…¶ CDFã€‚

##### Step 1: å°†åŒºé—´ \[0, 1] ç¦»æ•£åŒ–ä¸º 2â¿ ä¸ªå­åŒºé—´

æ¯”å¦‚ `N = 10`ï¼Œå°† `[0,1]` å‡åˆ†ä¸º 1024 ä¸ªå°åŒºé—´ï¼Œæ¯ä¸ªåŒºé—´å®½åº¦ä¸º $\Delta x = \frac{1}{1024}$ã€‚

##### Step 2: åœ¨æ¯ä¸ªå­åŒºé—´çš„ä¸­ç‚¹å¤„è®¡ç®— PDFï¼Œå¹¶ç”¨è¿™äº›è¿‘ä¼¼æ¦‚ç‡æ¥æ„é€ ç¦»æ•£ PMF

æˆ‘ä»¬ç”¨ beta.pdf è®¡ç®—æ¯ä¸ªä¸­ç‚¹çš„æ¦‚ç‡å¯†åº¦ï¼Œå½’ä¸€åŒ–å¾—åˆ°ä¸€ç»„è¿‘ä¼¼çš„æ¦‚ç‡å€¼ $p_i$ï¼Œå½¢æˆä¸€ä¸ªç¦»æ•£åˆ†å¸ƒã€‚

##### Step 3: æ„å»º CDFï¼Œç„¶åä½¿ç”¨ä¸€ä¸ª Uniform(0,1) éšæœºå˜é‡ `u` æ¥æŸ¥æ‰¾ `u` è½å…¥å“ªä¸ªç´¯ç§¯æ¦‚ç‡åŒºé—´

æˆ‘ä»¬ç”¨ `np.searchsorted(cdf, u)` æ‰¾åˆ°å¯¹åº”åŒºé—´ï¼Œå†è¿”å›è¯¥åŒºé—´ä¸­ç‚¹ä½œä¸ºé‡‡æ ·å€¼ã€‚



```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

# è®¾ç½® beta åˆ†å¸ƒå‚æ•°
a, b = 2, 5  # Beta(2,5)

# æ ·æœ¬æ•°é‡å’Œåˆ†å‰²æ·±åº¦
N = 10  # å°†åŒºé—´ [0,1] åˆ†ä¸º 2^N = 1024 ä¸ªå°åŒºé—´
num_samples = 10000

# æ„å»ºåŒºé—´åˆ’åˆ†
intervals = np.linspace(0, 1, 2**N + 1)
midpoints = (intervals[:-1] + intervals[1:]) / 2 # åŒºé—´ä¸­ç‚¹
probs = beta.pdf(midpoints, a, b) # åŒºé—´ä¸­ç‚¹çš„æ¦‚ç‡å¯†åº¦
probs /= probs.sum()  # å½’ä¸€åŒ–æˆæ¦‚ç‡ï¼Œä»è€Œè·å¾—ä¸€ä¸ªæ–°çš„ç¦»æ•£åˆ†å¸ƒ

# æ„å»ºç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰
cdf = np.cumsum(probs) # åŸºäºè¿™ä¸ªæ–°çš„ç¦»æ•£åˆ†å¸ƒæ„å»º CDF

# Grid sampling: åœ¨æ¯æ¬¡é‡‡æ ·æ—¶åªç”¨ä¸€ä¸ªå‡åŒ€éšæœºæ•°æŸ¥æ‰¾æ‰€å±åŒºé—´
uniform_samples = np.random.rand(num_samples)
samples = []

for u in uniform_samples:
    idx = np.searchsorted(cdf, u)
    # é€‰æ‹©è¯¥åŒºé—´çš„ä¸­ç‚¹ä½œä¸ºé‡‡æ ·å€¼
    samples.append(midpoints[idx])

# å¯è§†åŒ–ç»“æœ
x = np.linspace(0, 1, 500)
pdf = beta.pdf(x, a, b)

plt.figure(figsize=(10, 6))
plt.hist(samples, bins=50, density=True, alpha=0.6, label="Sampled Histogram")
plt.plot(x, pdf, 'r-', lw=2, label=f"Beta PDF a={a}, b={b}")
plt.title(f"Sampling Beta(2,5) using Grid Sampler (Îµ={1/(2**N):.4f})")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_134_0.png)
    


#### å¦ä¸€ç§å®ç°ï¼šä»¥ Beta(2,5) åˆ†å¸ƒä¸ºä¾‹

æ­¥éª¤ï¼šæ¯æ¬¡å°†åŒºé—´ä¸€åˆ†ä¸ºäºŒï¼Œè®¡ç®—å·¦è¾¹çš„æ¦‚ç‡ï¼Œç”¨å®ƒè¿›è¡Œä¼¯åŠªåˆ©é‡‡æ ·ï¼Œæœ€ç»ˆæ‹¼å‡ºä¸€ä¸ªäºŒè¿›åˆ¶å°æ•°ã€‚


```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta

def grid_sampler(cdf_func, N):
    """
    ä½¿ç”¨ Grid Sampler ä» [0,1] ä¸Šé‡‡æ ·
    :param cdf_func: ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆä¾‹å¦‚ scipy.stats.beta(a,b).cdfï¼‰
    :param N: è¿­ä»£æ¬¡æ•°ï¼ˆæ§åˆ¶ç²¾åº¦ï¼‰
    :return: é‡‡æ ·å€¼ x âˆˆ [0,1]
    """
    left, right = 0.0, 1.0
    x_bits = []

    for i in range(N):
        mid = (left + right) / 2
        p_left = (cdf_func(mid) - cdf_func(left)) / (cdf_func(right) - cdf_func(left))  # å½’ä¸€åŒ–æ¡ä»¶æ¦‚ç‡
        b = np.random.rand() < p_left  # ä¼¯åŠªåˆ©é‡‡æ ·
        x_bits.append(0 if b else 1)

        # æ›´æ–°åŒºé—´
        if b:
            right = mid
        else:
            left = mid

    # å°†äºŒè¿›åˆ¶ç»“æœè½¬æ¢ä¸º [0,1] ä¸Šçš„å°æ•°å€¼
    x = sum(bit * (0.5 ** (i + 1)) for i, bit in enumerate(x_bits))
    return x

# è®¾ç½®å‚æ•°
N = 10  # ç²¾åº¦æ§åˆ¶
samples = [grid_sampler(beta(2, 5).cdf, N) for _ in range(10000)]

# å¯è§†åŒ–
x = np.linspace(0, 1, 1000)
plt.figure(figsize=(10, 6))
plt.hist(samples, bins=50, density=True, alpha=0.6, label='Sampled via Grid Sampler')
plt.plot(x, beta.pdf(x, 2, 5), label='Beta(2,5) PDF', lw=2, color='red')
plt.title("Grid Sampling from Beta(2,5)")
plt.xlabel("x")
plt.ylabel("Density")
plt.legend()
plt.grid(True)
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_136_0.png)
    


### æ¥å—-æ‹’ç»é‡‡æ ·ï¼ˆAcceptance-Rejection Samplingï¼‰
Acceptance-Rejection Samplingï¼ˆæ¥å—-æ‹’ç»é‡‡æ ·ï¼Œç®€ç§° AR é‡‡æ ·ï¼‰æ˜¯ä¸€ç§éå¸¸é‡è¦ä¸”ç›´è§‚çš„ **ä»å¤æ‚åˆ†å¸ƒä¸­é‡‡æ ·çš„æ–¹æ³•**ï¼Œç‰¹åˆ«é€‚åˆå½“ï¼š

* âœ… ç›®æ ‡åˆ†å¸ƒçš„ **æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰å¯è®¡ç®—**ï¼›
* ğŸš« ç›®æ ‡åˆ†å¸ƒçš„ **CDF æ— æ³•åè§£**ï¼Œä¸èƒ½ç›´æ¥ä½¿ç”¨åå‡½æ•°é‡‡æ ·ï¼›
* âœ… ä½ æœ‰ä¸€ä¸ª**å®¹æ˜“é‡‡æ ·çš„è¾…åŠ©åˆ†å¸ƒï¼ˆproposal distributionï¼‰**ã€‚

**ğŸ§  åŸºæœ¬æ€æƒ³**

å¦‚æœä½ ä¸èƒ½ç›´æ¥ä»å¤æ‚åˆ†å¸ƒ $p(x)$ é‡‡æ ·ï¼Œé‚£å°±ï¼š

1. ä»ä¸€ä¸ªå®¹æ˜“é‡‡æ ·çš„åˆ†å¸ƒ $q(x)$ é‡‡æ ·ï¼›
2. ç„¶åâ€œç­›é€‰â€å‡ºè½åœ¨ç›®æ ‡åˆ†å¸ƒ $p(x)$ ä¸‹çš„ç‚¹ï¼

#### ğŸ“Œ ç†è®ºåŸºç¡€

ä»æ˜“é‡‡æ ·çš„åˆ†å¸ƒä¸­**ç­›é€‰å‡º**æœä» $p(x)$ çš„æ ·æœ¬

æˆ‘ä»¬å¼•å…¥ä¸€ä¸ª proposal distribution $q(x)$ï¼Œå¹¶è¦æ±‚å­˜åœ¨å¸¸æ•° $M \geq 1$ï¼Œä½¿å¾—ï¼š

$$
p(x) \leq M q(x),\quad \text{å¯¹æ‰€æœ‰ } x
$$

è¿™æ ·ï¼Œ$Mq(x)$ å°±æ˜¯ $p(x)$ çš„â€œä¸Šç•Œâ€ã€‚


##### âœ… ç†è®ºåŸºç¡€ä¸€ï¼š**è”åˆåˆ†å¸ƒä¸è¾¹ç¼˜åˆ†å¸ƒ**

æˆ‘ä»¬æ„é€ ä¸€ä¸ªäºŒç»´è”åˆåˆ†å¸ƒï¼š

$$
(x, u) \sim \text{joint distribution}, \quad x \sim q(x),\quad u \sim \text{Uniform}(0, 1)
$$

ç„¶åå®šä¹‰ä¸€ä¸ªäºŒç»´åŒºåŸŸï¼š

$$
\mathcal{A} = \left\{ (x, u) : u \leq \frac{p(x)}{Mq(x)} \right\}
$$

é‚£ä¹ˆæˆ‘ä»¬åœ¨ $(x, u) \in \mathcal{A}$ ä¸­ä¿ç•™çš„ $x$ï¼Œå…¶**è¾¹ç¼˜åˆ†å¸ƒ**å°±æ˜¯ $p(x)$ï¼

è¿™ç‚¹å¯ä»è”åˆå¯†åº¦æ¨å¯¼ï¼š

è®¾è”åˆå¯†åº¦ä¸ºï¼š

$$
f(x, u) = q(x) \cdot \mathbb{1}\left( 0 \leq u \leq \frac{p(x)}{Mq(x)} \right)
$$

é‚£ä¹ˆå¯¹ $x$ çš„è¾¹ç¼˜å¯†åº¦ï¼š

$$
f_{\text{accepted}}(x) = \int_0^{\frac{p(x)}{Mq(x)}} q(x) \, du = q(x)\cdot \frac{p(x)}{Mq(x)} = \frac{p(x)}{M}
$$

å³æ¯ä¸ªä¿ç•™ä¸‹æ¥çš„ $x$ï¼Œåˆ†å¸ƒæ¯”ä¾‹ä¸º $\propto p(x)$ã€‚è¯´æ˜æ¥å—çš„æ ·æœ¬æ­£æ¯”äº $p(x)$ï¼Œå³æˆ‘ä»¬ä» $p(x)$ ä¸­é‡‡æ ·äº†ï¼

##### âœ… ç†è®ºåŸºç¡€äºŒï¼š**æŠ½æ ·ä¸€è‡´æ€§**

è™½ç„¶æˆ‘ä»¬ä½¿ç”¨äº†â€œæ‹’ç»â€çš„æœºåˆ¶ï¼Œä½†è¿™å…¶å®å°±ç­‰ä»·äº**å¯¹ä¸€ä¸ªæ›´å¤§çš„åˆ†å¸ƒè¿›è¡Œæ¡ä»¶é‡‡æ ·**ï¼š

æˆ‘ä»¬ä»è”åˆåˆ†å¸ƒ $f(x, u) = q(x) \cdot \text{Unif}[0, Mq(x)]$ ä¸­é‡‡æ ·ï¼Œç„¶åä¿ç•™ç¬¦åˆæ¡ä»¶çš„ç‚¹ã€‚

**ä¿ç•™ä¸‹æ¥çš„ $x$** çš„åˆ†å¸ƒæ˜¯ï¼š

$$
p_{\text{accept}}(x) = \frac{p(x)}{M}
\Rightarrow \text{Normalizeåä»æ˜¯ } p(x)
$$

#### ğŸ² ç®—æ³•æ­¥éª¤

å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼š

1. ä» proposal åˆ†å¸ƒ $q(x)$ ä¸­é‡‡æ ·ä¸€ä¸ªå€™é€‰ç‚¹ $x \sim q(x)$
2. ä»å‡åŒ€åˆ†å¸ƒ $u \sim \text{Uniform}(0,1)$ é‡‡ä¸€ä¸ªè¾…åŠ©å˜é‡
3. æ¥å— $x$ å½“ä¸”ä»…å½“ï¼š

$$
u \leq \frac{f(x)}{M \cdot q(x)}
$$

å¦åˆ™æ‹’ç»ï¼Œé‡æ–°é‡‡æ ·ã€‚


#### ğŸ“ˆ æ¥å—ç‡å’Œæ•ˆç‡

æ¥å—ç‡ï¼ˆacceptance rateï¼‰ä¸ºï¼š

$$
\text{Accept Rate} = \frac{1}{M}
$$

å› æ­¤ï¼Œ**é€‰æ‹©å°½å¯èƒ½æ¥è¿‘ $f(x)$ çš„ $M \cdot q(x)$** å¾ˆé‡è¦ï¼Œè¿™æ ·å¯ä»¥å‡å°‘æµªè´¹ã€‚



#### âœ… ä¼˜ç‚¹

* ä¸éœ€è¦å½’ä¸€åŒ–å¸¸æ•° $Z$ï¼Œå¯ç”¨äºå¤æ‚åˆ†å¸ƒï¼ˆå¦‚æœªå½’ä¸€åŒ–çš„ posteriorï¼‰
* ç†è®ºç®€å•ï¼Œç›´è§‚


#### âš ï¸ ç¼ºç‚¹

* æ¥å—ç‡å¯èƒ½éå¸¸ä½ï¼ˆå°¤å…¶æ˜¯é«˜ç»´æƒ…å†µä¸‹ï¼‰
* å¿…é¡»æ‰¾åˆ°åˆé€‚çš„ proposal åˆ†å¸ƒå’Œåˆç†çš„ M

**ğŸ§  æ€»ç»“ä¸€å¥è¯**

> **æ¥å—-æ‹’ç»é‡‡æ ·å°±æ˜¯ç”¨ä¸€ä¸ªå®¹æ˜“é‡‡æ ·çš„â€œå¤–å£³â€åŒ…ä½ç›®æ ‡åˆ†å¸ƒï¼Œç„¶åéšæœºæ’’ç‚¹ï¼Œç•™ä¸‹è½åœ¨ç›®æ ‡åŒºåŸŸçš„é‚£äº›ç‚¹ã€‚**


#### åˆè§
- [æ¥å—/æ‹’ç»é‡‡æ · (Accept-Reject Sampling)](https://bocaiwen.github.io/accept-reject-sampling.html)


```python
import numpy as np
import plotly.graph_objects as go
from scipy.stats import beta

# è®¾ç½®ç›®æ ‡åˆ†å¸ƒå’Œ proposal
def f(x):
    return beta.pdf(x, 2, 5)

def g(x):
    return 1.0  # Uniform(0,1) ä¸Šçš„å¯†åº¦

# è®¾å®šå¸¸æ•° M
x_vals = np.linspace(0, 1, 500)
fx_vals = f(x_vals)
gx_vals = g(x_vals)
M = np.max(fx_vals)  # M >= max(f(x)/g(x))ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ g(x)=1ï¼Œæ‰€ä»¥M=max(f(x))

# å¼€å§‹é‡‡æ ·
np.random.seed(42)
N = 500
x_samples = np.random.uniform(0, 1, N)
u_samples = np.random.uniform(0, 1, N)

accepted_x = []
accepted_y = []
rejected_x = []
rejected_y = []

for x, u in zip(x_samples, u_samples):
    threshold = f(x) / M
    if u < threshold:
        accepted_x.append(x)
        accepted_y.append(u * M)  # æ˜ å°„å› f(x) çš„é«˜åº¦ç©ºé—´
    else:
        rejected_x.append(x)
        rejected_y.append(u * M)

# åˆ›å»ºäº¤äº’å›¾
fig = go.Figure()

# ç›®æ ‡åˆ†å¸ƒ f(x)
fig.add_trace(go.Scatter(x=x_vals, y=fx_vals, mode='lines', name='Target f(x)', line=dict(color='blue')))

# Proposal åˆ†å¸ƒ M * g(x)
fig.add_trace(go.Scatter(x=x_vals, y=[M * g(x) for x in x_vals], mode='lines', name='Mg(x)', line=dict(color='orange', dash='dash')))

# Accepted samples
fig.add_trace(go.Scatter(x=accepted_x, y=accepted_y, mode='markers', name='Accepted', marker=dict(color='green', size=6)))

# Rejected samples
fig.add_trace(go.Scatter(x=rejected_x, y=rejected_y, mode='markers', name='Rejected', marker=dict(color='red', size=4, opacity=0.5)))

fig.update_layout(title='Acceptance-Rejection Sampling with Joint Distribution View',
                  xaxis_title='x', yaxis_title='y',
                  height=500, width=800)

fig.show()

```




```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

# Target distribution: p(x) âˆ exp(-x^2 / 2) (standard normal, unnormalized)
def p(x):
    return np.exp(-x**2 / 2)

# Proposal distribution: uniform over [-3, 3]
def q(x):
    return np.full_like(x, 1 / 6)

# M such that p(x) <= M * q(x)
M = np.sqrt(2 * np.pi) / (1/6)  # since max(p) = 1 when x=0

# Generate samples
np.random.seed(0)
n_samples = 1000
x_vals = np.random.uniform(-3, 3, n_samples)
u_vals = np.random.uniform(0, M * q(x_vals), n_samples)

# Create acceptance mask
accept = u_vals <= p(x_vals)

# Prepare animation frames
x_range = np.linspace(-3, 3, 500)
fig, ax = plt.subplots(figsize=(8, 5))
frames = []

for i in range(0, n_samples, 20):
    ax.clear()
    ax.plot(x_range, p(x_range), label='Target p(x)', color='orange')
    ax.plot(x_range, M * q(x_range), label='M * q(x)', color='blue', linestyle='--')
    
    ax.scatter(x_vals[:i], u_vals[:i], color='gray', s=10, alpha=0.3, label='Rejected')
    ax.scatter(x_vals[:i][accept[:i]], u_vals[:i][accept[:i]], color='green', s=10, label='Accepted')

    ax.set_xlim(-3, 3)
    ax.set_ylim(0, M * 1.1 * np.max(q(x_range)))
    ax.set_title("Acceptance-Rejection Sampling")
    ax.legend(loc='upper right')
    ax.grid(True)

    # Capture the current frame
    frame = plt.gcf().canvas.copy_from_bbox(ax.bbox)
    frames.append([plt.scatter([], [])])  # dummy to hold frames

# Re-render using FuncAnimation
def update(i):
    ax.clear()
    ax.plot(x_range, p(x_range), label='Target p(x)', color='orange')
    ax.plot(x_range, M * q(x_range), label=f'M * q(x) (M={M:.2f})', color='blue', linestyle='--')
    
    ax.scatter(x_vals[:i], u_vals[:i], color='gray', s=10, alpha=0.3, label='Rejected')
    ax.scatter(x_vals[:i][accept[:i]], u_vals[:i][accept[:i]], color='green', s=10, label='Accepted')
    
    ax.set_xlim(-3, 3)
    ax.set_ylim(0, M * 1.1 * np.max(q(x_range)))
    ax.set_title(f"Acceptance-Rejection Sampling (Step {i})")
    ax.legend(loc='upper right')
    ax.grid(True)

ani = animation.FuncAnimation(fig, update, frames=range(0, n_samples, 20), interval=100)

# Save as GIF
from matplotlib.animation import PillowWriter
ani.save("acceptance_rejection_sampling.gif", writer=PillowWriter(fps=10))

plt.close(fig)

from IPython.display import Image
Image(filename="acceptance_rejection_sampling.gif")

```

![png](/img/contents/post/mcmc-statics/2_random-variables/acceptance_rejection_sampling.gif)



# é™„å½•

## ä»å·²çŸ¥äºŒç»´éšæœºå˜é‡ (X, Y) ä¸­é‡‡æ ·å…¶æåæ ‡å˜æ¢åçš„éšæœºå˜é‡

ä»å·²çŸ¥äºŒç»´éšæœºå˜é‡ $(X, Y)$ ä¸­é‡‡æ ·å…¶æåæ ‡å˜æ¢åçš„éšæœºå˜é‡ $(R^2, \alpha)$ï¼Œå…¶ä¸­ï¼š
* $r^2 = x^2 + y^2$ï¼ŒèŒƒå›´ä¸º $(0, \infty)$
* $\alpha = \arctan(y/x)$ï¼ŒèŒƒå›´ä¸º $(0, 2\pi)$


### ğŸ§® 1. å˜é‡å˜æ¢å’Œé›…å¯æ¯”ï¼ˆJacobianï¼‰æ¨å¯¼

æˆ‘ä»¬å®šä¹‰æ–°çš„å˜é‡ï¼š

$$
\begin{cases}
u(x,y) = r^2 = x^2 + y^2 \\
v(x,y) = \alpha = \arctan\left(\frac{y}{x}\right)
\end{cases}
\Rightarrow
\begin{cases}
x = \sqrt{u} \cos v \\
y = \sqrt{u} \sin v
\end{cases}
$$

è®¡ç®— [Jacobian](https://zh.wikipedia.org/wiki/%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5)ï¼š

æˆ‘ä»¬è®¡ç®—ä» $(u, v)$ åˆ° $(x, y)$ çš„é›…å¯æ¯”è¡Œåˆ—å¼ï¼š

$$
J = 
\begin{vmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v}
\end{vmatrix}
$$

å…ˆæ±‚å¯¼ï¼š

* $\frac{\partial x}{\partial u} = \frac{1}{2\sqrt{u}} \cos v$
* $\frac{\partial x}{\partial v} = -\sqrt{u} \sin v$
* $\frac{\partial y}{\partial u} = \frac{1}{2\sqrt{u}} \sin v$
* $\frac{\partial y}{\partial v} = \sqrt{u} \cos v$

ä»£å…¥ Jacobianï¼š

$$
J = 
\left| 
\begin{matrix}
\frac{1}{2\sqrt{u}} \cos v & -\sqrt{u} \sin v \\
\frac{1}{2\sqrt{u}} \sin v & \sqrt{u} \cos v
\end{matrix}
\right|
= \frac{1}{2\sqrt{u}} \cos v \cdot \sqrt{u} \cos v + \sqrt{u} \sin v \cdot \frac{1}{2\sqrt{u}} \sin v
= \frac{1}{2}(\cos^2 v + \sin^2 v) = \frac{1}{2}
$$

æ‰€ä»¥é›…å¯æ¯”è¡Œåˆ—å¼ä¸ºï¼š

$$
|J| = \frac{1}{2}
$$


### ğŸ§ª 2. å¯†åº¦å‡½æ•°å˜æ¢å…¬å¼

è‹¥ $(X, Y)$ çš„è”åˆå¯†åº¦ä¸º $f_{X,Y}(x,y)$ï¼Œåˆ™å˜æ¢åï¼š

$$
f_{R^2, \alpha}(u, v) = f_{X,Y}(x(u,v), y(u,v)) \cdot \left|J\right|^{-1} = 2 f_{X,Y}(\sqrt{u} \cos v, \sqrt{u} \sin v)
$$


```python
import numpy as np
import matplotlib.pyplot as plt

# æ„å»º (x, y) ç½‘æ ¼
x = np.linspace(-2, 2, 400)
y = np.linspace(-2, 2, 400)
X, Y = np.meshgrid(x, y)

# åŸå§‹åæ ‡ç³»ä¸‹çš„å‡½æ•°ï¼šr^2 = x^2 + y^2, alpha = arctan2(y, x)
R2 = X**2 + Y**2
Alpha = np.arctan2(Y, X)

# ç»˜åˆ¶ r^2 ç­‰é«˜çº¿
fig, ax = plt.subplots(1, 2, figsize=(12, 5))

r2_levels = [0.5, 1.0, 2.0, 3.0]
alpha_levels = [-np.pi, -np.pi/2, 0, np.pi/2, np.pi]

cont1 = ax[0].contour(X, Y, R2, levels=r2_levels, cmap="Blues")
ax[0].clabel(cont1, fmt="rÂ²=%.1f")
ax[0].set_title("Contours of $r^2 = x^2 + y^2$")
ax[0].set_xlabel("x")
ax[0].set_ylabel("y")
ax[0].axis("equal")
ax[0].grid(True)

# ç»˜åˆ¶ alpha ç­‰é«˜çº¿
cont2 = ax[1].contour(X, Y, Alpha, levels=alpha_levels, cmap="coolwarm")
ax[1].clabel(cont2, fmt="Î±=%.2f")
ax[1].set_title(r"Contours of $\alpha = \arctan2(y, x)$")
ax[1].set_xlabel("x")
ax[1].set_ylabel("y")
ax[1].axis("equal")
ax[1].grid(True)

plt.tight_layout()
plt.show()

```


    
![png](/img/contents/post/mcmc-statics/2_random-variables/1_mcmc_basics_142_0.png)
    



```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML

# åˆ›å»ºç½‘æ ¼
x = np.linspace(-1, 1, 20)
y = np.linspace(-1, 1, 20)
X, Y = np.meshgrid(x, y)
points = np.stack([X.ravel(), Y.ravel()], axis=1)

# éçº¿æ€§å˜æ¢ï¼ˆæ¨¡æ‹Ÿ Jacobian æ‹‰ä¼¸æ•ˆæœï¼‰
def transform(points, alpha):
    x, y = points[:, 0], points[:, 1]
    r = np.sqrt(x**2 + y**2)
    theta = np.arctan2(y, x)
    r_new = r + alpha * np.sin(3 * theta)  # æ·»åŠ æ‰°åŠ¨æ¨¡æ‹Ÿå½¢å˜
    x_new = r_new * np.cos(theta)
    y_new = r_new * np.sin(theta)
    return np.stack([x_new, y_new], axis=1)

# åˆå§‹åŒ–ç”»å¸ƒ
fig, ax = plt.subplots(figsize=(6, 6))
scat = ax.scatter(points[:, 0], points[:, 1], s=10)
ax.set_xlim(-2, 2)
ax.set_ylim(-2, 2)
ax.set_aspect('equal')
ax.set_title("Jacobian Deformation Demo")

# å¸§æ›´æ–°å‡½æ•°
def update(frame):
    alpha = 0.3 * np.sin(frame / 10)
    new_points = transform(points, alpha)
    scat.set_offsets(new_points)
    return scat,

# åŠ¨ç”»
ani = FuncAnimation(fig, update, frames=100, interval=100, blit=True)

# ä¿å­˜ä¸º GIFï¼ˆå¯é€‰ï¼‰
from matplotlib.animation import PillowWriter
ani.save("jacobian_area_deformation.gif", writer=PillowWriter(fps=10))

plt.close(fig)

from IPython.display import Image
Image(filename="jacobian_area_deformation.gif")

#HTML(ani.to_jshtml())
```

![png](/img/contents/post/mcmc-statics/2_random-variables/jacobian_area_deformation.gif)



### ğŸ¯ 3. é‡‡æ ·æ–¹æ¡ˆè®¾è®¡

#### âœ… ä¸€èˆ¬æ­¥éª¤ï¼š

1. ä»å·²çŸ¥è”åˆåˆ†å¸ƒ $f_{X,Y}(x,y)$ ä¸­é‡‡æ ·ä¸€ç»„ $(x,y)$
2. å°†å…¶è½¬æ¢ä¸ºæåæ ‡å½¢å¼ï¼š

   $$
   r^2 = x^2 + y^2, \quad \alpha = \arctan2(y, x)
   $$
3. å¾—åˆ°æ ·æœ¬ $(r^2, \alpha)$

#### âœ… åè¿‡æ¥ï¼šè‹¥ä½ æœ‰ $f_{R^2, \alpha}(r^2, \alpha)$ï¼Œå¦‚ä½•é‡‡æ ·ï¼Ÿ

* æ–¹æ³•ä¸€ï¼ˆ**åå‡½æ•°é‡‡æ ·**ï¼‰ï¼šè‹¥ $r^2$ å’Œ $\alpha$ å¯ç‹¬ç«‹åˆ†å¸ƒè¡¨ç¤ºï¼Œç›´æ¥åˆ†åˆ«é‡‡æ ·å†ç»„åˆã€‚
* æ–¹æ³•äºŒï¼ˆ**æ¥å—-æ‹’ç»é‡‡æ ·**ï¼‰ï¼šè®¾è®¡ proposal distributionï¼Œæ¯”å¦‚ï¼š

  * $r^2 \sim \text{Gamma}(k, \theta)$
  * $\alpha \sim \text{Uniform}(0, 2\pi)$

  ç„¶ååˆ©ç”¨ç›®æ ‡å¯†åº¦ $f_{R^2, \alpha}$ ä¸ proposal åšæ¯”å€¼ï¼Œä½¿ç”¨ rejection rule æ¥å—æˆ–æ‹’ç»ã€‚


