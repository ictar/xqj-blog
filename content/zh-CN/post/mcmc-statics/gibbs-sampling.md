---
title: "Gibbs é‡‡æ ·è¯¦è§£ï¼šåˆ†è€Œæ²»ä¹‹çš„é™ç»´æ™ºæ…§"
slug: "gibbs-sampling"
date: 2026-01-30
summary: "å½“é«˜ç»´ç©ºé—´è®©äººæ— ä»ä¸‹æ‰‹æ—¶ï¼ŒGibbs é‡‡æ ·é‡‡ç”¨äº†â€œå„ä¸ªå‡»ç ´â€çš„ç­–ç•¥ã€‚é€šè¿‡åˆ©ç”¨æ»¡æ¡ä»¶åˆ†å¸ƒï¼Œå®ƒå°†å¤æ‚çš„ N ç»´è”åˆåˆ†å¸ƒé‡‡æ ·æ‹†è§£ä¸º N ä¸ªç®€å•çš„ 1 ç»´é‡‡æ ·ã€‚æœ¬æ–‡è§£æå…¶ç›´è§‚ç›´è§‰ã€æ•°å­¦è¯æ˜ï¼ˆBrook's Lemmaï¼‰åŠä»£ç å®æˆ˜ã€‚"
series: ["MCMC"]
tags: ["Gibbsé‡‡æ ·", "MCMC", "æ¡ä»¶åˆ†å¸ƒ", "è´å¶æ–¯æ¨æ–­", "é™ç»´æ‰“å‡»", "Pythonå®ç°"]
toc: true
draft: false
---

{{< toc >}}

> Gibbs Sampler is an algorithm to build a **Markov Chain** having a given N-dimension **limit distribution**, exploring its **conditional distribution**.

# åˆ†è€Œæ²»ä¹‹çš„æ™ºæ…§ (The "Divide and Conquer" Intuition)

* **ç—›ç‚¹å›é¡¾ï¼š** åœ¨é«˜ç»´ç©ºé—´ï¼ˆæ¯”å¦‚ 100 ç»´ï¼‰åš MH é‡‡æ ·ï¼Œæƒ³ä¸€æ¬¡æ€§æè®®ä¸€ä¸ªæ–°çš„ 100 ç»´å‘é‡ $x_{new}$ ä¸”è¢«æ¥å—ï¼Œæ˜¯éå¸¸éš¾çš„ã€‚è¿™å°±åƒè¯•å›¾ä¸€æ¬¡æ€§çŒœä¸­ 100 ä¸ªç¡¬å¸çš„æ­£åé¢ã€‚
  * è™½ç„¶æˆ‘ä»¬å¯ä»¥æŠŠå¤šç»´é—®é¢˜é€šè¿‡ç»„åˆé™ç»´ä¸ºä¸€ä¸ªä¸€ç»´é—®é¢˜ã€‚ä½†æ˜¯å½“ç»´åº¦å˜å¤§å¹¶ä¸”æ¯ä¸ªç»´åº¦çš„çŠ¶æ€ç©ºé—´ä¹Ÿå¾ˆå¤§çš„æ—¶å€™ï¼Œé™ç»´ä¸ºä¸€ä¸ªä¸€ç»´é—®é¢˜åçš„çŠ¶æ€ç©ºé—´ä¼šæå¤§ä»¥è‡³äºè§£å†³æä¸ºå›°éš¾ã€‚
* **Gibbs çš„ç­–ç•¥ï¼š** **é™ç»´æ‰“å‡»**ã€‚
  * æˆ‘ä»¬ä¸ä¸€æ¬¡å˜åŠ¨æ‰€æœ‰ç»´åº¦ã€‚
  * æ¯æ¬¡**åªæ›´æ–°ä¸€ä¸ªç»´åº¦**ï¼ŒæŠŠå…¶ä»– 99 ä¸ªç»´åº¦è§†ä¸ºå¸¸é‡ï¼ˆå›ºå®šä½ï¼‰ã€‚


* **ç›´è§‚ç±»æ¯”ï¼š**
  * **MH ç®—æ³•**ï¼šåƒç›´å‡æœºä¸€æ ·åœ¨åœ°å›¾ä¸Šéšæœºè·³ã€‚
  * **Gibbs é‡‡æ ·**ï¼šåƒåœ¨æ›¼å“ˆé¡¿è¡—å¤´èµ°è·¯ï¼Œæ¯æ¬¡åªèƒ½æ²¿ä¸œè¥¿æ–¹å‘èµ°ï¼Œæˆ–è€…æ²¿å—åŒ—æ–¹å‘èµ°ï¼ˆAxis-aligned movesï¼‰ã€‚

## MH ç®—æ³•çš„å›°å¢ƒï¼šé«˜ç»´åº¦çš„â€œå½©ç¥¨â€

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨åšä¸€ä¸ª **100 ç»´** çš„é‡‡æ ·ä»»åŠ¡ï¼ˆæ¯”å¦‚ç”Ÿæˆä¸€å¼  10x10 åƒç´ çš„å°å›¾ç‰‡ï¼Œæ¯ä¸ªåƒç´ æ˜¯ä¸€ä¸ªç»´åº¦ï¼‰ã€‚

åœ¨ **Metropolis-Hastings (MH)** ä¸­ï¼Œä¸ºäº†ç”Ÿæˆä¸‹ä¸€ä¸ªæ ·æœ¬ï¼Œä½ é€šå¸¸éœ€è¦è®¾è®¡ä¸€ä¸ªæè®®åˆ†å¸ƒ $Q$ã€‚å¦‚æœä½ å°è¯•ä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰ 100 ä¸ªåƒç´ ï¼ˆç»´åº¦ï¼‰ï¼š
- ä½ æ˜¯åœ¨é—®ï¼šâ€œå˜¿ï¼Œè¿™ 100 ä¸ªæ•°å­—ç»„æˆçš„æ–°ç»„åˆï¼Œçœ‹èµ·æ¥åƒä¸€å¼ åˆç†çš„å›¾å—ï¼Ÿâ€
- åœ¨ä¸€ä¸ª 100 ç»´çš„ç©ºé—´é‡Œï¼ŒéšæœºççŒœä¸€ä¸ªâ€œå¥½ç‚¹â€çš„æ¦‚ç‡å°±åƒä¸­å½©ç¥¨ä¸€æ ·ä½ï¼
- ç»“æœï¼š ä½ çš„æè®®å‡ ä¹æ€»æ˜¯è¢«æ‹’ç»ã€‚æ¥å—ç‡æ¥è¿‘ 0ï¼Œç”µè„‘ç©ºè½¬ä¸€å¤©ä¹Ÿæ²¡åŠ¨é™ã€‚

## Gibbs çš„ç­–ç•¥ï¼šä¸€æ¬¡åªåšä¸€ä»¶äº‹

**Gibbs Sampling** è¯´ï¼šâ€œåˆ«è´ªå¿ƒã€‚æ—¢ç„¶åŒæ—¶çŒœ 100 ä¸ªæ•°å­—å¤ªéš¾ï¼Œé‚£æˆ‘ä»¬ä¸€æ¬¡åªçŒœ 1 ä¸ªæ•°å­—æ€ä¹ˆæ ·ï¼Ÿâ€

å®ƒçš„é€»è¾‘æ˜¯è¿™æ ·çš„ï¼š
1. é”å®šç¬¬ 2 åˆ°ç¬¬ 100 ä¸ªç»´åº¦ï¼ˆå‡è£…å®ƒä»¬æ˜¯å¸¸æ•°ï¼‰ã€‚
2. ç°åœ¨ï¼Œé—®é¢˜å˜æˆäº†ï¼šâ€œ**åœ¨å…¶ä»–äººéƒ½å›ºå®šçš„æƒ…å†µä¸‹ï¼Œç¬¬ 1 ä¸ªç»´åº¦çš„æœ€ä½³å€¼åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ**â€
3. è¿™æ˜¯ä¸€ä¸ª 1 ç»´ çš„é—®é¢˜ï¼è¿™å¤ªå®¹æ˜“äº†ã€‚æˆ‘ä»¬ç›´æ¥ä»è¿™ä¸ª**æ¡ä»¶åˆ†å¸ƒ (Conditional Distribution) **é‡ŒæŠ½ä¸€ä¸ªæ•°å‡ºæ¥ã€‚
4. æ›´æ–°ç¬¬ 1 ä¸ªç»´åº¦ã€‚æ¥ä¸‹æ¥ï¼Œé”å®šç¬¬ 1ã€3...100 ä¸ªç»´åº¦ï¼Œåªæ›´æ–°ç¬¬ 2 ä¸ª...

> æ ¸å¿ƒå“²å­¦ï¼š å°†ä¸€ä¸ªå¤æ‚çš„ $N$ ç»´é—®é¢˜ï¼Œæ‹†è§£æˆ $N$ ä¸ªç®€å•çš„ $1$ ç»´é—®é¢˜ã€‚
>
> æ•°å­¦ç»´åº¦ï¼š**å°†è”åˆåˆ†å¸ƒï¼ˆjoint distributionï¼‰è½¬æ¢ä¸ºæ¡ä»¶åˆ†å¸ƒï¼ˆconditional distributionï¼‰**

## è§†è§‰ç›´è§‰ï¼šæ›¼å“ˆé¡¿æ¼«æ­¥ (Manhattan Walk)

ä¸ºäº†å½¢è±¡åœ°ç†è§£å®ƒçš„è½¨è¿¹ï¼Œæˆ‘ä»¬å¯¹æ¯”ä¸€ä¸‹ MH å’Œ Gibbs åœ¨äºŒç»´åœ°å›¾ä¸Šçš„èµ°æ³•ã€‚

å‡è®¾æˆ‘ä»¬è¦çˆ¬ä¸€åº§å±±ï¼ˆç›®æ ‡åˆ†å¸ƒ $\pi$ï¼‰ï¼Œå±±é¡¶åœ¨å³ä¸Šè§’ã€‚

ğŸš **MH ç®—æ³•ï¼šç›´å‡æœºå¼è·³è·ƒ**
- åŠ¨ä½œï¼š å®ƒä¸ç®¡åœ°å½¢ï¼Œéšæœºå‘ä»»æ„æ–¹å‘ï¼ˆæ–œç€è·³ã€è¿œè·³ï¼‰æ‰”å‡ºä¸€ä¸ªæ¢æµ‹å™¨ã€‚
- è½¨è¿¹ï¼š å¯ä»¥åœ¨åœ°å›¾ä¸Šä»»æ„è§’åº¦ç§»åŠ¨ã€‚
- ä»£ä»·ï¼š å¦‚æœè·³åˆ°äº†æ‚¬å´–ï¼ˆä½æ¦‚ç‡åŒºï¼‰ï¼Œå°±ä¼šè¢«å¼¹å›æ¥ï¼ˆæ‹’ç»ï¼‰ã€‚

ğŸš¶ **Gibbs é‡‡æ ·ï¼šæ›¼å“ˆé¡¿è¡—å¤´æ¼«æ­¥**
- åŠ¨ä½œï¼š æƒ³è±¡ä½ åœ¨æ›¼å“ˆé¡¿è¿™ç§æ£‹ç›˜å¼è¡—é“çš„åŸå¸‚é‡Œèµ°è·¯ã€‚ä½ ä¸èƒ½ç©¿å¢™ï¼Œä¸èƒ½æ–œç€èµ°ã€‚ä½ åªèƒ½æ²¿ç€è¡—é“ï¼ˆåæ ‡è½´ï¼‰èµ°ã€‚
- è½¨è¿¹ï¼š
  1. å…ˆæ²¿ç€ X è½´ ç§»åŠ¨ï¼ˆæ›´æ–° $x$ï¼Œä¿æŒ $y$ ä¸å˜ï¼‰ã€‚
  2. å†æ²¿ç€ Y è½´ ç§»åŠ¨ï¼ˆæ›´æ–° $y$ï¼Œä¿æŒ $x$ ä¸å˜ï¼‰ã€‚
  3. é‡å¤ã€‚
- ç‰¹å¾ï¼š å®ƒçš„è½¨è¿¹æ°¸è¿œæ˜¯ç›´è§’æŠ˜çº¿ (Zig-zag)ï¼Œåƒæ˜¯åœ¨çˆ¬æ¥¼æ¢¯ã€‚

## ä¸ºä»€ä¹ˆè¿™ä¹ˆåšå®¹æ˜“ï¼Ÿ(åˆ‡ç‰‡æ€ç»´)
ä½ å¯èƒ½ä¼šé—®ï¼š**â€œåªæ˜¯æ¢ä¸ªæ–¹å‘èµ°ï¼Œä¸ºä»€ä¹ˆå°±ä¸ç”¨æ‹’ç»äº†ï¼Ÿâ€**

æƒ³è±¡ä¸€ä¸ªäºŒç»´çš„æ­£æ€åˆ†å¸ƒï¼ˆåƒä¸€ä¸ªå°å±±åŒ…ï¼‰ã€‚Gibbs é‡‡æ ·çš„æ¯ä¸€æ­¥ï¼Œå®é™…ä¸Šæ˜¯åœ¨å¯¹è¿™ä¸ªå±±åŒ…åš **â€œåˆ‡ç‰‡â€ (Slicing)**ã€‚
1. å½“ä½ å›ºå®š $y=5$ æ—¶ï¼Œä½ å°±åƒæ˜¯ç”¨ä¸€æŠŠåˆ€ï¼Œåœ¨ $y=5$ çš„ä½ç½®æ°´å¹³åˆ‡å¼€äº†è¿™ä¸ªå±±åŒ…ã€‚
2. **åˆ‡é¢æ˜¯ä¸€ä¸ª 1 ç»´çš„æ›²çº¿ã€‚**
3. Gibbs è¯´ï¼šâ€œè¯·ç›´æ¥åœ¨è¿™ä¸ª 1 ç»´æ›²çº¿ä¸Šé‡‡æ ·ã€‚â€
4. æ—¢ç„¶ä½ æ˜¯ç›´æ¥ä»è¿™ä¸ªåˆ‡é¢ä¸Šæ‹¿æ•°æ®ï¼Œæ‹¿åˆ°çš„è‚¯å®šæ˜¯åˆç†çš„ï¼Œæ‰€ä»¥æ¥å—ç‡ = 100%ï¼

# æ•°å­¦åŸç†

## æä¾›è”åˆåˆ†å¸ƒç­‰ä»·äºæä¾›æ‰€æœ‰çš„â€œæ»¡æ¡ä»¶åˆ†å¸ƒâ€

é€šå¸¸æˆ‘ä»¬è®¤ä¸ºï¼š**è”åˆåˆ†å¸ƒ (Joint Distribution) $P(x_1, \dots, x_n)$** åŒ…å«äº†æ‰€æœ‰çš„ä¿¡æ¯ï¼Œæœ‰äº†å®ƒï¼Œæ±‚è¾¹ç¼˜åˆ†å¸ƒæˆ–æ¡ä»¶åˆ†å¸ƒéƒ½æ˜¯ç®€å•çš„ç§¯åˆ†æˆ–é™¤æ³•è¿ç®—ã€‚ä½†åè¿‡æ¥å°±æ²¡é‚£ä¹ˆç›´è§‚äº†ï¼š**å¦‚æœæˆ‘åªç»™ä½ æ‰€æœ‰çš„â€œæ»¡æ¡ä»¶åˆ†å¸ƒâ€ (Full Conditionals) $P(x_i | x_{-i})$ï¼Œä½ çœŸçš„èƒ½å”¯ä¸€è¿˜åŸå‡ºåŸæ¥çš„è”åˆåˆ†å¸ƒå—ï¼Ÿ**

ç­”æ¡ˆæ˜¯ï¼š**æ˜¯çš„ï¼Œä½†æœ‰ä¸€ä¸ªå‰ææ¡ä»¶ï¼ˆæ­£æ€§å‡è®¾ï¼‰**ã€‚ è¿™ä¸ªç»“è®ºè¢«ç§°ä¸º **[Brook's Lemma (å¸ƒé²å…‹å¼•ç†)](https://zh.wikipedia.org/wiki/%E5%B8%83%E9%B2%81%E5%85%8B%E6%96%AF%E5%AE%9A%E7%90%86)**ã€‚

### è¯æ˜

**ç¬¬ä¸€æ­¥ï¼šJoint $\Rightarrow$ Conditionals (ç®€å•æ–¹å‘)**

è¿™ä¸ªæ–¹å‘éå¸¸ç›´è§‚ï¼Œå…¶å®å°±æ˜¯æ¡ä»¶æ¦‚ç‡çš„å®šä¹‰ã€‚å‡è®¾æˆ‘ä»¬è¦ä»è”åˆåˆ†å¸ƒ $P(x_1, \dots, x_n)$ æ¨å¯¼ç¬¬ $i$ ä¸ªå˜é‡çš„æ»¡æ¡ä»¶åˆ†å¸ƒã€‚
$$
P(x_i | x_{-i}) = \frac{P(x_1, \dots, x_n)}{P(x_{-i})} = \frac{P(x_1, \dots, x_n)}{\int P(x_1, \dots, x_n) dx_i}
$$

æ˜¾ç„¶ï¼Œåªè¦ç»™å®šäº†è”åˆåˆ†å¸ƒï¼Œåˆ†æ¯ï¼ˆè¾¹ç¼˜åˆ†å¸ƒï¼‰å¯ä»¥é€šè¿‡ç§¯åˆ†ç®—å‡ºï¼Œåˆ†å­æ˜¯å·²çŸ¥çš„ï¼Œæ‰€ä»¥æ‰€æœ‰çš„æ»¡æ¡ä»¶åˆ†å¸ƒä¹Ÿå°±å”¯ä¸€ç¡®å®šäº†ã€‚è¿™ä¸éœ€è¦ä»»ä½•æŠ€å·§ã€‚

**ç¬¬äºŒæ­¥ï¼šConditionals $\Rightarrow$ Joint (å›°éš¾æ–¹å‘ï¼šBrook's Lemma)**

è¿™æ˜¯ Gibbs Sampling çš„æ ¸å¿ƒã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªå¼•ç†ï¼Œæˆ‘ä»¬æ‹¿ç€ä¸€å †æ¡ä»¶åˆ†å¸ƒå»é‡‡æ ·ï¼Œæœ€åæ ¹æœ¬ä¸çŸ¥é“è‡ªå·±æ”¶æ•›åˆ°äº†ä»€ä¹ˆè”åˆåˆ†å¸ƒä¸Šã€‚

æˆ‘ä»¬éœ€è¦è¯æ˜ï¼š**ä»…é€šè¿‡æ‰€æœ‰çš„æ»¡æ¡ä»¶åˆ†å¸ƒï¼Œå¯ä»¥é‡æ„å‡ºè”åˆåˆ†å¸ƒï¼ˆå·®ä¸€ä¸ªå½’ä¸€åŒ–å¸¸æ•°ï¼‰ã€‚**

ä¸ºäº†è®©è¯æ˜ä¸é‚£ä¹ˆæ¯ç‡¥ï¼Œæˆ‘ä»¬ä»¥ äºŒç»´ (Bivariate) æƒ…å†µä¸ºä¾‹ã€‚å‡è®¾æœ‰ä¸¤ä¸ªå˜é‡ $x$ å’Œ $y$ã€‚
1. è®¾å®šç›®æ ‡ã€‚æˆ‘ä»¬è¦æ‰¾åˆ° $\frac{P(x, y)}{P(x_0, y_0)}$ çš„è¡¨è¾¾å¼ï¼Œå…¶ä¸­ $(x_0, y_0)$ æ˜¯ä»»æ„é€‰å®šçš„ä¸€ä¸ªå‚è€ƒç‚¹ï¼ˆåŸºå‡†ç‚¹ï¼‰ã€‚å¦‚æœæˆ‘ä»¬èƒ½ç”¨æ¡ä»¶åˆ†å¸ƒæŠŠè¿™ä¸ªæ¯”å€¼å†™å‡ºæ¥ï¼Œé‚£å°±è¯æ˜äº†è”åˆåˆ†å¸ƒæ˜¯å¯ä»¥è¢«è¿˜åŸçš„ã€‚
2. åˆ©ç”¨æ’ç­‰å¼ã€‚æˆ‘ä»¬æŠŠç›®æ ‡æ¯”å€¼æ‹†è§£æˆä¸¤æ­¥èµ°ï¼ˆä»åŸºå‡†ç‚¹ $(x_0, y_0)$ èµ°åˆ° $(x, y)$ï¼‰ï¼š$$\frac{P(x, y)}{P(x_0, y_0)} = \frac{P(x, y)}{P(x_0, y)} \cdot \frac{P(x_0, y)}{P(x_0, y_0)}$$æ³¨æ„çœ‹ï¼Œæˆ‘ä»¬æ’å…¥äº†ä¸€ä¸ªä¸­é—´çŠ¶æ€ $(x_0, y)$ã€‚
3. å±•å¼€ç¬¬ä¸€é¡¹ã€‚åˆ©ç”¨**è´å¶æ–¯å®šä¹‰**ï¼š$P(x, y) = P(x | y) P(y)$ å’Œ $P(x_0, y) = P(x_0 | y) P(y)$ã€‚ä»£å…¥ç¬¬ä¸€é¡¹ï¼š$$\frac{P(x, y)}{P(x_0, y)} = \frac{P(x | y) \cancel{P(y)}}{P(x_0 | y) \cancel{P(y)}} = \frac{P(x | y)}{P(x_0 | y)}$$çœ‹ï¼ è¾¹ç¼˜åˆ†å¸ƒ $P(y)$ æ¶ˆæ‰äº†ï¼è¿™ä¸€é¡¹å®Œå…¨åªç”±**æ¡ä»¶åˆ†å¸ƒ**å†³å®šã€‚
4. å±•å¼€ç¬¬äºŒé¡¹ã€‚åŒæ ·åˆ©ç”¨å®šä¹‰ï¼š$P(x_0, y) = P(y | x_0) P(x_0)$ å’Œ $P(x_0, y_0) = P(y_0 | x_0) P(x_0)$ã€‚ä»£å…¥ç¬¬äºŒé¡¹ï¼š$$\frac{P(x_0, y)}{P(x_0, y_0)} = \frac{P(y | x_0) \cancel{P(x_0)}}{P(y_0 | x_0) \cancel{P(x_0)}} = \frac{P(y | x_0)}{P(y_0 | x_0)}$$çœ‹ï¼ è¾¹ç¼˜åˆ†å¸ƒ $P(x_0)$ ä¹Ÿæ¶ˆæ‰äº†ï¼è¿™ä¸€é¡¹ä¹Ÿåªç”±æ¡ä»¶åˆ†å¸ƒå†³å®šã€‚
5. åˆå¹¶ç»“æœ (Brook's Lemma for 2D)å°†ä¸¤æ­¥åˆå¹¶ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š$$P(x, y) \propto \frac{P(x | y)}{P(x_0 | y)} \cdot \frac{P(y | x_0)}{P(y_0 | x_0)}$$

ç»“è®ºï¼šçœ‹ï¼Œç­‰å¼å³è¾¹**å…¨éƒ¨éƒ½æ˜¯æ¡ä»¶åˆ†å¸ƒ**ã€‚è¿™æ„å‘³ç€ï¼Œåªè¦ä½ å‘Šè¯‰æˆ‘ $P(x|y)$ å’Œ $P(y|x)$ é•¿ä»€ä¹ˆæ ·ï¼Œæˆ‘å°±èƒ½é€šè¿‡è¿™ä¸ªå…¬å¼ï¼Œç®—å‡ºä»»æ„ä¸€ç‚¹ $(x, y)$ ç›¸å¯¹äºåŸºå‡†ç‚¹ $(x_0, y_0)$ çš„æ¦‚ç‡æ¯”å€¼ã€‚è¿™å°±å”¯ä¸€ç¡®å®šäº†è”åˆåˆ†å¸ƒ $P(x, y)$ çš„å½¢çŠ¶ï¼ˆup to a constantï¼‰ã€‚

### æ¨å¹¿åˆ° N ç»´ (Brook's Lemma é€šå¼)

è¿™ä¸ªé€»è¾‘å¯ä»¥æ¨å¹¿åˆ° $n$ ç»´ã€‚æˆ‘ä»¬è¦è®¡ç®— $\frac{P(x_1, \dots, x_n)}{P(x_1^0, \dots, x_n^0)}$ã€‚

æˆ‘ä»¬å¯ä»¥åƒèµ°æ¥¼æ¢¯ä¸€æ ·ï¼Œæ¯æ¬¡åªæ”¹å˜ä¸€ä¸ªåæ ‡ï¼Œä» $\mathbf{x}^0$ èµ°åˆ° $\mathbf{x}$ï¼š$(0,0,0) \to (x_1, 0, 0) \to (x_1, x_2, 0) \to (x_1, x_2, x_3)$ã€‚

å…¬å¼é•¿è¿™æ ·ï¼š$$P(\mathbf{x}) \propto \prod_{i=1}^n \frac{P(x_i | x_1, \dots, x_{i-1}, x_{i+1}^0, \dots, x_n^0)}{P(x_i^0 | x_1, \dots, x_{i-1}, x_{i+1}^0, \dots, x_n^0)}$$

### é‡è¦çš„å‰æï¼šæ­£æ€§å‡è®¾ (Positivity Condition)

åœ¨ä¸Šé¢çš„è¯æ˜ä¸­ï¼Œä½ æœ‰æ²¡æœ‰å‘ç°ä¸€ä¸ªæ½œåœ¨çš„ Bugï¼Ÿæˆ‘ä»¬åœ¨åšé™¤æ³•ï¼åˆ†æ¯å‡ºç°äº† $P(x_0 | y)$ ä¹‹ç±»çš„é¡¹ã€‚

å¦‚æœåœ¨çŠ¶æ€ç©ºé—´ä¸­ï¼Œæ¦‚ç‡ $P(x)$ åœ¨æŸäº›åœ°æ–¹æ˜¯ 0 æ€ä¹ˆåŠï¼Ÿé™¤ä»¥ 0 æ˜¯éæ³•çš„ã€‚

è¿™å°±æ˜¯ **Hammersley-Clifford å®šç†** çš„è¦æ±‚ï¼š**è”åˆåˆ†å¸ƒå¿…é¡»æ»¡è¶³æ­£æ€§å‡è®¾ (Positivity Assumption)ã€‚** å³ï¼šå¯¹äºä»»æ„ $x_i$ï¼Œå¦‚æœè¾¹é™…ä¸Šå¯èƒ½å‘ç”Ÿï¼Œé‚£ä¹ˆå®ƒä»¬çš„ç»„åˆ $(x_1, \dots, x_n)$ çš„æ¦‚ç‡å¿…é¡» **å¤§äº 0**ã€‚

åä¾‹ï¼ˆæ¡ä»¶ä¸èƒ½å†³å®šè”åˆçš„æƒ…å†µï¼‰ï¼šæƒ³è±¡ä¸€ä¸ªæ£‹ç›˜ï¼Œåªæœ‰ç™½è‰²çš„æ ¼å­æœ‰æ¦‚ç‡ï¼ˆæ¦‚ç‡ä¸º 1ï¼‰ï¼Œé»‘è‰²çš„æ ¼å­æ¦‚ç‡ä¸º 0ã€‚
- $P(x|y)$ åªèƒ½å‘Šè¯‰ä½ ï¼šå¦‚æœåœ¨ç¬¬ $y$ è¡Œï¼Œä¸€å®šè¦åœ¨ç™½æ ¼å­é‡Œã€‚
- ä½†æ˜¯ï¼Œå®ƒæ— æ³•å‘Šè¯‰ä½ ï¼Œè¿™ä¸€è¡Œçš„ç™½æ ¼å­ å’Œ é‚£ä¸€è¡Œçš„ç™½æ ¼å­ è°çš„æ¦‚ç‡æ›´é«˜ã€‚å› ä¸ºå®ƒä»¬ä¹‹é—´å¯èƒ½è¢«é»‘æ ¼å­ï¼ˆæ¦‚ç‡ä¸º0çš„æ·±æ¸Šï¼‰éš”å¼€äº†ï¼Œä½ æ²¡æ³•é€šè¿‡â€œä¸­é—´è·¯å¾„â€èµ°è¿‡å»åšæ¯”è¾ƒï¼ˆæ¯”å€¼é“¾æ¡æ–­äº†ï¼‰ã€‚


## ä¸ºä»€ä¹ˆä¸ç”¨æ‹’ç»ï¼Ÿ(Why Acceptance is 100%?)

> Gibbs Sampling æœ¬è´¨ä¸Šå°±æ˜¯æ¥å—ç‡ $\alpha=1$ çš„ Metropolis-Hastings ç®—æ³•ã€‚

### ç›´è§‚ç†è§£ï¼šä¸ºä»€ä¹ˆä¸éœ€è¦â€œå®¡æ ¸â€ï¼Ÿ
å…ˆä¸ç”¨å…¬å¼ï¼Œå’±ä»¬ç”¨ä¸ªç”Ÿæ´»ä¸­çš„ä¾‹å­ã€‚
- Metropolis (ç›²çŒœ)ï¼šä½ è¦å»ä¹°è¡£æœã€‚ä½ é—­ç€çœ¼ç›éšæ‰‹æŠ“ä¸€ä»¶ï¼ˆæè®® $Q$ï¼‰ï¼Œç„¶åçå¼€çœ¼çœ‹çœ‹åˆä¸åˆèº«ï¼ˆè®¡ç®— $\pi$ï¼‰ã€‚å¦‚æœä¸åˆèº«ï¼Œä½ å°±æŠŠå®ƒæ‰”å›å»ï¼ˆæ‹’ç»ï¼‰ã€‚å› ä¸ºä½ æ˜¯çæŠ“çš„ï¼Œæ‰€ä»¥å¿…é¡»æœ‰â€œè¯•ç©¿å’Œæ‹’ç»â€çš„æœºåˆ¶æ¥ä¿è¯è´¨é‡ã€‚
- Gibbs (å®šåˆ¶):ä½ èµ°è¿›ä¸€å®¶è£ç¼åº—ã€‚è£ç¼é‡äº†ä½ çš„å°ºå¯¸ï¼ˆå›ºå®šä½å…¶ä»–å˜é‡ $x_{-i}$ï¼‰ï¼Œç„¶åç›´æ¥æŒ‰ç…§è¿™ä¸ªå°ºå¯¸ç»™ä½ åšäº†ä¸€ä»¶è¡£æœï¼ˆä»æ¡ä»¶åˆ†å¸ƒ $P(x_i | x_{-i})$ ä¸­é‡‡æ ·ï¼‰ã€‚è¯·é—®ï¼šè¿™ä»¶é‡èº«å®šåšçš„è¡£æœï¼Œè¿˜éœ€è¦â€œå®¡æ ¸â€å—ï¼Ÿä¸éœ€è¦ã€‚å› ä¸ºå®ƒæœ¬èº«å°±æ˜¯æ ¹æ®æ­£ç¡®çš„è§„åˆ™ç”Ÿæˆçš„ï¼Œæ‰€ä»¥å®ƒå¤©ç”Ÿå°±æ˜¯åˆæ³•çš„ã€‚

### æ•°å­¦è¯æ˜ï¼šMH æ¥å—ç‡å…¬å¼çš„å®Œç¾å¯¹æ¶ˆ

ç°åœ¨æˆ‘ä»¬ç”¨æ•°å­¦è¯­è¨€æ¥æŠŠè¿™ä¸ªâ€œé‡èº«å®šåšâ€çš„è¿‡ç¨‹ç¿»è¯‘ä¸€éã€‚

å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªå˜é‡ $x$ å’Œ $y$ã€‚**å½“å‰çŠ¶æ€ï¼š$(x, y)$** 
- åŠ¨ä½œï¼š æˆ‘ä»¬å†³å®šæ›´æ–° $x$ï¼ŒæŠŠ $y$ å›ºå®šä½ã€‚
- Gibbs çš„æè®®ï¼š ç›´æ¥ä»æ»¡æ¡ä»¶åˆ†å¸ƒä¸­é‡‡æ ·ä¸€ä¸ªæ–°çš„ $x^*$ã€‚

è¿™æ„å‘³ç€ï¼Œæˆ‘ä»¬çš„ **æè®®åˆ†å¸ƒ (Proposal Distribution) $Q$** å°±æ˜¯æ¡ä»¶æ¦‚ç‡ï¼š
$$Q(\text{new} | \text{old}) = Q(x^*, y | x, y) = P(x^* | y)$$
> æ³¨æ„ï¼šæè®®åªå–å†³äº $y$ï¼Œè·Ÿæ—§çš„ $x$ æ²¡å…³ç³»

åŒç†ï¼Œ**åå‘æè®®**ï¼ˆä»æ–°å˜å›æ—§ï¼‰çš„æ¦‚ç‡æ˜¯ï¼š$$Q(\text{old} | \text{new}) = Q(x, y | x^*, y) = P(x | y)$$

ç°åœ¨ï¼Œæˆ‘ä»¬è¦æŠŠè¿™äº›ä»£å…¥ MH æ¥å—ç‡å…¬å¼ï¼š$$\alpha = \frac{\pi(\text{new})}{\pi(\text{old})} \times \frac{Q(\text{old} | \text{new})}{Q(\text{new} | \text{old})}$$

1. ä»£å…¥ç›®æ ‡åˆ†å¸ƒ $\pi$ã€‚ç›®æ ‡åˆ†å¸ƒå°±æ˜¯è”åˆåˆ†å¸ƒ $P(x, y)$ã€‚$$\text{Target Ratio} = \frac{P(x^*, y)}{P(x, y)}$$
2. ä»£å…¥æè®®åˆ†å¸ƒ $Q$ã€‚å°±æ˜¯åˆšæ‰å†™çš„æ¡ä»¶åˆ†å¸ƒã€‚$$\text{Proposal Ratio} = \frac{P(x | y)}{P(x^* | y)}$$
3. åˆå¹¶å¹¶åˆ©ç”¨ä¹˜æ³•å…¬å¼$$A = \frac{P(x^*, y)}{P(x, y)} \times \frac{P(x | y)}{P(x^* | y)}$$åˆ©ç”¨æ¦‚ç‡ä¹˜æ³•å…¬å¼ï¼š**è”åˆæ¦‚ç‡ = æ¡ä»¶æ¦‚ç‡ $\times$ è¾¹ç¼˜æ¦‚ç‡**
   - åˆ†å­å±•å¼€ï¼š$P(x^*, y) = P(x^* | y) \cdot P(y)$
   - åˆ†æ¯å±•å¼€ï¼š$P(x, y) = P(x | y) \cdot P(y)$
   - ä»£å›å»ï¼š$$A = \frac{\mathbf{P(x^* | y)} \cdot \mathbf{P(y)}}{\mathbf{P(x | y)} \cdot \mathbf{P(y)}} \times \frac{\mathbf{P(x | y)}}{\mathbf{P(x^* | y)}}$$
4. è§è¯å¥‡è¿¹ (The Cancellation)æ‹¿å‡ºä½ çš„çº¢ç¬”ï¼Œå¼€å§‹æ¶ˆæ¶ˆä¹ï¼š
   - $P(y)$ï¼šåˆ†å­åˆ†æ¯éƒ½æœ‰ï¼ˆå› ä¸º $y$ æ²¡å˜ï¼‰ï¼Œæ¶ˆæ‰ï¼
   - $P(x^ | y)$*ï¼šå‰é¢çš„åˆ†å­æœ‰ï¼Œåé¢çš„åˆ†æ¯æœ‰ï¼Œæ¶ˆæ‰ï¼
   - $P(x | y)$ï¼šå‰é¢çš„åˆ†æ¯æœ‰ï¼Œåé¢çš„åˆ†å­æœ‰ï¼Œæ¶ˆæ‰ï¼
   - æœ€ç»ˆç»“æœï¼š$$A = 1$$

æ‰€ä»¥æ¥å—ç‡ $\alpha = \min(1, A) = 1$ã€‚

## Gibbs Sampling çš„æ­£ç¡®æ€§ï¼ˆç¼ºï¼‰


```python
##
```



# ç®—æ³•æµç¨‹

å‡è®¾æˆ‘ä»¬è¦ä»ä¸€ä¸ª $n$ ç»´çš„è”åˆåˆ†å¸ƒ $P(x_1, x_2, \dots, x_n)$ ä¸­è¿›è¡Œé‡‡æ ·ã€‚

1. åˆå§‹åŒ– (Initialization)ã€‚é€‰æ‹©ä¸€ä¸ªåˆå§‹çŠ¶æ€ $\mathbf{x}^{(0)} = (x_1^{(0)}, x_2^{(0)}, \dots, x_n^{(0)})$ã€‚è¿™ä¸ªç‚¹å¯ä»¥æ˜¯åœ¨çŠ¶æ€ç©ºé—´å†…éšæœºé€‰å–çš„ï¼Œæˆ–è€…æ ¹æ®å…ˆéªŒçŸ¥è¯†é€‰å®šçš„ã€‚
2. è¿­ä»£å¾ªç¯ (The Iteration Loop)å¯¹äºæ¯ä¸€æ¬¡è¿­ä»£ $t = 1, 2, \dots, T$ï¼šæˆ‘ä»¬è¦ä¾æ¬¡æ›´æ–°å‘é‡ $\mathbf{x}$ ä¸­çš„æ¯ä¸€ä¸ªåˆ†é‡ã€‚è¯·æ³¨æ„ï¼Œæ›´æ–°åçš„åˆ†é‡ä¼šç«‹å³å‚ä¸åˆ°ä¸‹ä¸€ä¸ªåˆ†é‡çš„é‡‡æ ·ä¸­ã€‚
   1. æ›´æ–°ç¬¬ 1 ç»´ï¼šä»ç¬¬ä¸€ä¸ªæ»¡æ¡ä»¶åˆ†å¸ƒä¸­é‡‡æ ·æ–°å€¼ $x_1^{(t)}$ï¼š$$x_1^{(t)} \sim P(x_1 \mid x_2^{(t-1)}, x_3^{(t-1)}, \dots, x_n^{(t-1)})$$
   2. æ›´æ–°ç¬¬ 2 ç»´ï¼šåˆ©ç”¨åˆšé‡‡åˆ°çš„ $x_1^{(t)}$ å’Œæ—§çš„å…¶ä½™ç»´åº¦ï¼š$$x_2^{(t)} \sim P(x_2 \mid x_1^{(t)}, x_3^{(t-1)}, \dots, x_n^{(t-1)})$$
   3. æ›´æ–°ç¬¬ $i$ ç»´ï¼š$$x_i^{(t)} \sim P(x_i \mid x_1^{(t)}, \dots, x_{i-1}^{(t)}, x_{i+1}^{(t-1)}, \dots, x_n^{(t-1)})$$
   4. æ›´æ–°ç¬¬ $n$ ç»´ï¼š$$x_n^{(t)} \sim P(x_n \mid x_1^{(t)}, x_2^{(t)}, \dots, x_{n-1}^{(t)})$$
3. æ”¶é›†æ ·æœ¬ (Data Collection)ã€‚å°†å®Œæˆä¸€è½®æ›´æ–°åçš„å‘é‡ $\mathbf{x}^{(t)} = (x_1^{(t)}, x_2^{(t)}, \dots, x_n^{(t)})$ è®°ä¸ºä¸€ä¸ªæ ·æœ¬ã€‚


## æ›´æ–°é¡ºåºçš„ä¸åŒç­–ç•¥

åœ¨å®é™…æ“ä½œä¸­ï¼Œæ›´æ–°é¡ºåºæœ‰å‡ ç§ä¸åŒçš„ç­–ç•¥ï¼š
1. **ç³»ç»Ÿæ‰«æ (Systematic Scan)**ï¼š
   - åšæ³•ï¼šä¸¥æ ¼æŒ‰ç…§ $1 \to 2 \to \dots \to n$ çš„é¡ºåºå¾ªç¯ã€‚
   - ç‰¹ç‚¹ï¼šå®ç°ç®€å•ï¼Œæœ€å¸¸ç”¨ã€‚
2. éšæœºæ‰«æ (Random Scan)ï¼š
   - åšæ³•ï¼šæ¯æ¬¡éšæœºæŠ½å–ä¸€ä¸ªç»´åº¦ $i \in \{1, \dots, n\}$ è¿›è¡Œæ›´æ–°ã€‚
   - ç‰¹ç‚¹ï¼šæ›´å®¹æ˜“æ»¡è¶³ç†è®ºä¸Šçš„ç»†è‡´å¹³è¡¡ï¼ˆDetailed Balanceï¼‰ï¼Œåœ¨æŸäº›ç‰¹å®šçš„æ•°å­¦è¯æ˜ä¸­æ›´å—é’çã€‚
3. åˆ†ç»„/å—é‡‡æ · (Blocked Gibbs)ï¼š
   - åšæ³•ï¼šå¦‚æœ $x_1$ å’Œ $x_2$ ç›¸å…³æ€§æå¼ºï¼ŒæŠŠå®ƒä»¬æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œä» $P(x_1, x_2 \mid \dots)$ ä¸­åŒæ—¶é‡‡æ ·ã€‚
   - ç‰¹ç‚¹ï¼šæœ‰æ•ˆè§£å†³ Gibbs åœ¨é¢å¯¹å¼ºç›¸å…³å˜é‡æ—¶â€œèµ°ä¸åŠ¨â€ï¼ˆæ”¶æ•›æ…¢ï¼‰çš„é—®é¢˜ã€‚


# ä»£ç å®æˆ˜
## ç¦»æ•£ç¤ºä¾‹ï¼šäºŒå…ƒç¦»æ•£ç³»ç»Ÿ (Bivariate Discrete System)

å‡è®¾æœ‰ä¸¤ä¸ªå˜é‡ $x$ å’Œ $y$ï¼Œå®ƒä»¬éƒ½æ˜¯ç¦»æ•£çš„ï¼Œä¸”åªèƒ½å– $0$ æˆ– $1$ã€‚è¿™å°±åƒæ˜¯æœ‰ä¸¤ä¸ªå¼€å…³ï¼Œæˆ–è€…æ˜¯ä¸¤åº§åªæœ‰ä¸¤ä¸ªåŒºåŸŸçš„å²›å±¿ã€‚

æˆ‘ä»¬å·²çŸ¥å®ƒä»¬åˆåœ¨ä¸€èµ·çš„æ¦‚ç‡è¡¨ï¼ˆè¿™æ˜¯æˆ‘ä»¬çš„ç›®æ ‡ï¼‰ï¼š


| x | y | P(x,y) | æè¿° |
| :--- | :--- | :--- | :--- |
| 0 | 0 | 0.1 | çŠ¶æ€ (0,0) |
| 0 | 1 | 0.4 | çŠ¶æ€ (0,1) |
| 1 | 0 | 0.3 | çŠ¶æ€ (1,0) |
| 1 | 1 | 0.2 | çŠ¶æ€ (1,1) |

ç›®æ ‡ï¼š æˆ‘ä»¬è¦åœ¨ä¸çŸ¥é“è¿™å¼ è¡¨å…¨è²Œçš„æƒ…å†µä¸‹ï¼Œåªé€šè¿‡å±€éƒ¨è§„åˆ™é‡‡æ ·ï¼Œæœ€ç»ˆè®©æ ·æœ¬çš„é¢‘ç‡ç¬¦åˆè¿™ä¸ªæ¯”ä¾‹ã€‚

åœ¨ç¦»æ•£ Gibbs é‡‡æ ·ä¸­ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“ï¼šå¦‚æœå›ºå®šä¸€ä¸ªï¼Œå¦ä¸€ä¸ªè¯¥æ€ä¹ˆå˜ï¼Ÿ

1. ç»™å®š $y$ï¼Œæ±‚ $x$ çš„æ¡ä»¶åˆ†å¸ƒ $P(x|y)$
   - å¦‚æœ $y=0$ï¼š
     - $P(x=0 | y=0) = \frac{P(0,0)}{P(0,0) + P(1,0)} = \frac{0.1}{0.1 + 0.3} = 0.25$
     - $P(x=1 | y=0) = 0.75$
   - å¦‚æœ $y=1$ï¼š
     - $P(x=0 | y=1) = \frac{P(0,1)}{P(0,1) + P(1,1)} = \frac{0.4}{0.4 + 0.2} \approx 0.67$
     - $P(x=1 | y=1) \approx 0.33$
2. ç»™å®š $x$ï¼Œæ±‚ $y$ çš„æ¡ä»¶åˆ†å¸ƒ $P(y|x)$
   - å¦‚æœ $x=0$ï¼š
     - $P(y=0 | x=0) = \frac{P(0,0)}{P(0,0) + P(0,1)} = \frac{0.1}{0.1 + 0.4} = 0.2$
     - $P(y=1 | x=0) = 0.8$
   - å¦‚æœ $x=1$ï¼š
     - $P(y=0 | x=1) = \frac{0.3}{0.3 + 0.2} = 0.6$
     - $P(y=1 | x=1) = 0.4$


```python
import numpy as np
import matplotlib.pyplot as plt

# 1. è®¾å®šæ»¡æ¡ä»¶åˆ†å¸ƒ (ä¾æ®ä¸Šé¢çš„è®¡ç®—ç»“æœ)
def sample_x_given_y(y):
    if y == 0:
        # P(x=0|y=0)=0.25, P(x=1|y=0)=0.75
        return np.random.choice([0, 1], p=[0.25, 0.75])
    else:
        # P(x=0|y=1)=0.67, P(x=1|y=1)=0.33
        return np.random.choice([0, 1], p=[0.67, 0.33])

def sample_y_given_x(x):
    if x == 0:
        # P(y=0|x=0)=0.2, P(y=1|x=0)=0.8
        return np.random.choice([0, 1], p=[0.2, 0.8])
    else:
        # P(y=0|x=1)=0.6, P(y=1|x=1)=0.4
        return np.random.choice([0, 1], p=[0.6, 0.4])

# 2. Gibbs é‡‡æ ·å¾ªç¯
def discrete_gibbs(n_iter):
    samples = []
    x, y = 0, 0  # åˆå§‹çŠ¶æ€
    
    for _ in range(n_iter):
        x = sample_x_given_y(y) # æ›´æ–° x
        y = sample_y_given_x(x) # æ›´æ–° y
        samples.append((x, y))
        
    return np.array(samples)

# 3. è¿è¡Œå¹¶åˆ†æç»“æœ
n_iter = 10000
results = discrete_gibbs(n_iter)

# è®¡ç®—æ¯ä¸ªçŠ¶æ€å‡ºç°çš„é¢‘ç‡
unique, counts = np.unique(results, axis=0, return_counts=True)
frequencies = counts / n_iter

print("--- ç¦»æ•£ Gibbs é‡‡æ ·ç»“æœ ---")
for state, freq in zip(unique, frequencies):
    print(f"çŠ¶æ€ {state}: é‡‡æ ·é¢‘ç‡ {freq:.4f}")

# å¯è§†åŒ–å‰ 50 æ­¥çš„è·¯å¾„
plt.figure(figsize=(6, 6))
plt.plot(results[:50, 0] + np.random.normal(0, 0.02, 50), 
         results[:50, 1] + np.random.normal(0, 0.02, 50), 
         'o-', alpha=0.5)
plt.xticks([0, 1])
plt.yticks([0, 1])
plt.title("Trace of First 50 Discrete Gibbs Steps\n(with small jitter for visibility)")
plt.xlabel("X state")
plt.ylabel("Y state")
plt.grid(True)
plt.show()
```

    --- ç¦»æ•£ Gibbs é‡‡æ ·ç»“æœ ---
    çŠ¶æ€ [0 0]: é‡‡æ ·é¢‘ç‡ 0.1017
    çŠ¶æ€ [0 1]: é‡‡æ ·é¢‘ç‡ 0.4054
    çŠ¶æ€ [1 0]: é‡‡æ ·é¢‘ç‡ 0.2928
    çŠ¶æ€ [1 1]: é‡‡æ ·é¢‘ç‡ 0.2001



    
![png](/img/contents/post/mcmc-statics/8_gibbs_sampling/8_mcmc_gibbs_10_1.png)
    


å³ä¾¿åœ¨å¦‚æ­¤ç®€å•çš„åªæœ‰ 4 ä¸ªçŠ¶æ€çš„ç¦»æ•£ç©ºé—´é‡Œï¼ŒGibbs ä¾ç„¶è¡¨ç°å¾—éå¸¸ä¼˜é›…ï¼š
- æ”¶æ•›æå¿«ï¼šå¯¹äºç®€å•çš„ç¦»æ•£é—®é¢˜ï¼ŒGibbs å‡ ä¹ç¬é—´å°±èƒ½é€šè¿‡å‡ æ¬¡â€œå…¨æ¡ä»¶é‡‡æ ·â€é”å®šç›®æ ‡åˆ†å¸ƒçš„æ¯”ä¾‹ã€‚
- è·¯å¾„ç‰¹å¾ï¼šè§‚å¯Ÿè½¨è¿¹å›¾ï¼ˆå‰ 50 æ­¥ï¼‰ï¼Œä½ ä¼šå‘ç°å®ƒåœ¨ $(0,0), (0,1), (1,0), (1,1)$ è¿™å››ä¸ªç‚¹ä¹‹é—´è·³è·ƒã€‚å› ä¸ºæ˜¯ç›´è§’ç§»åŠ¨ï¼Œå®ƒæ€»æ˜¯å…ˆæ¨ªç€å˜ï¼Œå†ç«–ç€å˜ã€‚
- åº”ç”¨åœºæ™¯ï¼šè¿™ç§ç¦»æ•£ Gibbs é‡‡æ ·æ˜¯ å›¾åƒå»å™ª (Image Denoising)ï¼ˆå¦‚ Ising æ¨¡å‹ï¼‰å’Œ éšå«ç‹„åˆ©å…‹é›·åˆ†å¸ƒ (LDA) ç­‰è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ã€‚åœ¨é‚£äº›åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬æœ‰æˆåƒä¸Šä¸‡ä¸ªç¦»æ•£å˜é‡ï¼ˆåƒç´ æˆ–å•è¯ï¼‰ï¼Œ Gibbs æ¯æ¬¡åªç¿»è½¬ä¸€ä¸ªåƒç´ æˆ–æ›´æ–°ä¸€ä¸ªå•è¯çš„åˆ†ç±»ã€‚


## è¿ç»­ç¤ºä¾‹ï¼šäºŒå…ƒæ­£æ€åˆ†å¸ƒ (Bivariate Normal Implementation)

æˆ‘ä»¬è¦é‡‡æ ·çš„ç›®æ ‡æ˜¯ä¸€ä¸ªäºŒç»´å‘é‡ $(x, y)$ï¼Œæœä»æ ‡å‡†äºŒå…ƒæ­£æ€åˆ†å¸ƒï¼š
- å‡å€¼ï¼š$\mu_x = 0, \mu_y = 0$
- æ–¹å·®ï¼š$\sigma_x = 1, \sigma_y = 1$
- ç›¸å…³ç³»æ•°ï¼š$\rho$ (rho)ã€‚è¿™æ˜¯ä¸€ä¸ª $[-1, 1]$ ä¹‹é—´çš„æ•°ï¼Œå†³å®šäº† $x$ å’Œ $y$ çš„å…³ç³»æœ‰å¤šç´§å¯†ã€‚

å¦‚æœæˆ‘ä»¬æƒ³ç›´æ¥ä»è¿™é‡Œé‡‡æ ·ï¼ˆæ¯”å¦‚ç”¨ Rejection Samplingï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†è¿™ä¸ªå…¬å¼ï¼š$$P(x, y) \propto \exp\left( -\frac{1}{2(1-\rho^2)} (x^2 - 2\rho xy + y^2) \right)$$

è¿™çœ‹èµ·æ¥å°±å¾ˆéº»çƒ¦ï¼Œå¯¹å§ï¼Ÿ

ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ Gibbs Samplerã€‚

æ ¹æ®å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„æ€§è´¨ï¼Œå¦‚æœæˆ‘ä»¬å·²çŸ¥ $y$ï¼Œé‚£ä¹ˆ $x$ çš„åˆ†å¸ƒå°±æ˜¯ï¼š$$P(x | y) = \mathcal{N}(\text{å‡å€¼}=\rho y, \text{æ–¹å·®}=1-\rho^2)$$

åè¿‡æ¥ï¼Œå¦‚æœæˆ‘ä»¬å·²çŸ¥ $x$ï¼Œé‚£ä¹ˆ $y$ çš„åˆ†å¸ƒå°±æ˜¯ï¼š$$P(y | x) = \mathcal{N}(\text{å‡å€¼}=\rho x, \text{æ–¹å·®}=1-\rho^2)$$

ç›´è§‚ç†è§£ï¼š
- å‡å€¼ $\rho y$ï¼šå¦‚æœä½ çŸ¥é“ $y$ æ˜¯æ­£çš„ï¼Œä¸” $x, y$ æ­£ç›¸å…³ ($\rho>0$)ï¼Œé‚£ä¹ˆ $x$ å¤§æ¦‚ç‡ä¹Ÿæ˜¯æ­£çš„ã€‚æ‰€ä»¥ $x$ çš„ä¸­å¿ƒä¼šå‘ $y$ åç§»ã€‚
- æ–¹å·® $1-\rho^2$ï¼šå¦‚æœç›¸å…³æ€§å¾ˆå¼º ($\rho \to 1$)ï¼Œæ–¹å·®è¶‹è¿‘äº 0ã€‚è¿™æ„å‘³ç€ä¸€æ—¦ $y$ ç¡®å®šäº†ï¼Œ$x$ å‡ ä¹ä¹Ÿå°±ç¡®å®šäº†ï¼ˆæ²¡ä»€ä¹ˆè‡ªç”±åº¦ï¼‰ã€‚


```python
import numpy as np
import matplotlib.pyplot as plt

# --- 1. å‚æ•°è®¾ç½® ---
rho = 0.8             # ç›¸å…³ç³»æ•° (Correlation)
n_samples = 2000      # é‡‡æ ·æ•°é‡
start_x, start_y = -4.0, -4.0 # æ•…æ„ä»ä¸€ä¸ªå¾ˆè¿œçš„è§’è½å¼€å§‹

# --- 2. Gibbs Sampler ---
def run_gibbs_sampler(n, rho, start_x, start_y):
    samples = np.zeros((n, 2))
    x, y = start_x, start_y
    
    # æ ‡å‡†å·® (Scale) æ˜¯æ–¹å·®çš„å¹³æ–¹æ ¹
    # conditional variance = 1 - rho^2
    cond_std = np.sqrt(1 - rho**2)
    
    for i in range(n):
        # A. å›ºå®š yï¼Œé‡‡æ · x
        # x ~ N(rho * y, 1 - rho^2)
        x = np.random.normal(loc=rho * y, scale=cond_std)
        
        # B. å›ºå®š xï¼Œé‡‡æ · y (æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯åˆšæ›´æ–°çš„ x)
        # y ~ N(rho * x, 1 - rho^2)
        y = np.random.normal(loc=rho * x, scale=cond_std)
        
        samples[i] = [x, y]
        
    return samples

# è¿è¡Œé‡‡æ ·
chain = run_gibbs_sampler(n_samples, rho, start_x, start_y)

# --- 3. ç»“æœå¯è§†åŒ– ---
plt.figure(figsize=(12, 5))

# å›¾ 1: è½¨è¿¹ç»†èŠ‚ (å‰ 50 æ­¥) - çœ‹çœ‹å®ƒæ˜¯æ€ä¹ˆèµ°çš„
plt.subplot(1, 2, 1)
plt.plot(chain[:50, 0], chain[:50, 1], 'o-', alpha=0.6, color='blue', markersize=4, label='Gibbs Path')
# ç”»å‡ºèµ·ç‚¹
plt.plot(start_x, start_y, 'ro', label='Start', markersize=8)
plt.title(f"Gibbs Trajectory (First 50 Steps)\nCorrelation rho={rho}")
plt.xlabel("X")
plt.ylabel("Y")
plt.legend()
plt.grid(True, alpha=0.3)
# å¼ºåˆ¶æ¨ªçºµæ¯”ä¾‹ä¸€è‡´ï¼Œè¿™æ ·æ‰èƒ½çœ‹å‡ºæ­£æ€åˆ†å¸ƒçš„æ¤­åœ†å½¢çŠ¶
plt.axis('equal')

# å›¾ 2: æœ€ç»ˆåˆ†å¸ƒæ•£ç‚¹å›¾
plt.subplot(1, 2, 2)
plt.scatter(chain[:, 0], chain[:, 1], s=5, alpha=0.3, color='green')
plt.title(f"Final Samples (N={n_samples})\nTarget: Bivariate Normal")
plt.xlabel("X")
plt.ylabel("Y")
plt.axis('equal')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```


    
![png](/img/contents/post/mcmc-statics/8_gibbs_sampling/8_mcmc_gibbs_13_0.png)
    


**ä»”ç»†è§‚å¯Ÿä¸Šé¢çš„å·¦å›¾ (è½¨è¿¹å›¾)ã€‚**

1. æ›¼å“ˆé¡¿å¼ç§»åŠ¨ (Orthogonal Moves)
   - è™½ç„¶æˆ‘ä»¬åœ¨ä»£ç é‡Œåªå­˜äº† `[x, y]` è¿™ä¸ªç‚¹ï¼Œä½†å®é™…ä¸Šæ¯æ¬¡æ›´æ–°åªå˜åŠ¨ä¸€ä¸ªåæ ‡ã€‚
   - å¦‚æœä½ æŠŠä¸­é—´è¿‡ç¨‹è¡¥å…¨ï¼Œå®ƒçš„çœŸå®è·¯å¾„å…¶å®æ˜¯ â€œ**ç›´è§’æŠ˜çº¿**â€ï¼š
     - å…ˆæ°´å¹³ç§»åŠ¨ï¼ˆæ›´æ–° $x$ï¼‰
     - å†å‚ç›´ç§»åŠ¨ï¼ˆæ›´æ–° $y$ï¼‰
     - å†æ°´å¹³ç§»åŠ¨...
   - è¿™éªŒè¯äº† Gibbs Sampling â€œæ²¿ç€åæ ‡è½´åˆ‡ç‰‡é‡‡æ ·â€ çš„æœ¬è´¨ã€‚
2. Burn-in çš„è¿‡ç¨‹
   - å°äººä» $(-4, -4)$ å‡ºå‘ã€‚
   - ä½ ä¼šçœ‹åˆ°å®ƒåœ¨å‰å‡ æ­¥è¿…é€Ÿåœ°å‘ä¸­å¿ƒ $(0, 0)$ é æ‹¢ã€‚
   - è¿™ä¸ªè¿‡ç¨‹éå¸¸å¿«ï¼Œå¯èƒ½åªéœ€è¦ 5-10 æ­¥å°±è¿›å…¥äº†â€œæ¤­åœ†â€çš„ä¸»ä½“åŒºåŸŸã€‚


# Gibbs çš„è½¯è‚‹â€”â€”å¼ºç›¸å…³æ€§ (The Kryptonite: High Correlation)

* **ä¸»è¦ç¼ºé™·ï¼š** è™½ç„¶æ¥å—ç‡æ˜¯ 100%ï¼Œä½†è¿™ä¸ä»£è¡¨å®ƒæ€»æ˜¯é«˜æ•ˆçš„ã€‚
* **åœºæ™¯æ¨¡æ‹Ÿï¼š** å½“ä¸¤ä¸ªå˜é‡é«˜åº¦ç›¸å…³ï¼ˆ$\rho = 0.99$ï¼‰æ—¶ï¼Œåˆ†å¸ƒå½¢çŠ¶åƒä¸€æ¡ç»†é•¿çš„å³¡è°·ã€‚
* **å›°å¢ƒï¼š** Gibbs åªèƒ½æ¨ªç€èµ°æˆ–ç«–ç€èµ°ã€‚åœ¨ç‹­çª„çš„æ–œå‘å³¡è°·é‡Œï¼Œå®ƒåªèƒ½ä»¥æå°çš„ç¢æ­¥åƒæ¥¼æ¢¯ä¸€æ ·æ…¢æ…¢æŒªåŠ¨ï¼ˆSlow Mixingï¼‰ã€‚
* **è§£å†³æ–¹æ¡ˆï¼š** Blocked Gibbs Samplingï¼ˆæ‰“åŒ…æ›´æ–°ï¼‰ã€‚


```python
import numpy as np
import matplotlib.pyplot as plt

# --- 1. å‚æ•°è®¾ç½® ---
rhos = {0: 'æ— ç›¸å…³', 0.99: 'æå¼ºç›¸å…³'}             # ç›¸å…³ç³»æ•° (Correlation)
n_samples = 2000      # é‡‡æ ·æ•°é‡
start_x, start_y = -4.0, -4.0 # æ•…æ„ä»ä¸€ä¸ªå¾ˆè¿œçš„è§’è½å¼€å§‹

# --- 2. Gibbs Sampler ---
def run_gibbs_sampler(n, rho, start_x, start_y):
    samples = np.zeros((n, 2))
    x, y = start_x, start_y
    
    # æ ‡å‡†å·® (Scale) æ˜¯æ–¹å·®çš„å¹³æ–¹æ ¹
    # conditional variance = 1 - rho^2
    cond_std = np.sqrt(1 - rho**2)
    
    for i in range(n):
        # A. å›ºå®š yï¼Œé‡‡æ · x
        # x ~ N(rho * y, 1 - rho^2)
        x = np.random.normal(loc=rho * y, scale=cond_std)
        
        # B. å›ºå®š xï¼Œé‡‡æ · y (æ³¨æ„ï¼šè¿™é‡Œç”¨çš„æ˜¯åˆšæ›´æ–°çš„ x)
        # y ~ N(rho * x, 1 - rho^2)
        y = np.random.normal(loc=rho * x, scale=cond_std)
        
        samples[i] = [x, y]
        
    return samples

# è¿è¡Œé‡‡æ ·
index = 1
for rho, rho_label in rhos.items():
    chain = run_gibbs_sampler(n_samples, rho, start_x, start_y)

    # --- 3. ç»“æœå¯è§†åŒ– ---
    plt.figure(figsize=(12, 10))

    # å›¾ 1: è½¨è¿¹ç»†èŠ‚ (å‰ 50 æ­¥) - çœ‹çœ‹å®ƒæ˜¯æ€ä¹ˆèµ°çš„
    plt.subplot(2, 2, index)
    plt.plot(chain[:50, 0], chain[:50, 1], 'o-', alpha=0.6, color='blue', markersize=4, label='Gibbs Path')
    # ç”»å‡ºèµ·ç‚¹
    plt.plot(start_x, start_y, 'ro', label='Start', markersize=8)
    plt.title(f"Gibbs Trajectory (First 50 Steps)\nCorrelation rho={rho}")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.legend()
    plt.grid(True, alpha=0.3)
    # å¼ºåˆ¶æ¨ªçºµæ¯”ä¾‹ä¸€è‡´ï¼Œè¿™æ ·æ‰èƒ½çœ‹å‡ºæ­£æ€åˆ†å¸ƒçš„æ¤­åœ†å½¢çŠ¶
    plt.axis('equal')
    index += 1

    # å›¾ 2: æœ€ç»ˆåˆ†å¸ƒæ•£ç‚¹å›¾
    plt.subplot(2, 2, index)
    plt.scatter(chain[:, 0], chain[:, 1], s=5, alpha=0.3, color='green')
    plt.title(f"Final Samples (N={n_samples})\nTarget: Bivariate Normal")
    plt.xlabel("X")
    plt.ylabel("Y")
    plt.axis('equal')
    plt.grid(True, alpha=0.3)
    index += 1

plt.tight_layout()
plt.show()
```


    
![png](/img/contents/post/mcmc-statics/8_gibbs_sampling/8_mcmc_gibbs_16_0.png)
    



    
![png](/img/contents/post/mcmc-statics/8_gibbs_sampling/8_mcmc_gibbs_16_1.png)
    


åœ¨ä¸Šé¢çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ä¿®æ”¹äº†ç›¸å…³å€¼ `rho`ã€‚
- è®¾å®š rho = 0 (æ— ç›¸å…³)ï¼š
  - æ¤­åœ†å˜æˆäº†ä¸€ä¸ª **æ­£åœ†**ã€‚
  - å°äººå¯ä»¥åœ¨åœ†é‡Œéšæ„è·³è·ƒï¼Œæ··åˆæå¿«ã€‚
- è®¾å®š rho = 0.99 (æå¼ºç›¸å…³)ï¼š
  - æ¤­åœ†å˜æˆäº†ä¸€æ¡ **æç»†çš„çº¿**ï¼ˆå³¡è°·ï¼‰ã€‚
  - è§‚å¯Ÿè½¨è¿¹ï¼š ä½ ä¼šå‘ç°å°äººåªèƒ½ä»¥æ­¤æå°çš„ç¢æ­¥æ²¿ç€å¯¹è§’çº¿æŒªåŠ¨ã€‚
  - åŸå› ï¼š å½“ $\rho=0.99$ æ—¶ï¼Œæ¡ä»¶æ–¹å·® $1-\rho^2$ æ¥è¿‘ 0ã€‚è¿™æ„å‘³ç€ $P(x|y)$ è¢«é”æ­»åœ¨ $y$ é™„è¿‘æå°çš„èŒƒå›´å†…ã€‚ä½ è™½ç„¶æ²¡æœ‰è¢«æ‹’ç»ï¼ˆæ¥å—ç‡100%ï¼‰ï¼Œä½†ä½ ä¹Ÿèµ°ä¸è¿œã€‚
  - è¿™å°±æ˜¯ Gibbs Sampling çš„è½¯è‚‹ï¼š**åœ¨å¼ºç›¸å…³åˆ†å¸ƒä¸­ï¼Œæ”¶æ•›ä¼šå˜å¾—éå¸¸æ…¢ã€‚**
