---
title: "Monte Carlo–Markov Chains Statistical Methods"
type: "docs"
layout: "section"
weight: 2
date: 2026-01-24
---

![MCMC Statistics Cover](/img/contents/post/mcmc-statics/mcmc_statics_cover.png)

Welcome to the **Monte Carlo–Markov Chains Statistical Methods** series, where we explore the theory and practice of probabilistic inference and MCMC sampling.

## Articles

1. [What is Probability?](./probability/)
2. [Random Variables and Sampling](./random-variables/)
   - Probability Density Function and Expectation
   - Sampling Methods for Simple Distributions
   - Introduction to Common Basic Sampling Algorithms
3. [Monte Carlo Methods](./monte-carlo/)
   - Importance Sampling
   - Variance Reduction Techniques
4. [Understanding Markov Chains](./markov-chains/)
   - What is a Markov Process
   - Stationary Distribution and Convergence
   - Constructing Simple State Transition Processes
5. [Introducing MCMC](./intro-mcmc/)
   - Why do we need MCMC?
   - From Markov Chains to Sampling
   - Theory and Intuition
6. [Metropolis Algorithm Explained: Implementation & Intuition](./metropolis/)
   - The Core Dilemma: Intractable Normalization Constants
   - Random Walk Metropolis Explained
   - Performance in High-Dimensional Distributions
7. [The Metropolis-Hastings Algorithm: Breaking the Symmetry](./metropolis-hastings/)
   - Why do we need "asymmetric" proposals?
   - Derivation and intuition of the Hastings Correction
   - Practical Case: Solving boundary problems with Log-Normal proposals
8. [Gibbs Sampling Explained: The Wisdom of Divide and Conquer](./gibbs-sampling/)
   - High-dimensional dilemmas and the "Manhattan Walk" intuition
   - Mathematical principle: Brook's Lemma
   - Python implementation for discrete and continuous distributions
9. [Deterministic Optimization Explained: The Mathematical Essence of Gradient Descent](./deterministic-optimization/)
   - Geometric Intuition of Convex vs. Non-Convex Optimization
   - Newton's Method and Second-Order Approximation
   - Connection between Coordinate Descent and Gibbs Sampling
   - Pros and Cons of Steepest Descent
10. [Stochastic Optimization Explained: Simulated Annealing & Pincus Theorem](./stochastic-optimization/)
    - From Energy Minimization to Probability Maximization: The Physics of Annealing
    - High-Temp Exploration & Low-Temp Exploitation: Another Perspective on Metropolis
    - Pincus Theorem: Mathematical Proof of Convergence to Global Optimum
11. [Convergence Diagnostics](./convergence/)
12. [Python Practical: MCMC Modeling](./python/)
