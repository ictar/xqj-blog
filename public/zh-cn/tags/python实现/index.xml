<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python实现 on 琼呀</title>
    <link>http://localhost:1313/zh-cn/tags/python%E5%AE%9E%E7%8E%B0/</link>
    <description>Recent content in Python实现 on 琼呀</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <managingEditor>ele.qiong@gmail.com (Qiongjie.X)</managingEditor>
    <webMaster>ele.qiong@gmail.com (Qiongjie.X)</webMaster>
    <lastBuildDate>Sun, 01 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/zh-cn/tags/python%E5%AE%9E%E7%8E%B0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>确定性优化算法详解：梯度下降的数学本质与代码实战</title>
      <link>http://localhost:1313/zh-cn/post/mcmc-statics/deterministic-optimization/</link>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/zh-cn/post/mcmc-statics/deterministic-optimization/</guid>
      <description>在机器学习的训练过程中，优化算法扮演着“导航员”的角色，指引模型参数从随机初始值逐步逼近最优解。本文将深入剖析最基础也是最重要的确定性优化算法——梯度下降（Gradient Descent）。我们将从其数学原理出发，推导其更新公式，探讨不同变体（批量、随机、小批量）的优缺点，并通过 Python 代码直观展示其收敛过程。</description>
    </item>
    <item>
      <title>Gibbs 采样详解：分而治之的降维智慧</title>
      <link>http://localhost:1313/zh-cn/post/mcmc-statics/gibbs-sampling/</link>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/zh-cn/post/mcmc-statics/gibbs-sampling/</guid>
      <description>当高维空间让人无从下手时，Gibbs 采样采用了“各个击破”的策略。通过利用满条件分布，它将复杂的 N 维联合分布采样拆解为 N 个简单的 1 维采样。本文解析其直观直觉、数学证明（Brook&amp;rsquo;s Lemma）及代码实战。</description>
    </item>
    <item>
      <title>Metropolis-Hastings 算法：打破对称性的束缚</title>
      <link>http://localhost:1313/zh-cn/post/mcmc-statics/metropolis-hastings/</link>
      <pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/zh-cn/post/mcmc-statics/metropolis-hastings/</guid>
      <description>原版 Metropolis 受限于对称提议，常在边界“撞墙”或高维迷路。MH 算法引入“哈斯廷斯修正项”，允许不对称提议（如 Langevin 动力学）并维持细致平衡，大幅提升效率。</description>
    </item>
    <item>
      <title>Metropolis 算法详解：从原理到 Python 实现</title>
      <link>http://localhost:1313/zh-cn/post/mcmc-statics/metropolis/</link>
      <pubDate>Sat, 24 Jan 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/zh-cn/post/mcmc-statics/metropolis/</guid>
      <description>Metropolis 算法是 MCMC 的基石。本文深入探讨其应对未归一化概率密度的策略，从随机游走机制到高维相关高斯分布的采样实战，提供完整的 Python 实现与可视化分析。</description>
    </item>
  </channel>
</rss>
