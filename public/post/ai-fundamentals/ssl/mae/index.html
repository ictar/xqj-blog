

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>MAE: Masked Autoencoders Are Scalable Vision Learners - </title>

  <meta name="description" content="Randomly mask image patches and reconstruct the missing ones to learn context-aware visual representations.">
  <meta name="author" content="Qiongjie.X"/><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Qiongjie\u0027s Notes",
    
    "url": "http:\/\/localhost:1313\/"
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "",
  "url": "http:\/\/localhost:1313\/"
  
  
  
  
}
</script>
<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "http:\/\/localhost:1313\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "http:\/\/localhost:1313\/post\/ai-fundamentals\/ssl\/mae\/",
          "name": "Mae masked autoencoders are scalable vision learners"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "Qiongjie.X"
  },
  "headline": "MAE: Masked Autoencoders Are Scalable Vision Learners",
  "description" : "Randomly mask image patches and reconstruct the missing ones to learn context-aware visual representations.",
  "inLanguage" : "en",
  "wordCount":  1330 ,
  "datePublished" : "2025-10-08T00:00:00\u002b00:00",
  "dateModified" : "2025-10-08T00:00:00\u002b00:00",
  "image" : "http:\/\/localhost:1313\/img\/avatar-icon.png",
  "keywords" : [ "SSL, Vision, Representation Learning, Masked Image Modeling" ],
  "mainEntityOfPage" : "http:\/\/localhost:1313\/post\/ai-fundamentals\/ssl\/mae\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "http:\/\/localhost:1313\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "http:\/\/localhost:1313\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>


<meta property="og:title" content="MAE: Masked Autoencoders Are Scalable Vision Learners" />
<meta property="og:description" content="Randomly mask image patches and reconstruct the missing ones to learn context-aware visual representations.">
<meta property="og:image" content="http://localhost:1313/img/avatar-icon.png" />
<meta property="og:url" content="http://localhost:1313/post/ai-fundamentals/ssl/mae/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Qiongjie&#39;s Notes" />

  <meta name="twitter:title" content="MAE: Masked Autoencoders Are Scalable Vision Learners" />
  <meta name="twitter:description" content="Randomly mask image patches and reconstruct the missing ones to learn context-aware visual representations.">
  <meta name="twitter:image" content="http://localhost:1313/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='http://localhost:1313/img/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.147.4">
  <link rel="alternate" href="http://localhost:1313/index.xml" type="application/rss+xml" title="Qiongjie&#39;s Notes"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="http://localhost:1313/css/main.css" /><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="http://localhost:1313/css/syntax.css" /><link rel="stylesheet" href="http://localhost:1313/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">



<link
  rel="stylesheet"
  href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css"
  crossorigin="anonymous"
/>
<link rel="stylesheet" href="http://localhost:1313/css/custom.css">

<script
  src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"
  crossorigin="anonymous"
></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://localhost:1313/">Qiongjie&#39;s Notes</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li>
              <a title="PROJECT" href="/">PROJECT</a>
            </li>
          
        
          
            <li>
              <a title="NOTE" href="/notes">NOTE</a>
            </li>
          
        
          
            <li>
              <a title="TAG" href="/tags">TAG</a>
            </li>
          
        
          
            <li>
              <a title="ABOUT ME" href="/page/about/">ABOUT ME</a>
            </li>
          
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" role="button" tabindex="0">post</a>
              <div class="navlinks-children">
                
                  <a href="/post/python-geodata">Remote Sensing with Python</a>
                
                  <a href="/post/mcmc-statics">Monte Carlo–Markov Chains Statistical Methods</a>
                
                  <a href="/post/ai-fundamentals">AI Fundamentals</a>
                
              </div>
            </li>
          
        

        
          
            <li>
              
            </li>
          
        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Qiongjie&#39;s Notes" href="http://localhost:1313/">
            <img class="avatar-img" src="http://localhost:1313/img/avatar-icon.png" alt="Qiongjie&#39;s Notes" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>MAE: Masked Autoencoders Are Scalable Vision Learners</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on October 8, 2025
  
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1330&nbsp;words
  
  
  
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row">
    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        <div class="model-card">
<h2 id="-model-name">🏷️ Model Name</h2>
<p><strong>MAE – Masked Autoencoders Are Scalable Vision Learners</strong></p>
<h2 id="-core-idea">🧠 Core Idea</h2>
<blockquote>
<p>Randomly mask image patches and reconstruct the missing ones to learn context-aware visual representations.</p></blockquote>
<p><img src="https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png" alt="MAE architecture"></p>
<h2 id="-architecture">🖼️ Architecture</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-plaintext" data-lang="plaintext"><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                |     Input Image       |
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                            |
</span></span><span style="display:flex;"><span>                            v
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                | Patch Embedding (ViT) |
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                            |
</span></span><span style="display:flex;"><span>                |-----------------------------|
</span></span><span style="display:flex;"><span>                |  Random Masking (e.g. 75%)  |
</span></span><span style="display:flex;"><span>                |-----------------------------|
</span></span><span style="display:flex;"><span>                            |
</span></span><span style="display:flex;"><span>                            v
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                |     Encoder (ViT)     |   &lt;-- only visible patches
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                            |
</span></span><span style="display:flex;"><span>                            v
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                | Add Mask Tokens + Pos |
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                            |
</span></span><span style="display:flex;"><span>                            v
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                |     Decoder (ViT)     |   &lt;-- lightweight
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                            |
</span></span><span style="display:flex;"><span>                            v
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                |  Reconstructed Image  |
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                            |
</span></span><span style="display:flex;"><span>                            v
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span><span style="display:flex;"><span>                |     MSE Loss          |
</span></span><span style="display:flex;"><span>                +-----------------------+
</span></span></code></pre></div><p>And the pseudocode:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> image <span style="color:#f92672">in</span> dataloader:
</span></span><span style="display:flex;"><span>    patches <span style="color:#f92672">=</span> patchify(image) <span style="color:#75715e"># Divide image into patches</span>
</span></span><span style="display:flex;"><span>    visible, masked <span style="color:#f92672">=</span> random_mask(patches, ratio<span style="color:#f92672">=</span><span style="color:#ae81ff">0.75</span>) <span style="color:#75715e"># Randomly mask 75% patches.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    latent <span style="color:#f92672">=</span> encoder(visible) <span style="color:#75715e"># Feed visible patches into encoder.</span>
</span></span><span style="display:flex;"><span>    full_tokens <span style="color:#f92672">=</span> fill_with_mask_tokens(latent, masked)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    reconstructed <span style="color:#f92672">=</span> decoder(full_tokens) <span style="color:#75715e"># Reconstruct all patches using a lightweight decoder.</span>
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> mse_loss(reconstructed[masked], patches[masked]) <span style="color:#75715e"># Compute loss between reconstructed and original patches.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Backpropagate through encoder + decoder.</span>
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    update(encoder, decoder)
</span></span></code></pre></div><h3 id="1-image-patching-and-masking">1️⃣ Image Patching and Masking</h3>
<ol>
<li><strong>Divide the image into patches</strong>: The input image is first divided into regular, non-overlapping patches, similar to the approach used in Vision Transformers (ViT).</li>
<li><strong>Apply masking</strong>: A large random subset of these patches is sampled and removed (masked out). This random sampling is typically performed <strong>without replacement</strong>, following a uniform distribution. Crucially, MAE usually employs a <strong>very high masking ratio</strong>, such as 75%, to reduce redundancy and force the model to learn holistic understanding rather than relying on low-level statistics.</li>
</ol>
<h3 id="2-mae-encoder-processing">2️⃣ MAE Encoder Processing</h3>
<p>The MAE encoder, which is a <text style="color:purple">Vision Transformer (ViT)</text>, operates exclusively on the small subset of <strong>visible, unmasked patches</strong>. Mask tokens are explicitly skipped at this stage.</p>
<p>The visible patches are first mapped via a <text style="color:purple">linear projection</text>, and <text style="color:purple">positional embeddings</text> are added. These patches are then processed through a series of <text style="color:purple">Transformer blocks</text>, generating a latent representation for the visible subset.</p>
<p>By having the encoder process only 25% (or less) of the input patches, the pre-training time is greatly reduced (e.g., 3× or more speedup) and memory consumption is lowered.</p>
<h3 id="3-decoder-input-preparation">3️⃣ Decoder Input Preparation</h3>
<ol>
<li><strong>Reintroduce missing patches</strong>: After the encoder processes the visible patches, the encoded visible patches are combined with a set of mask tokens. Each mask token is a shared, learned vector used to indicate the presence of a missing patch that needs to be predicted.</li>
<li><strong>Add positional information</strong>: Positional embeddings are added to all tokens in this full set (both the encoded visible patches and the mask tokens) so that the mask tokens receive information about their location in the original image. The encoder output may pass through a linear projection layer to match the decoder&rsquo;s width.</li>
</ol>
<h3 id="4-mae-decoder-reconstruction">4️⃣ MAE Decoder Reconstruction</h3>
<p>The full set of tokens (encoded patches + mask tokens) is processed by the MAE decoder, which consists of another series of Transformer blocks.</p>
<ol>
<li><strong>Asymmetric design</strong>: The decoder is intentionally designed to be lightweight (narrower and shallower than the encoder), for instance, having 8 blocks and a 512-dimensional width, accounting for a small fraction of the overall compute (e.g., &lt;10% computation per token vs. the encoder).</li>
<li><strong>Reconstruct pixels</strong>: The decoder performs the reconstruction task by predicting the pixel values for each masked patch. The final layer of the decoder is a linear projection, yielding a vector of pixel values for each patch. The decoder output is then reshaped to form the reconstructed image.</li>
</ol>
<h3 id="5-loss-calculation">5️⃣ Loss Calculation</h3>
<ol>
<li>Define reconstruction target: The reconstruction target is typically the pixel values of the original image. Using normalized pixel values (where the mean and standard deviation are computed per patch) often improves the quality of the learned representation.</li>
<li>Compute loss: The MAE uses the <text style="color:purple">mean squared error (MSE)</text> as its loss function, which is computed <strong>only on the masked patches</strong>.</li>
</ol>
<h3 id="-deployment">🎉 Deployment</h3>
<ol>
<li><strong>Discard the decoder</strong>: After the self-supervised pre-training is finished, the decoder component is discarded.</li>
<li><strong>Use the encoder</strong>: The encoder is retained and applied to uncorrupted images (the full set of patches) to extract features for various downstream recognition tasks, such as fine-tuning a classifier.</li>
</ol>
<h2 id="-downstream-tasks">🎯 Downstream Tasks</h2>
<ul>
<li>Image Classification: Evaluated via end-to-end fine-tuning on ImageNet-1K and using linear probing on frozen features.</li>
<li>Object Detection: Transfer learning, typically fine-tuned on datasets such as COCO and PASCAL VOC.</li>
<li>Instance Segmentation: Transfer learning, evaluated on datasets like COCO.</li>
<li>Semantic Segmentation: Transfer learning, evaluated on datasets like ADE20K using UperNet.</li>
<li>Transfer Classification Tasks: Fine-tuning on diverse classification datasets, including iNaturalists and Places (e.g., Places205, Places365).</li>
<li>Model Robustness: Evaluating performance on corrupted or modified image datasets (e.g., IN-Corruption, IN-Adversarial).</li>
<li>Low-Level Tasks: Masked Image Modeling (MIM), the paradigm MAE belongs to, is suitable for tasks such as denoising and superresolution.</li>
</ul>
<h2 id="-strengths">💡 Strengths</h2>
<ul>
<li>Training Efficiency and Scalability
<ul>
<li><strong>High Training Speed</strong>: MAE significantly accelerates training, achieving a speedup of 3x or more in pre-training time due to its asymmetric design. Wall-clock speedups can range from 2.8x to 4.1x for large models.</li>
<li><strong>Reduced Computation and Memory</strong>: The asymmetric architecture mandates that the encoder only processes the small subset of visible patches, leading to a large reduction in computation and lowering memory consumption. The overall training FLOPs can be reduced by 3.3x.</li>
<li><strong>Scalable Architecture</strong>: MAE is a scalable self-supervised learner that efficiently enables the training of very large models such as ViT-Huge.</li>
<li><strong>Simplicity of Implementation</strong>: The approach is conceptually simple, effective, and scalable. It does not require specialized sparse operations, and the pixel-based reconstruction target is much simpler than tokenization methods like BEiT.</li>
</ul>
</li>
<li>Superior Performance and Generalization
<ul>
<li>State-of-the-Art Accuracy: MAE pre-training enables high-capacity models to generalize well. A vanilla ViT-Huge model achieved the best reported accuracy (87.8% Top-1) among methods using only ImageNet-1K data.</li>
<li>Improved Transferability: Transfer performance on downstream tasks (e.g., object detection, instance segmentation, semantic segmentation) outperforms supervised pre-training.</li>
<li>Scaling Gains: MAE shows strong scaling behavior, with generalization gains increasing significantly as model capacity grows, following a trend similar to models supervisedly pre-trained on massive datasets (like JFT-300M).</li>
</ul>
</li>
<li>Architectural and Training Robustness
<ul>
<li><strong>Tolerance to High Masking</strong>: Masking a very high proportion of random patches (e.g., 75%) yields a non-trivial and meaningful self-supervisory task, which is crucial for reducing image redundancy and forcing holistic understanding.</li>
<li><strong>Minimal Data Augmentation Required</strong>: The framework works well with minimal or no data augmentation (only cropping/resizing), unlike contrastive methods that heavily rely on complex augmentations (like color jittering) for regularization.</li>
<li><strong>Effective Non-Linear Features</strong>: MAE produces stronger non-linear features compared to contrastive methods (like MoCo v3). Fine-tuning just the last one or two Transformer blocks can yield significant accuracy gains beyond linear probing.</li>
<li><strong>Flexibility of Decoder</strong>: The decoder is lightweight (e.g., &lt;10% computation per token vs. the encoder) and flexible in its design.</li>
</ul>
</li>
</ul>
<h2 id="-limitations">⚠️ Limitations</h2>
<ul>
<li>Weakness in Linear Separability
<ul>
<li><text style="background-color:yellow">Low Linear Probing Accuracy</text>: MAE representations are generally less linearly separable. The accuracy obtained via the linear probing protocol is typically lower than that achieved by contrastive methods (like MoCo v3).</li>
<li><text style="background-color:yellow">Lack of Correlation with Fine-Tuning</text>: Linear probing results are largely uncorrelated with end-to-end fine-tuning results.</li>
<li><text style="background-color:yellow">Dependency on Decoder Depth (for linear probing)</text>: Achieving high linear probing accuracy requires a sufficiently deep decoder to ensure the learned latent representations remain at a high abstract level, rather than specializing too much on pixel reconstruction.</li>
</ul>
</li>
<li>Architectural Constraints and Task Suitability
<ul>
<li><text style="background-color:yellow">ViT Dependency</text>: Masked Image Modeling (MIM) approaches, including MAE, require a ViT backbone because the method is based on processing sequential patches. This contrasts with Joint Embedding approaches which are architecture agnostic.</li>
<li><text style="background-color:yellow">Lower Semantic Level of Reconstruction</text>: Reconstructing raw pixels means the decoder output is often considered of a lower semantic level compared to the high-level semantic information generated in language models (e.g., predicting words in BERT).</li>
<li><text style="background-color:yellow">Qualitative Reconstruction Issues</text>: Since the loss is computed only on masked patches, the model output on visible patches is qualitatively worse in reconstructions.</li>
<li><text style="background-color:yellow">Masking Strategy Sensitivity</text>: While random sampling works best, the model is susceptible to poorer performance if less effective masking strategies, such as block-wise masking, are used at high ratios.</li>
</ul>
</li>
</ul>
<h2 id="-reference">📚 Reference</h2>
<ul>
<li><em>He et al., 2022</em>  <em>[Masked Autoencoders Are Scalable Vision Learners]</em>  🔗 <a href="https://arxiv.org/abs/2111.06377">arXiv:2111.06377</a></li>
<li><a href="https://github.com/facebookresearch/mae">Github: facebookresearch/mae</a></li>
</ul>
</div>

        
          <div class="blog-tags">
            
              
              <a href="http://localhost:1313/tags/ssl/">SSL</a>&nbsp;
            
              
              <a href="http://localhost:1313/tags/vision/">Vision</a>&nbsp;
            
              
              <a href="http://localhost:1313/tags/representation-learning/">Representation Learning</a>&nbsp;
            
              
              <a href="http://localhost:1313/tags/masked-image-modeling/">Masked Image Modeling</a>&nbsp;
            
          </div>
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fai-fundamentals%2fssl%2fmae%2f&amp;text=MAE%3a%20Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fpost%2fai-fundamentals%2fssl%2fmae%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fai-fundamentals%2fssl%2fmae%2f&amp;title=MAE%3a%20Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fai-fundamentals%2fssl%2fmae%2f&amp;title=MAE%3a%20Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fai-fundamentals%2fssl%2fmae%2f&amp;title=MAE%3a%20Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fai-fundamentals%2fssl%2fmae%2f&amp;description=MAE%3a%20Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          
            
          

          
                  <h4 class="see-also">See also</h4>
                  <ul>
                
                
                    <li><a href="/post/ai-fundamentals/ssl/maskfeat/">MaskFeat: Masked Feature Prediction for Self-Supervised Visual Pre-Training</a></li>
                
                    <li><a href="/post/ai-fundamentals/ssl/byol/">BYOL: Bootstrap Your Own Latent</a></li>
                
                    <li><a href="/post/ai-fundamentals/ssl/dino/">DINO: Self-Distillation with No Labels</a></li>
                
                    <li><a href="/post/ai-fundamentals/ssl/swav/">SwAV: Swapping Assignments between Views</a></li>
                
                    <li><a href="/post/ai-fundamentals/ssl/moco/">MoCo: Momentum Contrast for Unsupervised Visual Representation Learning</a></li>
                
              </ul>

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="http://localhost:1313/post/ai-fundamentals/ssl/swav/" data-toggle="tooltip" data-placement="top" title="SwAV: Swapping Assignments between Views">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="http://localhost:1313/post/ai-fundamentals/ssl/dino/" data-toggle="tooltip" data-placement="top" title="DINO: Self-Distillation with No Labels">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
		
		  <a href="mailto:ele.qiong@gmail.com" title="Email me">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://github.com/ictar" title="GitHub">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://linkedin.com/in/qiongjie-xu" title="LinkedIn">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="https://www.xuqiongjie.com">Qiongjie.X</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="http://localhost:1313/">Qiongjie&#39;s Notes</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.147.4</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="http://localhost:1313/js/main.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="http://localhost:1313/js/load-photoswipe.js"></script>










    
  </body>
</html>

