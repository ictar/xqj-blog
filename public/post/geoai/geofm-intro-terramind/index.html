

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Mastering TerraMind: From Understanding to Fine-tuning - </title>

  <meta name="description" content="TerraMind is the first large-scale, any-to-any generative multimodal foundation model proposed for the Earth Observation (EO) field. It is pre-trained by combining token-level and pixel-level dual-scale representations to learn high-level contextual information and fine-grained spatial details. The model aims to facilitate multimodal data integration, provide powerful generative capabilities, and support zero-shot and few-shot applications, while outperforming existing models on Earth Observation benchmarks and further improving performance by introducing &#39;Thinking in Modalities&#39; (TiM).">
  <meta name="author" content="Qiongjie.X"/>


<link rel="canonical" href="http://localhost:1313/post/geoai/geofm-intro-terramind/" />


<meta name="keywords" content="GeoFM, terramind, Remote Sensing, AI, Large Model, EO" /><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Qiongjie\u0027s Notes",
    
    "url": "http:\/\/localhost:1313\/"
}
</script>

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "http:\/\/localhost:1313\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "http:\/\/localhost:1313\/post\/geoai\/geofm-intro-terramind\/",
          "name": "Mastering terra mind from understanding to fine tuning"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "Qiongjie.X"
  },
  "headline": "Mastering TerraMind: From Understanding to Fine-tuning",
  "description" : "TerraMind is the first large-scale, any-to-any generative multimodal foundation model proposed for the Earth Observation (EO) field. It is pre-trained by combining token-level and pixel-level dual-scale representations to learn high-level contextual information and fine-grained spatial details. The model aims to facilitate multimodal data integration, provide powerful generative capabilities, and support zero-shot and few-shot applications, while outperforming existing models on Earth Observation benchmarks and further improving performance by introducing \u0027Thinking in Modalities\u0027 (TiM).",
  "inLanguage" : "en",
  "wordCount":  2211 ,
  "datePublished" : "2025-09-10T00:00:00\u002b00:00",
  "dateModified" : "2025-09-10T00:00:00\u002b00:00",
  "image" : "http:\/\/localhost:1313\/img\/avatar-icon.png",
  "keywords" : [ "GeoFM, terramind, Remote Sensing, AI, Large Model, EO" ],
  "mainEntityOfPage" : "http:\/\/localhost:1313\/post\/geoai\/geofm-intro-terramind\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "http:\/\/localhost:1313\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "http:\/\/localhost:1313\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>



<meta property="og:title" content="Mastering TerraMind: From Understanding to Fine-tuning" />
<meta property="og:description" content="TerraMind is the first large-scale, any-to-any generative multimodal foundation model proposed for the Earth Observation (EO) field. It is pre-trained by combining token-level and pixel-level dual-scale representations to learn high-level contextual information and fine-grained spatial details. The model aims to facilitate multimodal data integration, provide powerful generative capabilities, and support zero-shot and few-shot applications, while outperforming existing models on Earth Observation benchmarks and further improving performance by introducing &#39;Thinking in Modalities&#39; (TiM).">
<meta property="og:image" content="http://localhost:1313/img/avatar-icon.png" />
<meta property="og:url" content="http://localhost:1313/post/geoai/geofm-intro-terramind/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Qiongjie&#39;s Notes" />

  <meta name="twitter:title" content="Mastering TerraMind: From Understanding to Fine-tuning" />
  <meta name="twitter:description" content="TerraMind is the first large-scale, any-to-any generative multimodal foundation model proposed for the Earth Observation (EO) field. It is pre-trained by combining token-level and pixel-level â€¦">
  <meta name="twitter:image" content="http://localhost:1313/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='http://localhost:1313/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.147.4">
  <link rel="alternate" href="http://localhost:1313/index.xml" type="application/rss+xml" title="Qiongjie&#39;s Notes"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="http://localhost:1313/css/main.css" /><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="http://localhost:1313/css/syntax.css" /><link rel="stylesheet" href="http://localhost:1313/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">



<link
  rel="stylesheet"
  href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css"
  crossorigin="anonymous"
/>
<link rel="stylesheet" href="http://localhost:1313/css/custom.css"><link rel="stylesheet" href="http://localhost:1313/css/toc.css">

<script
  src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"
  crossorigin="anonymous"
></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>


<link rel="icon" type="image/png" href="http://localhost:1313/img/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/svg+xml" href="http://localhost:1313/img/favicon.svg" />
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313/img/apple-touch-icon.png" />
<link rel="manifest" href="http://localhost:1313/img/site.webmanifest" />

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://localhost:1313/">Qiongjie&#39;s Notes</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" role="button" tabindex="0" href="/post">BLOG</a>
              <div class="navlinks-children">
                
                  <a href="/post/python-geodata">Remote Sensing with Python</a>
                
                  <a href="/post/mcmc-statics">Monte Carloâ€“Markov Chains Statistical Methods</a>
                
                  <a href="/post/ai-fundamentals">AI Fundamentals</a>
                
              </div>
            </li>
          
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" role="button" tabindex="0" href="/projects">PROJECT</a>
              <div class="navlinks-children">
                
                  <a href="https://ictar.github.io/TerraFlow/">TerraFlow</a>
                
              </div>
            </li>
          
        
          
            <li>
              <a title="NOTE" href="/notes">NOTE</a>
            </li>
          
        
          
            <li>
              <a title="TAG" href="/tags">TAG</a>
            </li>
          
        
          
            <li>
              <a title="ABOUT ME" href="/page/about/">ABOUT ME</a>
            </li>
          
        

        
          
            <li>
              
                
                  <a href="http://localhost:1313/zh-cn/post/geoai/geofm-intro-terramind/">CH</a>
                
              
            </li>
          
        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Qiongjie&#39;s Notes" href="http://localhost:1313/">
            <img class="avatar-img" src="http://localhost:1313/img/avatar-icon.png" alt="Qiongjie&#39;s Notes" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>Mastering TerraMind: From Understanding to Fine-tuning</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on September 10, 2025
  
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;2211&nbsp;words
  
  
  &nbsp;&bull;&nbsp;Other languages: <a href="http://localhost:1313/zh-cn/post/geoai/geofm-intro-terramind/" lang="zh-cn">CH</a>
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row" style="display: flex; flex-wrap: wrap;">
    
    
    <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        
  <aside class="toc">
    <h2>ç›®å½•</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#why-terramind">Why TerraMind?</a></li>
    <li><a href="#where-does-terramind-come-from">Where Does TerraMind Come From?</a>
      <ul>
        <li><a href="#input-terramesh-dataset">Input: TerraMesh Dataset</a></li>
        <li><a href="#dual-scale-pretraining">Dual-Scale Pretraining</a></li>
        <li><a href="#two-stage-pretraining">Two-Stage Pretraining</a></li>
      </ul>
    </li>
    <li><a href="#what-can-terramind-do">What Can TerraMind Do?</a></li>
    <li><a href="#encoding-and-reconstruction-tokenizer">Encoding and Reconstruction (Tokenizer)</a></li>
    <li><a href="#cross-modal-generation-any-to-any">Cross-Modal Generation (Any-to-Any)</a>
      <ul>
        <li><a href="#single-patch-example">Single Patch Example</a></li>
        <li><a href="#tiled-inference">Tiled Inference</a></li>
      </ul>
    </li>
    <li><a href="#fine-tuning-tasks-fine-tuning-with-terratorch">Fine-tuning Tasks (Fine-tuning with TerraTorch)</a>
      <ul>
        <li><a href="#using-tim-in-fine-tuning">Using TiM in Fine-tuning</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#learn-more">Learn More</a></li>
  </ul>
</nav>
  </aside>

<p>Over the past few years, Earth Observation (EO) has been entering a &ldquo;model-centric&rdquo; era. The volume of satellite imagery is increasing, and the number of modalities (optical, radar, climate indices, geographical text descriptions&hellip;) is growing, but how to truly utilize this information remains a challenge.</p>
<p>Enter <strong>TerraMind</strong>. It is the <strong>first end-to-end generative, multimodal Earth Observation foundation model</strong>, jointly developed by IBM Research, ETH Zurich, Forschungszentrum JÃ¼lich, and ESA Î¦-Lab. It not only integrates data from different modalities but also possesses generative capabilities and excellent generalization. In other words, it attempts to play the role of a &ldquo;unified model,&rdquo; providing a powerful &ldquo;starting point&rdquo; for various downstream tasks (such as land cover classification, disaster monitoring, climate research, etc.).</p>
<h1 id="why-terramind">Why TerraMind?</h1>
<p>The proposal of TerraMind is primarily to <strong>solve problems in the EO field such as multimodal data integration, generation, and the lack of generalization capability in existing models</strong>. More specifically, it targets the following pain points:</p>
<ul>
<li>
<p><strong>Underutilization of Multimodal Data</strong>
Traditional models often focus only on specific modalities (like using only optical imagery) or serve only a specific task (like segmentation). TerraMind breaks down &ldquo;modality barriers&rdquo; by integrating radar, optical, land cover maps, NDVI, DEM, coordinate metadata, and even natural language descriptions.</p>
</li>
<li>
<p><strong>Lack of Generative Multimodal Capabilities</strong>
This is TerraMind&rsquo;s killer feature. It is the first &ldquo;large-scale, any-to-any&rdquo; generative multimodal EO model. It can not only perform traditional analysis tasks but also generate new data, supporting zero-shot and few-shot learning, thereby reducing labeling costs.</p>
</li>
<li>
<p><strong>Generalization and Data Efficiency Challenges</strong>
Past EO models easily &ldquo;overfit&rdquo; to a specific region or task, making migration difficult. TerraMind achieves stronger spatial and temporal generalization through self-supervised pre-training on massive unlabeled data, combined with small-scale supervised fine-tuning.</p>
</li>
<li>
<p><strong>Limitations of Existing Geospatial Foundation Models</strong>
Many GFMs (Geo-Foundation Models) essentially force-fit Computer Vision paradigms without considering the unique characteristics of remote sensing data. TerraMind has optimized its architecture, for example, with <strong>Dual-Scale Pre-training</strong> (token-level + pixel-level representations), which captures both global semantics and retains local spatial details.</p>
</li>
<li>
<p><strong>Accelerating Climate and Sustainability Applications</strong>
It is not just a model; it also comes with a distributed data processing and sampling framework that can directly connect to satellite data sources, enabling developers to deploy applications faster in climate and sustainability scenarios.</p>
</li>
</ul>
<p>In short, TerraMind&rsquo;s goal is: <strong>To be a general-purpose, powerful foundation model capable of integrating multimodal data, providing a more efficient &ldquo;starting point&rdquo; for EO research and applications.</strong></p>
<h1 id="where-does-terramind-come-from">Where Does TerraMind Come From?</h1>
<p><img src="https://huggingface.co/ibm-esa-geospatial/TerraMind-1.0-base/resolve/main/assets%2Fterramind_architecture.png" alt=""></p>
<h2 id="input-terramesh-dataset">Input: TerraMesh Dataset</h2>
<p>TerraMind is pre-trained on a custom-built, large-scale global geospatial dataset called <strong>TerraMesh</strong>, containing <strong>9 million samples and nine geospatial modalities</strong>:</p>
<ul>
<li>Optical Satellite Images: Copernicus Sentinel-2 L1C and L2A (RGB).</li>
<li>Radar Satellite Images: Copernicus Sentinel-1 GRD and RTC.</li>
<li>Task-Specific Modalities: Land Use/Land Cover (LULC) and Normalized Difference Vegetation Index (NDVI) maps.</li>
<li>Metadata: Digital Elevation Model (DEM) and geographical coordinates (discretized as strings).</li>
<li>Natural Language: Image captions synthesized from Sentinel-2 RGB images via LLaVA-Next.</li>
</ul>
<h2 id="dual-scale-pretraining">Dual-Scale Pretraining</h2>
<p>TerraMind combines data at both pixel level and token level, which is its core innovation.</p>
<ul>
<li><strong>Token Level</strong>: Encodes high-level contextual information, learns cross-modal relationships, and enables scalability.</li>
<li><strong>Pixel Level</strong>: Utilizes fine-grained representations to capture key spatial nuances.</li>
</ul>
<p>This dual-scale early fusion method outperforms other fusion methods, enabling artificial data generation, zero-shot, and few-shot applications.</p>
<h2 id="two-stage-pretraining">Two-Stage Pretraining</h2>
<ol>
<li><strong>Single-Modality Tokenizer Pretraining</strong>: A specific tokenizer is developed for each modality to encode data into discrete token sequences or decode from token sequences back to the original form. Image-based modalities (S-1, S-2, LULC, NDVI, DEM) use an <strong>architecture based on autoencoders and Finite Scalar Quantization (FSQ)</strong>. Sequence-based modalities (descriptions, geolocation) use WordPiece-based text tokenizers.</li>
<li><strong>TerraMind Encoder-Decoder Pretraining</strong>: Uses a symmetric Transformer architecture to process multimodal token sequences and accepts pixel-level cross-modal inputs. The pre-training objective is a cross-modal patch classification problem, reconstructing masked target tokens through cross-entropy loss.</li>
</ol>
<h1 id="what-can-terramind-do">What Can TerraMind Do?</h1>
<p>Its strengths can be summarized in three points:</p>
<ol>
<li><strong>Integration</strong>: Puts multiple modalities like radar, optical, DEM, LULC into a unified representation.</li>
<li><strong>Generation</strong>: Generative tasks between any modalities (e.g., synthesizing optical imagery from radar, or predicting NDVI from DEM).
<ul>
<li>Starting from Sentinel-2 L2A optical data, TerraMind can generate high-quality radar data, land use maps, and digital elevation maps.</li>
<li>Even starting from low-information geolocation info, the model can generate contextually relevant optical images (e.g., generating desert images from a Middle East location), although the structure might differ from the ground truth.</li>
</ul>
</li>
<li><strong>Generalization</strong>: Downstream tasks require only a small amount of labeling to adapt quickly, such as land cover classification, disaster monitoring, etc.
<ul>
<li><strong>Zero-Shot Learning</strong>
<ul>
<li><strong>Water Mapping</strong>: TerraMindv1-B achieves 45.4% IoU in a zero-shot setting. Using DynamicWorld LULC data for ablation experiments, it improves to 69.8%, approaching fine-tuned SOTA performance (84.4%).</li>
<li>Geolocation: Can accurately predict the geolocation of specific data instances, e.g., predicting the probability distribution for the &ldquo;Bareland&rdquo; category and identifying regions like the Sahara or the Middle East.</li>
</ul>
</li>
<li><strong>Few-Shot Learning</strong>: In 1-shot 5-way classification tasks on EuroSAT and METER-ML datasets, TerraMind&rsquo;s average accuracy is at least 10pp higher than other benchmarks, indicating its good latent spatial structure.</li>
</ul>
</li>
</ol>
<p>Furthermore, TerraMind introduces the innovative concept of <strong>&ldquo;Thinking in Modalities&rdquo; (TiM)</strong>, similar to &ldquo;chain-of-thought&rdquo; in large language models. By injecting generated artificial data during fine-tuning and inference, model output performance is improved. For example, in water mapping tasks, generating additional LULC data via TiM fine-tuning improved mIoU by 2pp compared to standard fine-tuning.</p>
<p><img src="https://research.ibm.com/_next/image?url=https%3A%2F%2Fresearch-website-prod-cms-uploads.s3.us.cloud-object-storage.appdomain.cloud%2FScreenshot_2025_10_20_at_3_49_35_PM_9f03a58e80.png&amp;w=1920&amp;q=85" alt=""></p>
<h1 id="encoding-and-reconstruction-tokenizer">Encoding and Reconstruction (Tokenizer)</h1>
<p>First, one must understand TerraMind&rsquo;s <strong>Tokenizer</strong>. Its working principle is similar to &ldquo;compressionâ€”decompression&rdquo;:</p>
<ul>
<li>Input Image â†’ Convert to token representation</li>
<li>Tokens capture spatial details and contextual information</li>
<li>Reconstruct image via decoder â†’ Verify if the model &ldquo;understood&rdquo; the input</li>
</ul>
<p>This part mainly <strong>verifies the model&rsquo;s information retention capability</strong>.</p>
<p>Below is an actual Python script (based on Sentinel-2 L2A data), referencing <a href="https://github.com/IBM/terramind/blob/main/notebooks/terramind_tokenizer_reconstruction.ipynb">IBM/terramind/notebooks/terramind_tokenizer_reconstruction.ipynb</a>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Build tokenizer model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> FULL_MODEL_REGISTRY<span style="color:#f92672">.</span>build(<span style="color:#e6db74">&#39;terramind_v1_tokenizer_s2l2a&#39;</span>, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load example image and construct tensor</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load an example Sentinel-2 image</span>
</span></span><span style="display:flex;"><span>examples <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;../examples/S2L2A/38D_378R_2_3.tif&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;../examples/S2L2A/282D_485L_3_3.tif&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;../examples/S2L2A/433D_629L_3_1.tif&#39;</span>,
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> rxr<span style="color:#f92672">.</span>open_rasterio(examples[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convert to [B, C, 224, 224]</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(data<span style="color:#f92672">.</span>values, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Normalization (Standardization)</span>
</span></span><span style="display:flex;"><span>mean <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(v1_pretraining_mean[<span style="color:#e6db74">&#39;untok_sen2l2a@224&#39;</span>])
</span></span><span style="display:flex;"><span>std <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(v1_pretraining_std[<span style="color:#e6db74">&#39;untok_sen2l2a@224&#39;</span>])
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> (data <span style="color:#f92672">-</span> mean[<span style="color:#66d9ef">None</span>, :, <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>]) <span style="color:#f92672">/</span> std[<span style="color:#66d9ef">None</span>, :, <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run model (Encode -&gt; Decode)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    reconstruction <span style="color:#f92672">=</span> model(input, timesteps<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># De-normalization (Restore to original scale)</span>
</span></span><span style="display:flex;"><span>reconstruction <span style="color:#f92672">=</span> reconstruction<span style="color:#f92672">.</span>cpu()
</span></span><span style="display:flex;"><span>reconstruction <span style="color:#f92672">=</span> (reconstruction <span style="color:#f92672">*</span> std[<span style="color:#66d9ef">None</span>, :, <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>]) <span style="color:#f92672">+</span> mean[<span style="color:#66d9ef">None</span>, :, <span style="color:#66d9ef">None</span>, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The decoded reconstruction needs to be multiplied by std and added to mean to bring the values back to the &#34;real&#34; scale (same range as original data), otherwise it will look like &#34;small numbers with mean 0&#34;.</span>
</span></span></code></pre></div><p>When visualizing, you can compare the RGB representation of the input image and the reconstructed image:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>fig, ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">5</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input RGB</span>
</span></span><span style="display:flex;"><span>rgb_input <span style="color:#f92672">=</span> data[<span style="color:#ae81ff">0</span>, [<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>]]<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>imshow((rgb_input<span style="color:#f92672">/</span><span style="color:#ae81ff">2000</span>)<span style="color:#f92672">.</span>clip(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Input&#34;</span>); ax[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reconstructed RGB</span>
</span></span><span style="display:flex;"><span>rgb_recon <span style="color:#f92672">=</span> reconstruction[<span style="color:#ae81ff">0</span>, [<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">1</span>]]<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>imshow((rgb_recon<span style="color:#f92672">/</span><span style="color:#ae81ff">2000</span>)<span style="color:#f92672">.</span>clip(<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>set_title(<span style="color:#e6db74">&#34;Reconstruction&#34;</span>); ax[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#34;off&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>Result: The left is the original Sentinel-2 input, and the right is the reconstruction result after Tokenizer decoding.</p>
<p>ðŸ‘‰ This proves that <strong>TerraMind can represent images using tokens and retain information very well</strong>.</p>
<h1 id="cross-modal-generation-any-to-any">Cross-Modal Generation (Any-to-Any)</h1>
<p>What&rsquo;s exciting is TerraMind&rsquo;s <strong>generative capability</strong>.
It can not only &ldquo;reconstruct&rdquo; input but also achieve <strong>Cross-Modal Generation (Any-to-Any)</strong>:</p>
<ul>
<li>Optical Image â†’ Generate Radar Image</li>
<li>DEM â†’ Generate Optical or NDVI</li>
<li>Image â†’ Generate LULC Map</li>
<li>It can even do Image â†” Text</li>
</ul>
<p>This <strong>any-modality-to-any-modality generation capability</strong> is TerraMind&rsquo;s biggest breakthrough compared to traditional models.
It means: even if some data is missing, it can be filled in through generation; even if labeling is limited, training can be augmented through generation.</p>
<h2 id="single-patch-example">Single Patch Example</h2>
<ul>
<li>Input: Single patch, can be S2-L2A data (224Ã—224)</li>
<li>Output: Multiple target modalities, such as S1GRD, DEM, LULC, etc.</li>
<li>Features:
<ul>
<li>Each output modality corresponds to its own tokenizer (increases memory consumption).</li>
<li>Uses diffusion steps (e.g., 10 steps) to generate results â†’ ensures diversity and quality of output.</li>
<li>Can standardize input directly (<code>standardize=True</code>).</li>
</ul>
</li>
<li>Steps:
<ol>
<li>Build Model
<ul>
<li>Instantiate TerraMind model from <code>FULL_MODEL_REGISTRY</code>.</li>
<li>Specify <strong>Input Modalities</strong> (e.g., <code>S2L2A</code>) and <strong>Output Modalities</strong> (e.g., <code>S1GRD, DEM, LULC</code>).</li>
<li>Select whether to load pre-trained weights (<code>pretrained=True</code>).</li>
<li>Configure standardization (<code>standardize=True</code> applies pre-training mean/variance automatically).</li>
</ul>
</li>
<li>Prepare Data
<ul>
<li>Load raster data</li>
<li>Use <code>rioxarray.open_rasterio</code> to read as array.</li>
<li>Convert to input shape required by model [Batch, Channels, Height, Width], e.g., [1, C, 224, 224].</li>
</ul>
</li>
<li>Load Input Data: Plot input data as RGB image (helps understand input content).</li>
<li>Execute Diffusion Generation
<ul>
<li>Send input to GPU/CPU (<code>input.to(device)</code>).</li>
<li>Run diffusion generation under <code>torch.no_grad()</code> (<code>model(input, timesteps=10)</code>)</li>
<li><code>timesteps</code> controls the number of iterations in the diffusion process (more steps mean finer generation but take longer).</li>
</ul>
</li>
<li>Get Multimodal Output
<ul>
<li>Model returns a dictionary (<code>{modality_name: generation_result}</code>)</li>
<li>Each output modality has a corresponding <strong>generation tensor</strong>.</li>
</ul>
</li>
<li>Visualize Results.
<ul>
<li>Iterate through output modalities, call <code>plot_modality()</code> to plot each result.</li>
<li>Display side-by-side with input image for comparison.</li>
</ul>
</li>
</ol>
</li>
</ul>
<p>The following Python snippet implements generating other modalities (S-1, DEM, LULC&hellip;) using S-2 input, referencing <a href="https://github.com/IBM/terramind/blob/main/notebooks/terramind_generation.ipynb">IBM/terramind/notebooks/terramind_generation.ipynb</a>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Load example S-2 L2A and construct model input</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> rxr<span style="color:#f92672">.</span>open_rasterio(examples[example_id])          <span style="color:#75715e"># rioxarray DataArray</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>Tensor(data<span style="color:#f92672">.</span>values, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># -&gt; [B, C, H, W]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> FULL_MODEL_REGISTRY<span style="color:#f92672">.</span>build(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;terramind_v1_base_generate&#39;</span>,
</span></span><span style="display:flex;"><span>    modalities<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;S2L2A&#39;</span>],
</span></span><span style="display:flex;"><span>    output_modalities<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;S1GRD&#39;</span>, <span style="color:#e6db74">&#39;DEM&#39;</span>, <span style="color:#e6db74">&#39;LULC&#39;</span>],
</span></span><span style="display:flex;"><span>    pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    standardize<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Move to device</span>
</span></span><span style="display:flex;"><span>_ <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run generation (diffusion steps)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>  generated <span style="color:#f92672">=</span> model(input, verbose<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, timesteps<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><p>Essentially, this is cross-modal translation: <strong>Give you a Sentinel-2 patch, I can &ldquo;imagine&rdquo; other modalities like SAR, DEM, NDVI, etc.</strong></p>
<h2 id="tiled-inference">Tiled Inference</h2>
<p><strong>Why use tiled_inference?</strong></p>
<p>Large tiles (full tile) usually exceed GPU memory, so big images are cut into many small tiles (patches) to perform inference separately on the GPU, and then merged back into the full image. <code>tiled_inference</code> is a utility function (provided by TerraTorch/user library) responsible for cutting patches, calling the model in batches, and merging patch outputs into a full output.</p>
<p>In summary:</p>
<ul>
<li>Input: Full tile (e.g., Singapore, Santiago, could be 1000Ã—1000 or larger).</li>
<li>Problem: GPU memory isn&rsquo;t enough to hold the whole image.</li>
<li>Steps:
<ol>
<li>Load and (optionally) crop large tile</li>
<li>Convert to [B,C,H,W]</li>
<li>Build generation model (specify output modalities, timesteps)</li>
<li>Use <code>tiled_inference</code> to cut patches and call <code>model_forward</code> in batches</li>
<li>Merge back to full image to get [C_total,H,W]</li>
<li>Split channels for each modality</li>
<li>De-standardize continuous modalities, argmax classification modalities</li>
<li>Save GeoTIFF / Visualize / Evaluate metrics.</li>
</ol>
</li>
</ul>
<p>Below is an actual Python script, referencing <a href="https://github.com/IBM/terramind/blob/main/notebooks/large_tile_generation.ipynb">IBM/terramind/notebooks/large_tile_generation.ipynb</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Input preparation (crop / batch dim)</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> rxr<span style="color:#f92672">.</span>open_rasterio(<span style="color:#e6db74">&#39;examples/S2L2A/Santiago.tif&#39;</span>)<span style="color:#f92672">.</span>values
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">500</span>:<span style="color:#ae81ff">1500</span>]           <span style="color:#75715e"># Optional crop for speed</span>
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(data, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float, device<span style="color:#f92672">=</span>device)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build and prepare generation model: condition on S2L2A, generate S1GRD and LULC</span>
</span></span><span style="display:flex;"><span>output_modalities <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;S1GRD&#39;</span>, <span style="color:#e6db74">&#39;LULC&#39;</span>]
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> FULL_MODEL_REGISTRY<span style="color:#f92672">.</span>build(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;terramind_v1_base_generate&#39;</span>,
</span></span><span style="display:flex;"><span>    modalities<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;S2L2A&#39;</span>],
</span></span><span style="display:flex;"><span>    output_modalities<span style="color:#f92672">=</span>output_modalities,
</span></span><span style="display:flex;"><span>    pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    standardize<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    timesteps<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># From dict -&gt; tensor</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">model_forward</span>(x):
</span></span><span style="display:flex;"><span>    generated <span style="color:#f92672">=</span> model(x)                       <span style="color:#75715e"># dict: {modality: tensor}</span>
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>concat([generated[m] <span style="color:#66d9ef">for</span> m <span style="color:#f92672">in</span> output_modalities], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output of tiled_inference and remove batch dim</span>
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> tiled_inference(model_forward, input, crop<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">192</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, verbose<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>pred <span style="color:#f92672">=</span> pred<span style="color:#f92672">.</span>squeeze(<span style="color:#ae81ff">0</span>)   <span style="color:#75715e"># From [1, C, H, W] -&gt; [C, H, W]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split concatenated channels back to modalities</span>
</span></span><span style="display:flex;"><span>num_channels <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;S2L2A&#39;</span>:<span style="color:#ae81ff">12</span>, <span style="color:#e6db74">&#39;S1GRD&#39;</span>:<span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#39;S1RTC&#39;</span>:<span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#39;DEM&#39;</span>:<span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#39;LULC&#39;</span>:<span style="color:#ae81ff">10</span>, <span style="color:#e6db74">&#39;NDVI&#39;</span>:<span style="color:#ae81ff">1</span>}
</span></span><span style="display:flex;"><span>num_channels <span style="color:#f92672">=</span> {m: num_channels[m] <span style="color:#66d9ef">for</span> m <span style="color:#f92672">in</span> output_modalities}
</span></span><span style="display:flex;"><span>start_idx <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>cumsum([<span style="color:#ae81ff">0</span>] <span style="color:#f92672">+</span> list(num_channels<span style="color:#f92672">.</span>values()))
</span></span><span style="display:flex;"><span>generated <span style="color:#f92672">=</span> {m: pred[i:i<span style="color:#f92672">+</span>c]<span style="color:#f92672">.</span>cpu() <span style="color:#66d9ef">for</span> m, i, c <span style="color:#f92672">in</span> zip(output_modalities, start_idx, num_channels<span style="color:#f92672">.</span>values())}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Post-processing for LULC (from probability -&gt; discrete category)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;LULC&#39;</span> <span style="color:#f92672">in</span> generated<span style="color:#f92672">.</span>keys():
</span></span><span style="display:flex;"><span>    generated[<span style="color:#e6db74">&#39;LULC&#39;</span>] <span style="color:#f92672">=</span> generated[<span style="color:#e6db74">&#39;LULC&#39;</span>]<span style="color:#f92672">.</span>argmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span></code></pre></div><p>Thus, enabling TerraMind to be not just a small patch demo, but applicable to real-world large-scale Earth scenarios.</p>
<h1 id="fine-tuning-tasks-fine-tuning-with-terratorch">Fine-tuning Tasks (Fine-tuning with TerraTorch)</h1>
<p>Another highlight of TerraMind is its convenience for fine-tuning on downstream tasks (like semantic segmentation). We use <a href="https://github.com/IBM/terramind/blob/main/notebooks/terramind_v1_base_sen1floods11.ipynb">IBM/terramind/notebooks/terramind_v1_base_sen1floods11.ipynb</a> as an example to explain the main flow of fine-tuning:</p>
<ol>
<li>
<p><strong>Data Preparation (DataModule)</strong>
Use <code>GenericMultiModalDataModule</code> to define modalities, paths, split files, standardization parameters. For example:</p>
<ul>
<li>Modalities: <code>[&quot;S2L1C&quot;, &quot;S1GRD&quot;]</code></li>
<li>Labels: <code>*_LabelHand.tif</code></li>
<li>Train/Val/Test Splits: <code>flood_train_data.txt</code>, etc.</li>
</ul>
<p>The benefit of this is that whether you use Optical+Radar, or add DEM, LULC, you can configure it quickly.</p>
</li>
<li>
<p><strong>Load Pre-trained Backbone</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> BACKBONE_REGISTRY<span style="color:#f92672">.</span>build(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;terramind_v1_base&#34;</span>,
</span></span><span style="display:flex;"><span>    modalities<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;S2L1C&#34;</span>, <span style="color:#e6db74">&#34;S1GRD&#34;</span>],
</span></span><span style="display:flex;"><span>    pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div></li>
<li>
<p><strong>Define Downstream Task (SemanticSegmentationTask)</strong></p>
<ul>
<li>Backbone: TerraMind</li>
<li>Neck: Feature extraction and reshape</li>
<li>Decoder: UNetDecoder</li>
<li>Loss: Dice or CE</li>
</ul>
<p>Also supports freezing/unfreezing backbone, adjusting learning rate (1e-5 ~ 1e-4).</p>
</li>
<li>
<p><strong>Training and Testing</strong>
Start training via PyTorch Lightning&rsquo;s <code>Trainer.fit()</code>, save the best checkpoint.
Then evaluate on <code>Trainer.test()</code>, and finally perform prediction and visualization.</p>
</li>
</ol>
<p>Running through the entire fine-tuning script allows you to get a &ldquo;task-adapted model&rdquo; from a &ldquo;pre-trained foundation model,&rdquo; seeing results even after training for just a few epochs.</p>
<h2 id="using-tim-in-fine-tuning">Using TiM in Fine-tuning</h2>
<p>To use TiM in fine-tuning, simply use a backbone ending with <code>_tim</code> and specify the modalities to use via <code>backbone_tim_modalities</code>. For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">backbone</span>: <span style="color:#ae81ff">terramind_v1_base_tim     </span> <span style="color:#75715e"># Instead of terramind_v1_base</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">backbone_tim_modalities</span>: [<span style="color:#ae81ff">LULC]     </span> <span style="color:#75715e"># or S1GRD / NDVI / DEM / â€¦</span>
</span></span></code></pre></div><h1 id="summary">Summary</h1>
<ul>
<li><strong>What is TerraMind?</strong> A multimodal foundation model for Earth Observation.</li>
<li><strong>What can it do?</strong> Multimodal integration, generative tasks, downstream generalization.</li>
<li><strong>How to play?</strong>
<ul>
<li>Experience generative capability first: Input Image â†’ Tokens â†’ Reconstruct Image.</li>
<li>Then do downstream fine-tuning: Configure DataModule, load pre-trained model, define task, train/test.</li>
</ul>
</li>
</ul>
<p>It is not just a model, but more like a <strong>General-Purpose AI Platform for EO</strong>. In the future, whether it&rsquo;s climate research, disaster response, or land cover monitoring, it can help achieve rapid deployment.</p>
<h1 id="learn-more">Learn More</h1>
<ul>
<li><a href="https://github.com/IBM/terramind/tree/main">Github: IBM/terramind</a></li>
<li><a href="https://huggingface.co/ibm-esa-geospatial/TerraMind-1.0-base">Hugging Face: ibm-esa-geospatial/TerraMind-1.0-base</a></li>
<li><a href="https://doi.org/10.48550/arXiv.2504.11171">Paper (arxiv): TerraMind: Large-Scale Generative Multimodality for Earth Observation</a></li>
<li><a href="https://research.ibm.com/blog/thinking-in-modalities-terramind">Introducing Thinking-in-Modalities with TerraMind</a></li>
</ul>


        
          <div class="blog-tags">
            
              <a href="http://localhost:1313/tags/geofm/">GeoFM</a>&nbsp;
            
              <a href="http://localhost:1313/tags/terramind/">terramind</a>&nbsp;
            
              <a href="http://localhost:1313/tags/remote-sensing/">Remote Sensing</a>&nbsp;
            
              <a href="http://localhost:1313/tags/ai/">AI</a>&nbsp;
            
              <a href="http://localhost:1313/tags/large-model/">Large Model</a>&nbsp;
            
              <a href="http://localhost:1313/tags/eo/">EO</a>&nbsp;
            
          </div>
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
    <ul class="share">
      
      <li>
        <a href="//twitter.com/share?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fgeoai%2fgeofm-intro-terramind%2f&amp;text=Mastering%20TerraMind%3a%20From%20Understanding%20to%20Fine-tuning&amp;via=" target="_blank" title="Share on Twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fpost%2fgeoai%2fgeofm-intro-terramind%2f" target="_blank" title="Share on Facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fgeoai%2fgeofm-intro-terramind%2f&amp;title=Mastering%20TerraMind%3a%20From%20Understanding%20to%20Fine-tuning" target="_blank" title="Share on Reddit">
          <i class="fab fa-reddit"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.linkedin.com/shareArticle?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fgeoai%2fgeofm-intro-terramind%2f&amp;title=Mastering%20TerraMind%3a%20From%20Understanding%20to%20Fine-tuning" target="_blank" title="Share on LinkedIn">
          <i class="fab fa-linkedin"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.stumbleupon.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fgeoai%2fgeofm-intro-terramind%2f&amp;title=Mastering%20TerraMind%3a%20From%20Understanding%20to%20Fine-tuning" target="_blank" title="Share on StumbleUpon">
          <i class="fab fa-stumbleupon"></i>
        </a>
      </li>
  
      
      <li>
        <a href="//www.pinterest.com/pin/create/button/?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fgeoai%2fgeofm-intro-terramind%2f&amp;description=Mastering%20TerraMind%3a%20From%20Understanding%20to%20Fine-tuning" target="_blank" title="Share on Pinterest">
          <i class="fab fa-pinterest"></i>
        </a>
      </li>
    </ul>
  </div>
  

              </div>
            </section>
        

        
          

          
        
      </article>

      
        <ul class="pager blog-pager">
          
          
        </ul>
      


      

    </div>
    
    
    <div class="col-lg-3 visible-lg-block">
      
      
      <div class="sidebar-toc">
        <h2 class="sidebar-toc-title">ç›®å½•</h2>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#why-terramind">Why TerraMind?</a></li>
    <li><a href="#where-does-terramind-come-from">Where Does TerraMind Come From?</a>
      <ul>
        <li><a href="#input-terramesh-dataset">Input: TerraMesh Dataset</a></li>
        <li><a href="#dual-scale-pretraining">Dual-Scale Pretraining</a></li>
        <li><a href="#two-stage-pretraining">Two-Stage Pretraining</a></li>
      </ul>
    </li>
    <li><a href="#what-can-terramind-do">What Can TerraMind Do?</a></li>
    <li><a href="#encoding-and-reconstruction-tokenizer">Encoding and Reconstruction (Tokenizer)</a></li>
    <li><a href="#cross-modal-generation-any-to-any">Cross-Modal Generation (Any-to-Any)</a>
      <ul>
        <li><a href="#single-patch-example">Single Patch Example</a></li>
        <li><a href="#tiled-inference">Tiled Inference</a></li>
      </ul>
    </li>
    <li><a href="#fine-tuning-tasks-fine-tuning-with-terratorch">Fine-tuning Tasks (Fine-tuning with TerraTorch)</a>
      <ul>
        <li><a href="#using-tim-in-fine-tuning">Using TiM in Fine-tuning</a></li>
      </ul>
    </li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#learn-more">Learn More</a></li>
  </ul>
</nav>
      </div>
      
    </div>
    
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
		
		  <a href="mailto:ele.qiong@gmail.com" title="Email me">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://github.com/ictar" title="GitHub">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://linkedin.com/in/qiongjie-xu" title="LinkedIn">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="https://www.xuqiongjie.com">Qiongjie.X</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2025
          

          
            &nbsp;&bull;&nbsp;
            <a href="http://localhost:1313/">Qiongjie&#39;s Notes</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.147.4</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="http://localhost:1313/js/main.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="http://localhost:1313/js/load-photoswipe.js"></script>









<script src="http://localhost:1313/js/toc-enhancements.js"></script>


    
  </body>
</html>

