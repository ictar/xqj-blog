

<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

 


      <title>Introduction to MCMC - </title>

  <meta name="description" content="The reason we need MCMC is that many distributions are only known in their unnormalized form, making traditional sampling/integration methods ineffective. By constructing a &#39;correct Markov chain&#39;, we can obtain the target distribution from its stationary distribution, meaning the long-term distribution of the trajectory â‰ˆ target distribution.">
  <meta name="author" content="Qiongjie.X"/>


<link rel="canonical" href="http://localhost:1313/post/mcmc-statics/intro-mcmc/" />


<meta name="keywords" content="Monte Carlo, Markov Chain, Sampling, Mathematics, python" /><script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "WebSite",
    "name": "Qiongjie\u0027s Notes",
    
    "url": "http:\/\/localhost:1313\/"
}
</script>

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "item": {
          "@id": "http:\/\/localhost:1313\/",
          "name": "home"
        }
    },{
        "@type": "ListItem",
        "position": 3,
        "item": {
          "@id": "http:\/\/localhost:1313\/post\/mcmc-statics\/intro-mcmc\/",
          "name": "Introduction to MC MC"
        }
    }]
}
</script><script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "author": {
    "name" : "Qiongjie.X"
  },
  "headline": "Introduction to MCMC",
  "description" : "The reason we need MCMC is that many distributions are only known in their unnormalized form, making traditional sampling\/integration methods ineffective. By constructing a \u0027correct Markov chain\u0027, we can obtain the target distribution from its stationary distribution, meaning the long-term distribution of the trajectory â‰ˆ target distribution.",
  "inLanguage" : "en",
  "wordCount":  1757 ,
  "datePublished" : "2025-08-22T00:00:00\u002b00:00",
  "dateModified" : "2025-08-22T00:00:00\u002b00:00",
  "image" : "http:\/\/localhost:1313\/img\/avatar-icon.png",
  "keywords" : [ "Monte Carlo, Markov Chain, Sampling, Mathematics, python" ],
  "mainEntityOfPage" : "http:\/\/localhost:1313\/post\/mcmc-statics\/intro-mcmc\/",
  "publisher" : {
    "@type": "Organization",
    "name" : "http:\/\/localhost:1313\/",
    "logo" : {
        "@type" : "ImageObject",
        "url" : "http:\/\/localhost:1313\/img\/avatar-icon.png",
        "height" :  60 ,
        "width" :  60
    }
  }
}
</script>



<meta property="og:title" content="Introduction to MCMC" />
<meta property="og:description" content="The reason we need MCMC is that many distributions are only known in their unnormalized form, making traditional sampling/integration methods ineffective. By constructing a &#39;correct Markov chain&#39;, we can obtain the target distribution from its stationary distribution, meaning the long-term distribution of the trajectory â‰ˆ target distribution.">
<meta property="og:image" content="http://localhost:1313/img/avatar-icon.png" />
<meta property="og:url" content="http://localhost:1313/post/mcmc-statics/intro-mcmc/" />
<meta property="og:type" content="website" />
<meta property="og:site_name" content="Qiongjie&#39;s Notes" />

  <meta name="twitter:title" content="Introduction to MCMC" />
  <meta name="twitter:description" content="The reason we need MCMC is that many distributions are only known in their unnormalized form, making traditional sampling/integration methods ineffective. By constructing a &#39;correct Markov chain&#39;, we â€¦">
  <meta name="twitter:image" content="http://localhost:1313/img/avatar-icon.png" />
  <meta name="twitter:card" content="summary_large_image" />
  <link href='http://localhost:1313/favicon.ico' rel='icon' type='image/x-icon'/>
  <meta name="generator" content="Hugo 0.147.4">
  <link rel="alternate" href="http://localhost:1313/index.xml" type="application/rss+xml" title="Qiongjie&#39;s Notes"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.6.0/css/all.css" integrity="sha384-h/hnnw1Bi4nbpD6kE7nYfCXzovi622sY5WBxww8ARKwpdLj5kUWjRuyiXaD1U2JT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous"><link rel="stylesheet" href="http://localhost:1313/css/main.css" /><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" />
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" /><link rel="stylesheet" href="http://localhost:1313/css/syntax.css" /><link rel="stylesheet" href="http://localhost:1313/css/codeblock.css" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.css" integrity="sha384-h/L2W9KefUClHWaty3SLE5F/qvc4djlyR4qY3NUV5HGQBBW7stbcfff1+I/vmsHh" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/default-skin/default-skin.min.css" integrity="sha384-iD0dNku6PYSIQLyfTOpB06F2KCZJAKLOThS5HRe8b3ibhdEQ6eKsFf/EeFxdOt5R" crossorigin="anonymous">



<link
  rel="stylesheet"
  href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css"
  crossorigin="anonymous"
/>
<link rel="stylesheet" href="http://localhost:1313/css/custom.css"><link rel="stylesheet" href="http://localhost:1313/css/toc.css">

<script
  src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"
  crossorigin="anonymous"
></script>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>


<link rel="icon" type="image/png" href="http://localhost:1313/img/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/svg+xml" href="http://localhost:1313/img/favicon.svg" />
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313/img/apple-touch-icon.png" />
<link rel="manifest" href="http://localhost:1313/img/site.webmanifest" />

  </head>
  <body>
    <nav class="navbar navbar-default navbar-fixed-top navbar-custom">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#main-navbar">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="http://localhost:1313/">Qiongjie&#39;s Notes</a>
    </div>

    <div class="collapse navbar-collapse" id="main-navbar">
      <ul class="nav navbar-nav navbar-right">
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" role="button" tabindex="0" href="/post">BLOG</a>
              <div class="navlinks-children">
                
                  <a href="/post/python-geodata">Remote Sensing with Python</a>
                
                  <a href="/post/mcmc-statics">Monte Carloâ€“Markov Chains Statistical Methods</a>
                
                  <a href="/post/ai-fundamentals">AI Fundamentals</a>
                
              </div>
            </li>
          
        
          
            <li class="navlinks-container">
              <a class="navlinks-parent" role="button" tabindex="0" href="/projects">PROJECT</a>
              <div class="navlinks-children">
                
                  <a href="https://ictar.github.io/TerraFlow/">TerraFlow</a>
                
              </div>
            </li>
          
        
          
            <li>
              <a title="NOTE" href="/notes">NOTE</a>
            </li>
          
        
          
            <li>
              <a title="TAG" href="/tags">TAG</a>
            </li>
          
        
          
            <li>
              <a title="ABOUT ME" href="/page/about/">ABOUT ME</a>
            </li>
          
        

        
          
            <li>
              
                
                  <a href="http://localhost:1313/zh-cn/post/mcmc-statics/intro-mcmc/">CH</a>
                
              
            </li>
          
        

        
      </ul>
    </div>

    
      <div class="avatar-container">
        <div class="avatar-img-border">
          <a title="Qiongjie&#39;s Notes" href="http://localhost:1313/">
            <img class="avatar-img" src="http://localhost:1313/img/avatar-icon.png" alt="Qiongjie&#39;s Notes" />
           
          </a>
        </div>
      </div>
    

  </div>
</nav>




    


<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>


  
  
  






  

  <header class="header-section ">
    
    
    <div class="intro-header no-img">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="post-heading">
              
                <h1>Introduction to MCMC</h1>
              
              
              
              
                <span class="post-meta">
  
  
  <i class="fas fa-calendar"></i>&nbsp;Posted on August 22, 2025
  
  
  
    &nbsp;|&nbsp;<i class="fas fa-book"></i>&nbsp;1757&nbsp;words
  
  
  &nbsp;&bull;&nbsp;Other languages: <a href="http://localhost:1313/zh-cn/post/mcmc-statics/intro-mcmc/" lang="zh-cn">CH</a>
</span>


              
            </div>
          </div>
        </div>
      </div>
    </div>
  
  </header>


    
<div class="container" role="main">
  <div class="row" style="display: flex; flex-wrap: wrap;">
    
    
    <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1">
      <article role="main" class="blog-post">
        
  <aside class="toc">
    <h2>ç›®å½•</h2>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#why-do-we-need-mcmc">Why Do We Need MCMC?</a>
      <ul>
        <li><a href="#example">Example</a></li>
      </ul>
    </li>
    <li><a href="#from-markov-chain-to-sampling">From Markov Chain to Sampling</a>
      <ul>
        <li><a href="#turning-sampling-into-building-a-chain">Turning Sampling into &ldquo;Building a Chain&rdquo;</a></li>
        <li><a href="#ensuring-reachable-aperiodic-forgetting-start">Ensuring &ldquo;Reachable, Aperiodic, Forgetting Start&rdquo;</a></li>
        <li><a href="#why-unnormalized-is-fine">Why &ldquo;Unnormalized is Fine&rdquo;</a></li>
        <li><a href="#example-1">Example</a></li>
      </ul>
    </li>
    <li><a href="#theory-and-intuition">Theory and Intuition</a>
      <ul>
        <li><a href="#detailed-balance-reversibility-and-stationarity">Detailed Balance (Reversibility) and Stationarity</a></li>
        <li><a href="#convergence-ergodic-theorem-lln-clt">Convergence: Ergodic Theorem, LLN, CLT</a></li>
        <li><a href="#mixing-time-spectral-gap-and-geometric-convergence">Mixing Time, Spectral Gap, and Geometric Convergence</a></li>
        <li><a href="#burn-in-thinning-diagnostics-from-theory-to-practice">Burn-in, Thinning, Diagnostics (From Theory to Practice)</a></li>
        <li><a href="#why-mcmc-can-walk-and-stay-in-high-density-areas">Why MCMC can &ldquo;Walk and Stay in High Density Areas&rdquo;</a></li>
        <li><a href="#example-2">Example</a></li>
      </ul>
    </li>
    <li><a href="#critical-indicators-in-practice-no-algorithms-yet">Critical Indicators in Practice (No Algorithms Yet)</a></li>
    <li><a href="#summary">Summary</a></li>
  </ul>
</nav>
  </aside>

<h1 id="why-do-we-need-mcmc">Why Do We Need MCMC?</h1>
<blockquote>
<p>In short, because many distributions are only known in unnormalized forms, making traditional sampling/integration methods ineffective.</p></blockquote>
<p><strong>Goal</strong>: Sample from a complex distribution $\pi(x)$ (often only known as &ldquo;<strong>unnormalized</strong>&rdquo; $\tilde\pi(x)\propto \pi(x)$), or compute expectation/marginal:</p>
$$
\mathbb{E}_\pi[f(X)] \;=\; \int f(x)\,\pi(x)\,dx.
$$<p><strong>Reality Difficulties</strong>:</p>
<ul>
<li><strong>High Dimensionality</strong>: As dimensions increase, grid/numerical integration explodes exponentially;</li>
<li><strong>Unknown Normalization Constant</strong>: In Bayesian context, the denominator $p(y)=\int p(y\mid \theta)p(\theta)\,d\theta$ in posterior $\pi(\theta\mid y)\propto p(y\mid \theta)p(\theta)$ is often intractable;</li>
<li><strong>Multi-modal/Strong Correlation</strong>: Variance of Rejection Sampling or Importance Sampling becomes huge or suffers from &ldquo;weight degeneracy&rdquo;.</li>
</ul>
<p><strong>Core Idea of Monte Carlo</strong>: Given samples $x^{(1)},\dots,x^{(T)}\sim \pi$, we can use sample mean</p>
$$
\frac{1}{T}\sum_{t=1}^T f\!\big(x^{(t)}\big)
$$<p>to approximate $\mathbb{E}_\pi[f(X)]$. The problem is <strong>how to sample from $\pi$</strong>? This is exactly what <strong>MCMC</strong> solves: <strong>Without requiring the normalization constant, just being able to compute $\tilde\pi(x)$ (or its log)</strong>, we can construct a stochastic process that &ldquo;stays at $\pi$ in the long run&rdquo; to draw samples.</p>
<p><strong>Comparison with Other Approaches</strong> (Intuition):</p>
<ul>
<li><strong>Variational Inference (VI)</strong>: Fast, scalable, but approximates with a &ldquo;tractable family&rdquo;, introducing <strong>approximation bias</strong>;</li>
<li><strong>SMC/Particle Methods</strong>: Suitable for sequential problems, but design and resampling/annealing are complex;</li>
<li><strong>MCMC</strong>: <strong>Asymptotically Unbiased</strong> (can arbitrarily approximate $\pi$ if run long enough), but samples are <strong>correlated</strong>, computation can be expensive, and requires diagnostics/tuning.</li>
</ul>
<h2 id="example">Example</h2>
<p>Suppose we want to sample from the following distribution:</p>
$$
\pi(x) \propto e^{-x^4}, \quad x \in \mathbb{R}.
$$<ul>
<li>This is a &ldquo;super sharp&rdquo; unimodal distribution.</li>
<li>Normalization constant $Z=\int e^{-x^4}\,dx$ is unknown (cannot be solved by hand).</li>
<li>Want to compute expectation $\mathbb{E}[X^2]$.</li>
</ul>
<p>ðŸ‘‰ <strong>Problem</strong>:</p>
<ul>
<li>Direct integration is impossible (analytically intractable).</li>
<li>Rejection Sampling needs a &ldquo;suitable envelope function&rdquo;, but the tail here is very heavy, hard to find.</li>
</ul>
<p>ðŸ‘‰ <strong>Intuitive Conclusion</strong>:
This is <strong>where MCMC shines</strong>: As long as we can compute $\tilde\pi(x)=e^{-x^4}$, i.e., the unnormalized density, we can design a Markov chain to converge to it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Plot e^{-x^4}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Domain range (-5 to 5 is enough to see main shape)</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4001</span>) 
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x<span style="color:#f92672">**</span><span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x, y, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;Curve of $e^{-x^4}$&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;$e^{-x^4}$&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Normalize e^{-x^4} to make it a Probability Density Function (PDF)</span>
</span></span><span style="display:flex;"><span>Z <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>trapezoid(y, x)                    <span style="color:#75715e"># Numerical integration</span>
</span></span><span style="display:flex;"><span>pdf <span style="color:#f92672">=</span> y <span style="color:#f92672">/</span> Z
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x, pdf, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Normalized $e^{-x^4}$ (PDF)&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#34;Normalized PDF from $e^{-x^4}$&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;x&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Density&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/img/contents/post/mcmc-statics/5_intro-mcmc/5_mcmc_intro_3_0.png" alt="png"></p>
<p><img src="/img/contents/post/mcmc-statics/5_intro-mcmc/5_mcmc_intro_3_1.png" alt="png"></p>
<h1 id="from-markov-chain-to-sampling">From Markov Chain to Sampling</h1>
<blockquote>
<blockquote>
<p>In short, by constructing a &ldquo;correct Markov chain&rdquo;, its stationary distribution becomes the target distribution; trajectory&rsquo;s long-term distribution â‰ˆ target distribution.</p></blockquote></blockquote>
<h2 id="turning-sampling-into-building-a-chain">Turning Sampling into &ldquo;Building a Chain&rdquo;</h2>
<p>Let state space be $\mathsf{X}$, want samples from target distribution $\pi$. We don&rsquo;t sample directly, but <strong>construct a transition kernel</strong> $P(x,A)=\Pr(X_{t+1}\in A\mid X_t=x)$, making $\pi$ its <strong>Stationary Distribution</strong> (invariant):</p>
$$
\pi(A) \;=\; \int_{\mathsf{X}} \pi(dx)\,P(x,A),\quad \forall A.
$$<p>Intuition: <strong>If you randomly pick a start point under $\pi$, then take a step according to $P$, the distribution remains unchanged</strong>. Such a &ldquo;distribution-invariant&rdquo; random walk, keeps walking, keeps staying on $\pi$.</p>
<h2 id="ensuring-reachable-aperiodic-forgetting-start">Ensuring &ldquo;Reachable, Aperiodic, Forgetting Start&rdquo;</h2>
<p>&ldquo;Stationary&rdquo; alone is not enough; we also need the chain to <strong>converge to</strong> $\pi$. Common sufficient conditions:</p>
<ul>
<li><strong>Irreducible</strong>: Can reach any &ldquo;mass&rdquo; region from anywhere with positive probability in finite steps;</li>
<li><strong>Aperiodic</strong>: Not stuck in fixed cycles;</li>
<li>Reasonable &ldquo;Recurrence&rdquo; (Harris recurrence etc. technical conditions).</li>
</ul>
<p>With these, classic results tell us: regardless of initial distribution, as time $t\to\infty$, distribution $P^t(x_0,\cdot)$ converges to $\pi$ <strong>in Total Variation distance</strong>:</p>
$$
\big\|P^t(x_0,\cdot)-\pi\big\|_{\mathrm{TV}}\to 0.
$$<p>So, <strong>discard early samples (burn-in)</strong>, subsequent trajectory is approximately from $\pi$.</p>
<h2 id="why-unnormalized-is-fine">Why &ldquo;Unnormalized is Fine&rdquo;</h2>
<p>Many MCMC constructions only need the ratio $\tilde\pi(x)\propto \pi(x)$. The reason lies in <strong>Detailed Balance / Reversibility</strong> (see next section): using only the <strong>ratio</strong> $\tilde\pi(y)/\tilde\pi(x)$ ensures &ldquo;symmetric flow&rdquo;, thereby getting $\pi$ as stationary distribution. <strong>No need for $Z$</strong> is the key advantage of MCMC.</p>
<h2 id="example-1">Example</h2>
<p>To avoid introducing specific algorithms, determining let&rsquo;s use the <strong>most familiar distribution: Uniform Distribution</strong> to illustrate.</p>
<p>Suppose target distribution is</p>
$$
\pi(x) = \text{Uniform}\{1,2,3\}.
$$<p>We design a Markov chain:</p>
<ul>
<li>Walk among 3 states: 1, 2, 3;</li>
<li>At each position, jump to another position with equal probability;</li>
<li>E.g., at 1, jump to 2 with prob 0.5, to 3 with prob 0.5.</li>
</ul>
<p>ðŸ‘‰ The transition matrix P:</p>
$$
P=\begin{bmatrix}
0 & 0.5 & 0.5 \\
0.5 & 0 & 0.5 \\
0.5 & 0.5 & 0
\end{bmatrix}.
$$<p>Let&rsquo;s run it and see how its distribution evolves.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Transition Matrix P</span>
</span></span><span style="display:flex;"><span>P <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.0</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initial distribution: All in state 1</span>
</span></span><span style="display:flex;"><span>dist <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.0</span>])
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> [dist]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Evolve 20 steps</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">20</span>):
</span></span><span style="display:flex;"><span>    dist <span style="color:#f92672">=</span> dist <span style="color:#f92672">@</span> P
</span></span><span style="display:flex;"><span>    history<span style="color:#f92672">.</span>append(dist)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(history)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Theoretical stationary distribution (Uniform)</span>
</span></span><span style="display:flex;"><span>pi <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(history[:,i], label<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;State </span><span style="color:#e6db74">{</span>i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axhline(pi[<span style="color:#ae81ff">0</span>], color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;k&#34;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;Stationary dist.&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Step&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Probability&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Markov chain approaching uniform distribution&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/img/contents/post/mcmc-statics/5_intro-mcmc/5_mcmc_intro_6_0.png" alt="png"></p>
<p>We can see:</p>
<ul>
<li>Initially distribution is all in <strong>state 1</strong>;</li>
<li>As steps increase, three curves gradually approach $1/3$;</li>
<li>Finally converges to stationary distribution (Uniform).</li>
</ul>
<p>ðŸ‘‰ <strong>Intuition</strong>:
Markov chain&rsquo;s &ldquo;walking&rdquo; allows us to sample from $\pi$ by relying on &ldquo;<strong>long-term stay proportion</strong>&rdquo; even if we don&rsquo;t sample directly from $\pi$.</p>
<h1 id="theory-and-intuition">Theory and Intuition</h1>
<blockquote>
<p>Why it works, why it converges</p>
<p>In short,</p>
<ul>
<li>Stationary distribution exists, and satisfies detailed balance â†’ Correctness.</li>
<li>Convergence speed (Mixing time) affects sample quality.</li>
<li>Smaller auto-correlation, larger ESS, &ldquo;more efficient&rdquo; chain.</li>
</ul></blockquote>
<h2 id="detailed-balance-reversibility-and-stationarity">Detailed Balance (Reversibility) and Stationarity</h2>
<p>If there exists $\pi$ such that</p>
$$
\pi(dx)\,P(x,dy) \;=\; \pi(dy)\,P(y,dx) \quad (\text{Symmetric Flow})
$$<p>Then the chain is called <strong>reversible</strong> with respect to $\pi$, implying $\pi$ is a stationary distribution.</p>
<p><strong>Intuition</strong>: Starting from $\pi$, the joint distribution of a forward step is same as a &ldquo;backward step&rdquo;, overall &ldquo;no net flow&rdquo;, so the steady state is &ldquo;undisturbed&rdquo;.</p>
<blockquote>
<p>Many MCMC algorithms (MH, Gibbs, HMC, etc.) explicitly or implicitly construct this reversibility/invariance.</p></blockquote>
<h2 id="convergence-ergodic-theorem-lln-clt">Convergence: Ergodic Theorem, LLN, CLT</h2>
<p>When the chain is <strong>Ergodic</strong> (irreducible, aperiodic, and properly recurrent), we have:</p>
<ul>
<li>
<p><strong>Ergodic Theorem / Markov Chain Law of Large Numbers</strong></p>
$$
  \frac{1}{T}\sum_{t=1}^T f(X_t) \;\xrightarrow{a.s.}\; \mathbb{E}_\pi[f(X)].
  $$<p>This guarantees using trajectory mean to estimate expectation is <strong>consistent</strong>.</p>
</li>
<li>
<p><strong>Central Limit Theorem (CLT)</strong> (under geometric ergodicity etc. conditions)</p>
$$
  \sqrt{T}\Big(\bar f_T-\mathbb{E}_\pi[f]\Big)\ \Rightarrow\ \mathcal N\!\Big(0,\ \sigma_f^2\Big),
  $$<p>Where</p>
$$
  \sigma_f^2 \;=\; \mathrm{Var}_\pi(f)\Big(1+2\sum_{k=1}^\infty \rho_k\Big),
  $$<p>$\rho_k$ is the auto-correlation at lag $k$. Define <strong>Integrated Autocorrelation Time</strong> (IACT)</p>
$$
  \tau_{\text{int}} \;=\; 1+2\sum_{k\ge1}\rho_k,
  $$<p>Then <strong>Effective Sample Size</strong> $\mathrm{ESS}\approx T/\tau_{\text{int}}$.
<strong>Intuition</strong>: Stronger correlation (slow decay of $\rho_k$) means each sample has <strong>lower information</strong>, smaller ESS.</p>
</li>
</ul>
<h2 id="mixing-time-spectral-gap-and-geometric-convergence">Mixing Time, Spectral Gap, and Geometric Convergence</h2>
<ul>
<li>
<p><strong>Mixing Time</strong> characterizes <strong>approach speed</strong> of $P^t$ to $\pi$, commonly defined by TV distance:</p>
$$
  \tau(\varepsilon)\;=\;\min\{t:\ \sup_{x_0}\|P^t(x_0,\cdot)-\pi\|_{\mathrm{TV}}\le \varepsilon\}.
  $$</li>
<li>
<p>For <strong>finite reversible</strong> chains, convergence speed is closely related to <strong>Spectral Gap</strong> $\gamma=1-\lambda_\star$ (second largest eigenvalue modulus $\lambda_\star$): $\tau(\varepsilon)$ typically scales as $\frac{1}{\gamma}\log(1/\varepsilon)$.
<strong>Intuition</strong>: Large $\gamma$ â‡’ &ldquo;Weak memory, fast forgetting&rdquo;, faster mixing.</p>
</li>
<li>
<p><strong>Conductance / Bottleneck</strong> characterizes &ldquo;difficulty of crossing regions&rdquo;, related to $\gamma$ via Cheeger inequality.
<strong>Intuition</strong>: Multi-modal distribution, deep &ldquo;valleys&rdquo; between peaks â‡’ Small conductance â‡’ Slow mixing (easy to get &ldquo;stuck in a mode&rdquo;).</p>
</li>
</ul>
<h2 id="burn-in-thinning-diagnostics-from-theory-to-practice">Burn-in, Thinning, Diagnostics (From Theory to Practice)</h2>
<ul>
<li>
<p><strong>Burn-in</strong>: Early samples &ldquo;not yet close to $\pi$&rdquo; bias the estimate, discarding an initial segment is usually safer.</p>
</li>
<li>
<p><strong>Thinning</strong>: &ldquo;Subsampling&rdquo; to save storage/reduce correlation, but from variance minimization perspective <strong>not strictly necessary</strong>; often better to keep all samples and use IACT/ESS for correct variance estimation.</p>
</li>
<li>
<p><strong>Diagnostics</strong> (Intuition-driven):</p>
<ul>
<li><strong>Trace</strong>: Wandering freely without obvious drift?</li>
<li><strong>ACF/ESS</strong>: Does correlation decay fast enough?</li>
<li><strong>Multi-chain R-hat</strong>: Do chains from multiple starting points mix together?</li>
<li><strong>Multi-modal</strong>: Any pattern of &ldquo;stuck in mode&rdquo; (long stay + sudden jump)?</li>
</ul>
</li>
</ul>
<h2 id="why-mcmc-can-walk-and-stay-in-high-density-areas">Why MCMC can &ldquo;Walk and Stay in High Density Areas&rdquo;</h2>
<p>View $\pi$ as Boltzmann distribution: $\pi(x)\propto e^{-U(x)}$, $U(x)=-\log \tilde\pi(x)$ looks like &ldquo;energy landscape&rdquo;. MCMC is like <strong>random thermal motion</strong> in the landscape:</p>
<ul>
<li>More willing to stay in <strong>Low Energy (High Density)</strong> regions;</li>
<li>But through mechanisms like &ldquo;perturbation/momentum/accept-reject&rdquo;, still has chance to <strong>cross energy barriers</strong> to explore other regions;</li>
<li>As long as mechanism guarantees <strong>invariant distribution is $\pi$</strong> and chain is <strong>ergodic</strong>, long-term statistics will be correct.</li>
</ul>
<h2 id="example-2">Example</h2>
<p>Let&rsquo;s build a &ldquo;Slow Mixing&rdquo; chain:</p>
<ul>
<li>States $\{0,1\}$,</li>
<li>From 0, stay with prob 0.95, jump to 1 with prob 0.05;</li>
<li>From 1, same logic.</li>
</ul>
<p>ðŸ‘‰ Although its stationary distribution is still uniform $[0.5,0.5]$, &ldquo;correlation&rdquo; is very strong.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Construct 2-state chain (Sticky)</span>
</span></span><span style="display:flex;"><span>P_slow <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0.95</span>, <span style="color:#ae81ff">0.05</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.95</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Simulate trajectory</span>
</span></span><span style="display:flex;"><span>T <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(T, dtype<span style="color:#f92672">=</span>int)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, T):
</span></span><span style="display:flex;"><span>    x[t] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>], p<span style="color:#f92672">=</span>P_slow[x[t<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compute Auto-Correlation Function (ACF)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">autocorr</span>(x, lag):
</span></span><span style="display:flex;"><span>    n <span style="color:#f92672">=</span> len(x)
</span></span><span style="display:flex;"><span>    x_mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(x)
</span></span><span style="display:flex;"><span>    num <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum((x[:n<span style="color:#f92672">-</span>lag]<span style="color:#f92672">-</span>x_mean)<span style="color:#f92672">*</span>(x[lag:]<span style="color:#f92672">-</span>x_mean)) <span style="color:#75715e"># Auto-covariance</span>
</span></span><span style="display:flex;"><span>    den <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sum((x<span style="color:#f92672">-</span>x_mean)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>) <span style="color:#75715e"># Variance</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> num<span style="color:#f92672">/</span>den
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lags <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>acfs <span style="color:#f92672">=</span> [autocorr(x, lag) <span style="color:#66d9ef">for</span> lag <span style="color:#f92672">in</span> lags]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>,<span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(lags, acfs)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Lag&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Autocorrelation&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;ACF of slow-mixing 2-state chain&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/img/contents/post/mcmc-statics/5_intro-mcmc/5_mcmc_intro_10_0.png" alt="png"></p>
<p>Here you can see:</p>
<ul>
<li>Auto-Correlation Function (ACF) <strong>decays very slowly</strong>;</li>
<li>Means adjacent samples are highly dependent, <strong>Effective Sample Size (ESS)</strong> is far less than total steps.</li>
</ul>
<p>ðŸ‘‰ <strong>Intuitive Summary</strong>:</p>
<ul>
<li>Markov chain will definitely converge to stationary distribution (if conditions met);</li>
<li>But &ldquo;Mixing Speed&rdquo; varies: some equalize in few steps, some drag on;</li>
<li>In MCMC, <strong>slow mixing chain has poor efficiency</strong>, resulting samples have &ldquo;little information&rdquo;.</li>
</ul>
<h1 id="critical-indicators-in-practice-no-algorithms-yet">Critical Indicators in Practice (No Algorithms Yet)</h1>
<ol>
<li>
<p><strong>Only need $\tilde\pi(x)$</strong>: Can compute log-density up to constant, then MCMC is go.</p>
</li>
<li>
<p><strong>Three things must be balanced</strong>:</p>
<ul>
<li><strong>Invariance</strong> ($\pi$ is stationary) â€” if not invariant then long-term wrong;</li>
<li><strong>Reachable</strong> (Irreducible/Aperiodic) â€” if unreachable then pointless;</li>
<li><strong>Fast Mixing</strong> (Spectral gap/Conductance/Small IACT) â€” otherwise ESS too low, cost too high.</li>
</ul>
</li>
<li>
<p><strong>Error Assessment</strong>: Use IACT/ESS + Markov Chain CLT for MC variance and confidence intervals.</p>
</li>
<li>
<p><strong>Multi-modal Warning</strong>: Multi-modal + high-dim correlation often leads to <strong>Metastability</strong>, need algorithms or strategies better at &ldquo;climbing hills crossing valleys&rdquo; (Tempering, Gradient methods, etc.) to improve conductance.</p>
</li>
</ol>
<h1 id="summary">Summary</h1>
<ul>
<li>
<p><strong>Purpose</strong>: Estimate expectation/sample from complex $\pi$; <strong>No normalization constant needed</strong>.</p>
</li>
<li>
<p><strong>Method</strong>: Build a chain $P$ such that $\pi$ is invariant (often using <strong>Detailed Balance</strong>).</p>
</li>
<li>
<p><strong>Condition</strong>: Irreducible + Aperiodic + Proper Recurrence â‡’ $P^t\to\pi$.</p>
</li>
<li>
<p><strong>Assessment</strong>:</p>
<ul>
<li>Convergence (after burn-in) + Mixing (IACT/ESS, Spectral gap, Conductance)</li>
<li>Diagnostics: Trace, ACF, R-hat, Multi-chain consistency</li>
</ul>
</li>
<li>
<p><strong>Error</strong>: $\mathrm{ESS}\approx T/\tau_{\text{int}}$, CLT gives confidence intervals.</p>
</li>
<li>
<p><strong>Difficulty</strong>: Multi-modal/High-dim correlation â‡’ Slow mixing; Need better &ldquo;moving strategies&rdquo;.</p>
</li>
</ul>


        
          <div class="blog-tags">
            
              <a href="http://localhost:1313/tags/monte-carlo/">Monte Carlo</a>&nbsp;
            
              <a href="http://localhost:1313/tags/markov-chain/">Markov Chain</a>&nbsp;
            
              <a href="http://localhost:1313/tags/sampling/">Sampling</a>&nbsp;
            
              <a href="http://localhost:1313/tags/mathematics/">Mathematics</a>&nbsp;
            
              <a href="http://localhost:1313/tags/python/">python</a>&nbsp;
            
          </div>
        

        
            <hr/>
            <section id="social-share">
              <div class="list-inline footer-links">
                

<div class="share-box" aria-hidden="true">
  <ul class="share">
    
    <li>
      <a
        href="//twitter.com/share?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fmcmc-statics%2fintro-mcmc%2f&amp;text=Introduction%20to%20MCMC&amp;via="
        target="_blank"
        title="Share on Twitter"
        class="share-btn twitter"
      >
        <i class="fab fa-twitter"></i>
      </a>
    </li>

    
    <li>
      <a
        href="//www.facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fpost%2fmcmc-statics%2fintro-mcmc%2f"
        target="_blank"
        title="Share on Facebook"
        class="share-btn facebook"
      >
        <i class="fab fa-facebook"></i>
      </a>
    </li>

    
    <li>
      <a
        href="//service.weibo.com/share/share.php?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fmcmc-statics%2fintro-mcmc%2f&amp;appkey=&amp;title=Introduction%20to%20MCMC&amp;pic="
        target="_blank"
        title="Share on Weibo"
        class="share-btn weibo"
      >
        <i class="fab fa-weibo"></i>
      </a>
    </li>

    
    <li>
      <a
        href="javascript:void(0);"
        onclick="openWechatModal('http:\/\/localhost:1313\/post\/mcmc-statics\/intro-mcmc\/')"
        title="Share on WeChat"
        class="share-btn wechat"
      >
        <i class="fab fa-weixin"></i>
      </a>
    </li>

    
    <li>
      <a
        href="//www.linkedin.com/shareArticle?url=http%3a%2f%2flocalhost%3a1313%2fpost%2fmcmc-statics%2fintro-mcmc%2f&amp;title=Introduction%20to%20MCMC"
        target="_blank"
        title="Share on LinkedIn"
        class="share-btn linkedin"
      >
        <i class="fab fa-linkedin"></i>
      </a>
    </li>

    
    <li>
      <a
        href="javascript:void(0);"
        onclick="copyToClipboard('http:\/\/localhost:1313\/post\/mcmc-statics\/intro-mcmc\/', 'Introduction to MCMC')"
        title="Copy Link"
        class="share-btn copy-link"
      >
        <i class="fas fa-link"></i>
      </a>
    </li>
  </ul>
</div>


<div id="wechat-modal" class="wechat-modal">
  <div class="wechat-modal-content">
    <span class="wechat-close" onclick="closeWechatModal()">&times;</span>
    <h4>
      Scan to Share <br />
      å¾®ä¿¡æ‰«ä¸€æ‰«åˆ†äº«
    </h4>
    <div id="qrcode" class="qrcode-container"></div>
  </div>
</div>

<script type="text/javascript">
  function copyToClipboard(url, title) {
    navigator.clipboard.writeText(url).then(
      function () {
        
        var existingToast = document.getElementById("share-toast");
        if (existingToast) {
          existingToast.remove();
        }

        var toast = document.createElement("div");
        toast.id = "share-toast";
        toast.innerText = "Link copied to clipboard!";
        toast.style.position = "fixed";
        toast.style.bottom = "20px";
        toast.style.left = "50%";
        toast.style.transform = "translateX(-50%)";
        toast.style.backgroundColor = "rgba(0,0,0,0.8)";
        toast.style.color = "#fff";
        toast.style.padding = "10px 20px";
        toast.style.borderRadius = "5px";
        toast.style.zIndex = "10000";
        toast.style.opacity = "0";
        toast.style.transition = "opacity 0.5s ease-in-out";

        document.body.appendChild(toast);

        
        void toast.offsetWidth;

        toast.style.opacity = "1";

        setTimeout(function () {
          toast.style.opacity = "0";
          setTimeout(function () {
            if (toast.parentNode) toast.parentNode.removeChild(toast);
          }, 500);
        }, 3000);
      },
      function (err) {
        console.error("Could not copy text: ", err);
      },
    );
  }

  function openWechatModal(url) {
    var modal = document.getElementById("wechat-modal");
    modal.style.display = "flex";
    var qrcodeContainer = document.getElementById("qrcode");
    qrcodeContainer.innerHTML = "";
    var img = document.createElement("img");
    img.src =
      "https://api.qrserver.com/v1/create-qr-code/?size=200x200&data=" +
      encodeURIComponent(url);
    img.style.width = "200px";
    img.style.height = "200px";
    qrcodeContainer.appendChild(img);
  }

  function closeWechatModal() {
    var modal = document.getElementById("wechat-modal");
    modal.style.display = "none";
  }

  
  window.onclick = function (event) {
    var modal = document.getElementById("wechat-modal");
    if (event.target == modal) {
      modal.style.display = "none";
    }
  };
</script>


              </div>
            </section>
        

        
          
            
          

          
                  <h4 class="see-also">See also</h4>
                  <ul>
                
                
                    <li><a href="/post/mcmc-statics/markov-chains/">Understanding Markov Chains</a></li>
                
                    <li><a href="/post/mcmc-statics/monte-carlo/">Monte Carlo Sampling</a></li>
                
                    <li><a href="/post/mcmc-statics/probability/">What is Probability?</a></li>
                
                    <li><a href="/post/mcmc-statics/random-variables/">Random Variables and Sampling</a></li>
                
                    <li><a href="/notes/probability_theory_preliminaries/">[Course Notes] Probability Theory and Mathematical Statistics | Preliminaries</a></li>
                
              </ul>

          
        
      </article>

      
        <ul class="pager blog-pager">
          
            <li class="previous">
              <a href="http://localhost:1313/post/mcmc-statics/probability/" data-toggle="tooltip" data-placement="top" title="What is Probability?">&larr; Previous Post</a>
            </li>
          
          
            <li class="next">
              <a href="http://localhost:1313/post/mcmc-statics/monte-carlo/" data-toggle="tooltip" data-placement="top" title="Monte Carlo Sampling">Next Post &rarr;</a>
            </li>
          
        </ul>
      


      

    </div>
    
    
    <div class="col-lg-3 visible-lg-block">
      
      
      <div class="sidebar-toc">
        <h2 class="sidebar-toc-title">ç›®å½•</h2>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#why-do-we-need-mcmc">Why Do We Need MCMC?</a>
      <ul>
        <li><a href="#example">Example</a></li>
      </ul>
    </li>
    <li><a href="#from-markov-chain-to-sampling">From Markov Chain to Sampling</a>
      <ul>
        <li><a href="#turning-sampling-into-building-a-chain">Turning Sampling into &ldquo;Building a Chain&rdquo;</a></li>
        <li><a href="#ensuring-reachable-aperiodic-forgetting-start">Ensuring &ldquo;Reachable, Aperiodic, Forgetting Start&rdquo;</a></li>
        <li><a href="#why-unnormalized-is-fine">Why &ldquo;Unnormalized is Fine&rdquo;</a></li>
        <li><a href="#example-1">Example</a></li>
      </ul>
    </li>
    <li><a href="#theory-and-intuition">Theory and Intuition</a>
      <ul>
        <li><a href="#detailed-balance-reversibility-and-stationarity">Detailed Balance (Reversibility) and Stationarity</a></li>
        <li><a href="#convergence-ergodic-theorem-lln-clt">Convergence: Ergodic Theorem, LLN, CLT</a></li>
        <li><a href="#mixing-time-spectral-gap-and-geometric-convergence">Mixing Time, Spectral Gap, and Geometric Convergence</a></li>
        <li><a href="#burn-in-thinning-diagnostics-from-theory-to-practice">Burn-in, Thinning, Diagnostics (From Theory to Practice)</a></li>
        <li><a href="#why-mcmc-can-walk-and-stay-in-high-density-areas">Why MCMC can &ldquo;Walk and Stay in High Density Areas&rdquo;</a></li>
        <li><a href="#example-2">Example</a></li>
      </ul>
    </li>
    <li><a href="#critical-indicators-in-practice-no-algorithms-yet">Critical Indicators in Practice (No Algorithms Yet)</a></li>
    <li><a href="#summary">Summary</a></li>
  </ul>
</nav>
      </div>
      
    </div>
    
  </div>
</div>

      <footer>
  <div class="container">
    
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center footer-links">
          
              <li>
		
		  <a href="mailto:ele.qiong@gmail.com" title="Email me">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://github.com/ictar" title="GitHub">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li>
		
		  <a href="https://linkedin.com/in/qiongjie-xu" title="LinkedIn">
		
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
          
          
          
        </ul>
        <p class="credits copyright text-muted">
          
            
              <a href="https://www.xuqiongjie.com">Qiongjie.X</a>
            
          

          &nbsp;&bull;&nbsp;&copy;
          
            2026
          

          
            &nbsp;&bull;&nbsp;
            <a href="http://localhost:1313/">Qiongjie&#39;s Notes</a>
          
        </p>
        
        <p class="credits theme-by text-muted">
          <a href="https://gohugo.io">Hugo v0.147.4</a> powered &nbsp;&bull;&nbsp; Theme <a href="https://github.com/halogenica/beautifulhugo">Beautiful Hugo</a> adapted from <a href="https://deanattali.com/beautiful-jekyll/">Beautiful Jekyll</a>
          
        </p>
      </div>
    </div>
  </div>
</footer><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<script src="https://code.jquery.com/jquery-3.7.0.slim.min.js" integrity="sha384-w5y/xIeYixWvfM+A1cEbmHPURnvyqmVg5eVENruEdDjcyRLUSNej7512JQGspFUr" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.4.1/dist/js/bootstrap.min.js" integrity="sha384-aJ21OjlMXNL5UyIl/XNwTMqvzeRMZH2w8c5cRVpzpU8Y5bApTppSuUkhZXN0VxHd" crossorigin="anonymous"></script>

<script src="http://localhost:1313/js/main.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe.min.js" integrity="sha384-QELNnmcmU8IR9ZAykt67vGr9/rZJdHbiWi64V88fCPaOohUlHCqUD/unNN0BXSqy" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/photoswipe/4.1.2/photoswipe-ui-default.min.js" integrity="sha384-m67o7SkQ1ALzKZIFh4CiTA8tmadaujiTa9Vu+nqPSwDOqHrDmxLezTdFln8077+q" crossorigin="anonymous"></script><script src="http://localhost:1313/js/load-photoswipe.js"></script>









<script src="http://localhost:1313/js/toc-enhancements.js"></script>


    
  </body>
</html>

