<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Monte Carlo–Markov Chains Statistical Methods on Qiongjie&#39;s Notes</title>
    <link>http://localhost:1313/post/mcmc-statics/</link>
    <description>Recent content in Monte Carlo–Markov Chains Statistical Methods on Qiongjie&#39;s Notes</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>ele.qiong@gmail.com (Qiongjie.X)</managingEditor>
    <webMaster>ele.qiong@gmail.com (Qiongjie.X)</webMaster>
    <lastBuildDate>Mon, 02 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/post/mcmc-statics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Stochastic Optimization Explained: Simulated Annealing &amp; Pincus Theorem</title>
      <link>http://localhost:1313/post/mcmc-statics/stochastic-optimization/</link>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/stochastic-optimization/</guid>
      <description>When optimization problems are trapped in the maze of local optima, deterministic algorithms are often helpless. This article takes you into the world of stochastic optimization, exploring how to transform the problem of finding minimum energy into finding maximum probability. We will delve into the physical intuition and mathematical principles of the Simulated Annealing algorithm, demonstrate its elegant mechanism of &amp;lsquo;high-temperature exploration, low-temperature locking&amp;rsquo; through dynamic visualization, and derive the Pincus Theorem in detail, mathematically proving why the annealing algorithm can find the global optimal solution.</description>
    </item>
    <item>
      <title>Deterministic Optimization Explained: The Mathematical Essence of Gradient Descent</title>
      <link>http://localhost:1313/post/mcmc-statics/deterministic-optimization/</link>
      <pubDate>Sun, 01 Feb 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/deterministic-optimization/</guid>
      <description>Deterministic optimization is the cornerstone for understanding modern MCMC algorithms (like HMC, Langevin). This article delves into three classic deterministic optimization strategies: Newton&amp;rsquo;s Method (second-order perspective using curvature), Coordinate Descent (the divide-and-conquer predecessor to Gibbs), and Steepest Descent (greedy first-order exploration). Through mathematical derivation and Python visualization, we compare their behavioral patterns and convergence characteristics across different terrains (convex surfaces, narrow valleys, strong coupling).</description>
    </item>
    <item>
      <title>Gibbs Sampling Explained: The Wisdom of Divide and Conquer</title>
      <link>http://localhost:1313/post/mcmc-statics/gibbs-sampling/</link>
      <pubDate>Fri, 30 Jan 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/gibbs-sampling/</guid>
      <description>When high-dimensional spaces are overwhelming, Gibbs sampling adopts a &amp;lsquo;divide and conquer&amp;rsquo; strategy. By utilizing full conditional distributions, it breaks down complex N-dimensional joint sampling into N simple 1-dimensional sampling steps. This article explains its intuition, mathematical proof (Brook&amp;rsquo;s Lemma), and Python implementation.</description>
    </item>
    <item>
      <title>The Metropolis-Hastings Algorithm: Breaking the Symmetry</title>
      <link>http://localhost:1313/post/mcmc-statics/metropolis-hastings/</link>
      <pubDate>Thu, 29 Jan 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/metropolis-hastings/</guid>
      <description>The original Metropolis is limited by symmetric proposals, often &amp;lsquo;hitting walls&amp;rsquo; at boundaries or getting lost in high dimensions. The MH algorithm introduces the &amp;lsquo;Hastings Correction&amp;rsquo;, allowing asymmetric proposals (like Langevin dynamics) while maintaining detailed balance, significantly improving efficiency.</description>
    </item>
    <item>
      <title>Metropolis Algorithm Explained: Implementation &amp; Intuition</title>
      <link>http://localhost:1313/post/mcmc-statics/metropolis/</link>
      <pubDate>Sat, 24 Jan 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/metropolis/</guid>
      <description>The Metropolis algorithm is the cornerstone of MCMC. We delve into its strategy for handling unnormalized densities, from the random walk mechanism to sampling 2D correlated Gaussians, complete with Python implementation and visual diagnostics.</description>
    </item>
    <item>
      <title>Understanding Markov Chains</title>
      <link>http://localhost:1313/post/mcmc-statics/markov-chains/</link>
      <pubDate>Fri, 23 Jan 2026 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/markov-chains/</guid>
      <description>Learn about Markov processes, stationary distributions, and convergence of Markov chains.</description>
    </item>
    <item>
      <title>Monte Carlo Sampling</title>
      <link>http://localhost:1313/post/mcmc-statics/monte-carlo/</link>
      <pubDate>Sat, 30 Aug 2025 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/monte-carlo/</guid>
      <description>Understand the core concepts of Monte Carlo: Law of Large Numbers, rejection sampling, importance sampling, variance reduction techniques (antithetic variates, control variates, stratified sampling).</description>
    </item>
    <item>
      <title>Introduction to MCMC</title>
      <link>http://localhost:1313/post/mcmc-statics/intro-mcmc/</link>
      <pubDate>Fri, 22 Aug 2025 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/intro-mcmc/</guid>
      <description>The reason we need MCMC is that many distributions are only known in their unnormalized form, making traditional sampling/integration methods ineffective. By constructing a &amp;lsquo;correct Markov chain&amp;rsquo;, we can obtain the target distribution from its stationary distribution, meaning the long-term distribution of the trajectory ≈ target distribution.</description>
    </item>
    <item>
      <title>What is Probability?</title>
      <link>http://localhost:1313/post/mcmc-statics/probability/</link>
      <pubDate>Fri, 15 Aug 2025 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/probability/</guid>
      <description>This article introduces the basic concepts and rigorous formulas of probability, serving as the foundation for understanding random variables, sampling, and MCMC.</description>
    </item>
    <item>
      <title>Random Variables and Sampling</title>
      <link>http://localhost:1313/post/mcmc-statics/random-variables/</link>
      <pubDate>Sat, 02 Aug 2025 00:00:00 +0000</pubDate><author>ele.qiong@gmail.com (Qiongjie.X)</author>
      <guid>http://localhost:1313/post/mcmc-statics/random-variables/</guid>
      <description>Understand concepts of random variables, PDF, expectation, and sampling methods for common distributions (Uniform, Normal, Exponential).</description>
    </item>
  </channel>
</rss>
